{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/ml_tricks/colab/colab_connect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"ubgiAABGmadO"},"source":["# This Colab notebook is ment to be executed for using the GPU units in Colab\n","The idea is to remotely execute the repo files (classes, main, etc)"]},{"cell_type":"markdown","metadata":{"id":"Kc3SGLPSX_7E"},"source":["# 0: Before starting, verifiy that: **After pulling the repo, you have copied the .env file into the Github repo !**"]},{"cell_type":"markdown","metadata":{"id":"r0sG1QyylCkW"},"source":["# 1 Mount Google Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1452,"status":"ok","timestamp":1755966348280,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"w69hWB_qkwLI","outputId":"9b868b31-c88a-4491-e2b5-d24a2a57d50b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"FPK232yxlHD1"},"source":["# 2 config Git"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":137,"status":"ok","timestamp":1755966348426,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"GradLm7NkxKN"},"outputs":[],"source":["# !git config --global user.name \"zbotta\"\n","# !git config --global user.email \"zbotta@proton.me\"\n","\n","!git config --global user.name \"SamdGuizani\"\n","!git config --global user.email \"samd.guizani@gmail.com\""]},{"cell_type":"markdown","metadata":{"id":"tQRJVtrHynSw"},"source":["## RUN THIS CELL ONLY ONCE!\n","To clone the Github repo"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":569,"status":"ok","timestamp":1755966348997,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"E1otG_64l1Qu"},"outputs":[],"source":["from google.colab import userdata\n","# github_token = userdata.get('zbotta_token')\n","github_token = userdata.get('GitHub_Samd_ReportAgent_GoogleColab')\n","\n","token = github_token\n","username = \"zbotta\"\n","repo = 'reportingAgent'\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1755966349187,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"PQgo_qSExuXm","outputId":"f008b567-de5d-46aa-c986-4280803623dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path '/content/drive/MyDrive/GitHub/reportingAgent' already exists and is not an empty directory.\n"]}],"source":["!git clone https://{username}:{github_token}@github.com/{username}/{repo}.git /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1755966349209,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"ze8qK-7nmx_O","outputId":"2e6471f6-b51a-4a7e-f219-7b529f5f1935"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"kVgHJTXrHLIX","executionInfo":{"status":"ok","timestamp":1755966349285,"user_tz":-120,"elapsed":73,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"outputs":[],"source":["#!git remote set-url origin https://{username}:{github_token}@github.com/{username}/{repo}.git"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"0Rwp4A70m_2U","executionInfo":{"status":"ok","timestamp":1755966349288,"user_tz":-120,"elapsed":23,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"outputs":[],"source":["#!git remote get-url origin"]},{"cell_type":"markdown","metadata":{"id":"q0fNz_r7m4UG"},"source":["# Use git commands"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3715,"status":"ok","timestamp":1755966352989,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"ybZIZbg2-qhU","outputId":"f1a9c32f-653b-46b4-9a96-cb9458b23fc5"},"outputs":[{"output_type":"stream","name":"stdout","text":["M\tPoC/PoC_01_Prompt and report gen with different models.ipynb\n","M\tPoC/PoC_04_Evaluating metrics correlations.ipynb\n","M\tPoC/PoC_05_Model grid search analysis.ipynb\n","D\tPoC/PoC_Prompt and report gen.ipynb\n","M\tPoC/reportAgent-remote_Samd.ipynb\n","M\tapp/datasets/pharma_dev_reports_collection.xlsx\n","Already on 'dev'\n","Your branch is up to date with 'origin/dev'.\n"]}],"source":["!git fetch\n","!git checkout dev"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":969,"status":"ok","timestamp":1755966353960,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"g9D7zP_oRsaN","outputId":"7f3dd280-a597-4acf-b7ca-e0ade5637825"},"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}],"source":["!git pull"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1755966354305,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"ooqu1-hlm7Af","outputId":"ad19990e-054f-413b-a1be-081a9ada70d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["On branch dev\n","Your branch is up to date with 'origin/dev'.\n","\n","Changes not staged for commit:\n","  (use \"git add/rm <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   PoC/PoC_01_Prompt and report gen with different models.ipynb\u001b[m\n","\t\u001b[31mmodified:   PoC/PoC_04_Evaluating metrics correlations.ipynb\u001b[m\n","\t\u001b[31mmodified:   PoC/PoC_05_Model grid search analysis.ipynb\u001b[m\n","\t\u001b[31mdeleted:    PoC/PoC_Prompt and report gen.ipynb\u001b[m\n","\t\u001b[31mmodified:   PoC/reportAgent-remote_Samd.ipynb\u001b[m\n","\t\u001b[31mmodified:   app/datasets/pharma_dev_reports_collection.xlsx\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mPoC/Archive/\u001b[m\n","\t\u001b[31mPoC/python_env_setup.sh\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}],"source":["!git status"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":60,"status":"ok","timestamp":1755966354306,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"BsN8-zMZ8Z-n"},"outputs":[],"source":["# !git push origin dev\n"]},{"cell_type":"markdown","metadata":{"id":"nXz98pxKo8oH"},"source":["# Using project scripts\n","\n","Reference : [Importing python library from Drive](https://colab.research.google.com/drive/12qC2abKAIAlUM_jNAokGlooKY-idbSxi#scrollTo=prUMpfLaB-D7)"]},{"cell_type":"markdown","metadata":{"id":"mvLEUb7VzVD0"},"source":["## Install project dependencies\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1755966354311,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"S6sPDJjVSHQf","outputId":"e31bef91-f042-4322-e701-45effc250c15"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-lZSS9qFzSLk","outputId":"6133d313-f8a5-4fe2-885b-0829ce53d60f","collapsed":true,"executionInfo":{"status":"ok","timestamp":1755966363423,"user_tz":-120,"elapsed":9127,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: absl-py~=1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 1)) (1.4.0)\n","Requirement already satisfied: aiohappyeyeballs~=2.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 2)) (2.6.1)\n","Requirement already satisfied: aiohttp~=3.12.15 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 3)) (3.12.15)\n","Requirement already satisfied: aiosignal~=1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 4)) (1.4.0)\n","Requirement already satisfied: annotated-types~=0.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 5)) (0.7.0)\n","Requirement already satisfied: anyio~=4.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 6)) (4.10.0)\n","Requirement already satisfied: attrs~=25.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 7)) (25.3.0)\n","Requirement already satisfied: audioread~=3.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 8)) (3.0.1)\n","Requirement already satisfied: bert-score~=0.3.13 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 9)) (0.3.13)\n","Requirement already satisfied: certifi~=2025.8.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 10)) (2025.8.3)\n","Requirement already satisfied: charset-normalizer~=3.4.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 11)) (3.4.3)\n","Requirement already satisfied: click~=8.2.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 12)) (8.2.1)\n","Requirement already satisfied: cloudpickle~=3.1.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 13)) (3.1.1)\n","Requirement already satisfied: colorama~=0.4.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 14)) (0.4.6)\n","Requirement already satisfied: contourpy~=1.3.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 15)) (1.3.3)\n","Requirement already satisfied: cycler~=0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 16)) (0.12.1)\n","Requirement already satisfied: datasets~=4.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 17)) (4.0.0)\n","Requirement already satisfied: dill~=0.3.8 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 18)) (0.3.8)\n","Requirement already satisfied: distributed~=2025.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 19)) (2025.5.0)\n","Requirement already satisfied: diskcache~=5.6.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 20)) (5.6.3)\n","Requirement already satisfied: distro~=1.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 21)) (1.9.0)\n","Requirement already satisfied: docstring_parser~=0.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 22)) (0.17.0)\n","Requirement already satisfied: docutils~=0.21.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 23)) (0.21.2)\n","Requirement already satisfied: dotenv~=0.9.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 24)) (0.9.9)\n","Requirement already satisfied: et_xmlfile~=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 25)) (2.0.0)\n","Requirement already satisfied: evaluate~=0.4.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 26)) (0.4.5)\n","Requirement already satisfied: Farama-Notifications~=0.0.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 27)) (0.0.4)\n","Requirement already satisfied: fastapi~=0.116.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 28)) (0.116.1)\n","Requirement already satisfied: filelock~=3.18.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 29)) (3.18.0)\n","Requirement already satisfied: fonttools~=4.59.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 30)) (4.59.1)\n","Requirement already satisfied: frozenlist~=1.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 31)) (1.7.0)\n","Requirement already satisfied: fsspec~=2025.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 32)) (2025.3.0)\n","Requirement already satisfied: genson~=1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 33)) (1.3.0)\n","Requirement already satisfied: groq~=0.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 34)) (0.26.0)\n","Requirement already satisfied: h11~=0.16.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 35)) (0.16.0)\n","Requirement already satisfied: httpcore~=1.0.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 36)) (1.0.9)\n","Requirement already satisfied: httpx~=0.28.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 37)) (0.28.1)\n","Requirement already satisfied: huggingface-hub~=0.34.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 38)) (0.34.4)\n","Requirement already satisfied: idna~=3.10 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 39)) (3.10)\n","Requirement already satisfied: iniconfig~=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 40)) (2.1.0)\n","Requirement already satisfied: instructor~=1.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 41)) (1.10.0)\n","Requirement already satisfied: interegular~=0.3.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 42)) (0.3.3)\n","Requirement already satisfied: iso3166~=2.1.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 43)) (2.1.1)\n","Requirement already satisfied: Jinja2~=3.1.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 44)) (3.1.6)\n","Requirement already satisfied: jiter~=0.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 45)) (0.10.0)\n","Requirement already satisfied: joblib~=1.5.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 46)) (1.5.1)\n","Requirement already satisfied: jsonpath-ng~=1.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 47)) (1.7.0)\n","Requirement already satisfied: jsonpickle~=4.1.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 48)) (4.1.1)\n","Requirement already satisfied: jsonpointer~=3.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 49)) (3.0.0)\n","Requirement already satisfied: jsonschema~=4.25.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 50)) (4.25.1)\n","Requirement already satisfied: jsonschema-specifications~=2025.4.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 51)) (2025.4.1)\n","Requirement already satisfied: kiwisolver~=1.4.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 52)) (1.4.9)\n","Requirement already satisfied: lark~=1.2.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 53)) (1.2.2)\n","Requirement already satisfied: markdown-it-py~=4.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 54)) (4.0.0)\n","Requirement already satisfied: MarkupSafe~=3.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 55)) (3.0.2)\n","Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 56)) (3.10.0)\n","Requirement already satisfied: mdurl~=0.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 57)) (0.1.2)\n","Requirement already satisfied: mpmath~=1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 58)) (1.3.0)\n","Requirement already satisfied: multidict~=6.6.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 59)) (6.6.4)\n","Requirement already satisfied: multiprocess~=0.70.16 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 60)) (0.70.16)\n","Requirement already satisfied: nest-asyncio~=1.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 61)) (1.6.0)\n","Requirement already satisfied: networkx~=3.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 62)) (3.5)\n","Requirement already satisfied: nltk~=3.9.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 63)) (3.9.1)\n","Requirement already satisfied: numpy~=2.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 64)) (2.0.2)\n","Requirement already satisfied: openai~=1.99.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 65)) (1.99.9)\n","Requirement already satisfied: openpyxl~=3.1.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 66)) (3.1.5)\n","Requirement already satisfied: outlines~=1.1.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 67)) (1.1.1)\n","Requirement already satisfied: outlines_core~=0.1.26 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 68)) (0.1.26)\n","Requirement already satisfied: packaging~=25.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 69)) (25.0)\n","Requirement already satisfied: pandas~=2.2.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 70)) (2.2.2)\n","Requirement already satisfied: pillow~=11.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 71)) (11.3.0)\n","Requirement already satisfied: pluggy~=1.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 72)) (1.6.0)\n","Requirement already satisfied: ply~=3.11 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 73)) (3.11)\n","Requirement already satisfied: propcache~=0.3.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 74)) (0.3.2)\n","Requirement already satisfied: pyarrow~=18.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 75)) (18.1.0)\n","Requirement already satisfied: pydantic~=2.11.7 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 76)) (2.11.7)\n","Requirement already satisfied: pydantic_core~=2.33.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 77)) (2.33.2)\n","Requirement already satisfied: Pygments~=2.19.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 78)) (2.19.2)\n","Requirement already satisfied: pyparsing~=3.2.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 79)) (3.2.3)\n","Requirement already satisfied: pytest~=8.4.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 80)) (8.4.1)\n","Requirement already satisfied: python-dateutil~=2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 81)) (2.9.0.post0)\n","Requirement already satisfied: python-louvain~=0.16 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 82)) (0.16)\n","Requirement already satisfied: python-dotenv~=1.1.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 83)) (1.1.1)\n","Requirement already satisfied: pytz~=2025.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 84)) (2025.2)\n","Requirement already satisfied: PyYAML~=6.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 85)) (6.0.2)\n","Requirement already satisfied: referencing~=0.36.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 86)) (0.36.2)\n","Requirement already satisfied: regex~=2024.11.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 87)) (2024.11.6)\n","Requirement already satisfied: reportlab>=3.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 88)) (4.4.3)\n","Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 89)) (2.32.4)\n","Requirement already satisfied: rich~=13.9.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 90)) (13.9.4)\n","Requirement already satisfied: rouge_score~=0.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 91)) (0.1.2)\n","Requirement already satisfied: rpds-py~=0.27.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 92)) (0.27.0)\n","Requirement already satisfied: safetensors~=0.6.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 93)) (0.6.2)\n","Requirement already satisfied: scikit-learn~=1.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 94)) (1.6.1)\n","Requirement already satisfied: scipy~=1.16.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 95)) (1.16.1)\n","Requirement already satisfied: sentence-transformers~=5.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 96)) (5.1.0)\n","Requirement already satisfied: shellingham~=1.5.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 97)) (1.5.4)\n","Requirement already satisfied: six~=1.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 98)) (1.17.0)\n","Requirement already satisfied: sniffio~=1.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 99)) (1.3.1)\n","Requirement already satisfied: starlette~=0.47.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 100)) (0.47.2)\n","Requirement already satisfied: streamlit>=1.35 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 101)) (1.48.1)\n","Requirement already satisfied: streamlit-extras>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 102)) (0.6.0)\n","Requirement already satisfied: sympy~=1.13.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 103)) (1.13.3)\n","Requirement already satisfied: tenacity~=9.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 104)) (9.1.2)\n","Requirement already satisfied: threadpoolctl~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 105)) (3.6.0)\n","Requirement already satisfied: tokenizers~=0.21.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 106)) (0.21.4)\n","Requirement already satisfied: torch~=2.8 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 107)) (2.8.0+cu126)\n","Requirement already satisfied: tqdm~=4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 108)) (4.67.1)\n","Requirement already satisfied: transformers~=4.53.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 109)) (4.53.3)\n","Requirement already satisfied: typer~=0.16.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 110)) (0.16.0)\n","Requirement already satisfied: typing-inspection~=0.4.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 111)) (0.4.1)\n","Requirement already satisfied: typing_extensions~=4.14.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 112)) (4.14.1)\n","Requirement already satisfied: tzdata~=2025.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 113)) (2025.2)\n","Requirement already satisfied: urllib3~=2.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 114)) (2.5.0)\n","Requirement already satisfied: xxhash~=3.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 115)) (3.5.0)\n","Requirement already satisfied: yarl~=1.20.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 116)) (1.20.1)\n","Requirement already satisfied: dask==2025.5.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (2025.5.0)\n","Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (1.0.0)\n","Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (1.1.1)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (5.9.5)\n","Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (2.4.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (3.1.0)\n","Requirement already satisfied: toolz>=0.11.2 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (0.12.1)\n","Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (6.4.2)\n","Requirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (3.0.0)\n","Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask==2025.5.0->distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (1.4.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub~=0.34.4->-r requirements_colab.txt (line 38)) (1.1.7)\n","Requirement already satisfied: airportsdata in /usr/local/lib/python3.12/dist-packages (from outlines~=1.1.1->-r requirements_colab.txt (line 67)) (20250811)\n","Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (5.5.2)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (5.29.5)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (0.10.2)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (3.1.45)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (0.9.1)\n","Requirement already satisfied: entrypoints>=0.4 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.4)\n","Requirement already satisfied: htbuilder>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.9.0)\n","Requirement already satisfied: markdownlit>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.0.7)\n","Requirement already satisfied: plotly>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (5.24.1)\n","Requirement already satisfied: prometheus-client>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.22.1)\n","Requirement already satisfied: st-annotated-text>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (4.0.2)\n","Requirement already satisfied: st-theme>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (1.2.3)\n","Requirement already satisfied: streamlit-avatar>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.1.3)\n","Requirement already satisfied: streamlit-camera-input-live>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.2.0)\n","Requirement already satisfied: streamlit-card>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (1.0.2)\n","Requirement already satisfied: streamlit-embedcode>=0.1.2 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.1.2)\n","Requirement already satisfied: streamlit-faker>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.0.4)\n","Requirement already satisfied: streamlit-image-coordinates<0.2.0,>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.1.9)\n","Requirement already satisfied: streamlit-keyup>=0.1.9 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.3.0)\n","Requirement already satisfied: streamlit-toggle-switch>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (1.0.2)\n","Requirement already satisfied: streamlit-vertical-slider>=2.5.5 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (2.5.5)\n","Requirement already satisfied: validators>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.35.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (75.2.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (3.4.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.35->-r requirements_colab.txt (line 101)) (2.1.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.35->-r requirements_colab.txt (line 101)) (4.0.12)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (3.8.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (5.4.0)\n","Requirement already satisfied: favicon in /usr/local/lib/python3.12/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.7.0)\n","Requirement already satisfied: pymdown-extensions in /usr/local/lib/python3.12/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (10.16.1)\n","Requirement already satisfied: altex>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-faker>=0.0.2->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.2.0)\n","Requirement already satisfied: faker in /usr/local/lib/python3.12/dist-packages (from streamlit-faker>=0.0.2->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (37.5.3)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.35->-r requirements_colab.txt (line 101)) (5.0.2)\n","Requirement already satisfied: beautifulsoup4>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from favicon->markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (4.13.4)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.7.0->favicon->markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (2.7)\n"]}],"source":["# !pip install -r requirements.txt\n","!pip install -r requirements_colab.txt"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9835,"status":"ok","timestamp":1755966373262,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"7jbcWB1PGsoj","outputId":"bec58f21-e87e-4370-b0ec-22ee71ad5303","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"]}],"source":["!pip install --upgrade torch torchvision"]},{"cell_type":"markdown","metadata":{"id":"-2wXabjgzszt"},"source":["## Import a python script from project"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1755966373283,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"a8WjxROAzwUT","outputId":"feaba4fb-6ae5-4e81-844e-5d188419ad6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":801,"status":"ok","timestamp":1755966374087,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"-oZlOWBODm4M","outputId":"a47fb086-a352-40a5-ea70-1acc215b89d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}],"source":["!git pull"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":126,"status":"ok","timestamp":1755966374216,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"4r_Hif3sXJDN","outputId":"340a95be-8de2-4a83-daa3-119df0ea3ce7"},"outputs":[{"output_type":"stream","name":"stdout","text":["08/23/2025 16:18:13 - __main__ - INFO - Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['HuggingFaceTB/SmolLM3-3B'], 'prompt_method': ['B'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [105], 'end_idx': [106], 'temperature': [1.0], 'top_p': [0.3], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True], 'repetition_penalty': [1.0]}\n","08/23/2025 16:18:13 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/23/2025 16:18:13 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/23/2025 16:18:18 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/23/2025 16:18:18 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/23/2025 16:18:20 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/23/2025 16:18:20 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/23/2025 16:18:24 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/23/2025 16:18:24 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/23/2025 16:18:24 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/23/2025 16:18:24 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/23/2025 16:18:24 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/23/2025 16:18:24 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/23/2025 16:18:24 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/23/2025 16:18:24 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/23/2025 16:18:24 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/23/2025 16:18:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/23/2025 16:18:26 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/23/2025 16:18:26 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/23/2025 16:18:26 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/23/2025 16:18:26 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/23/2025 16:18:26 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/23/2025 16:18:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/23/2025 16:18:26 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/23/2025 16:18:26 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/23/2025 16:18:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/23/2025 16:18:26 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/23/2025 16:18:26 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/23/2025 16:18:27 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/pharma_dev_reports_collection.xlsx\n","08/23/2025 16:18:27 - huggingface_hub.file_download - INFO - Downloading 'generation_config.json' to '/root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/5339609fb994a8644fe865353cf0f1d3ee0130c8.incomplete'\n","08/23/2025 16:18:28 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/5339609fb994a8644fe865353cf0f1d3ee0130c8\n","08/23/2025 16:18:28 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/1c00fc78bd9cf90108046bc433cb34992480f1c1/generation_config.json\n","08/23/2025 16:18:28 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 128012,\n","  \"pad_token_id\": 128004,\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.95\n","}\n","\n","08/23/2025 16:18:28 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","08/23/2025 16:18:28 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","08/23/2025 16:18:28 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","08/23/2025 16:18:28 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 0.6, 'top_k': 50, 'top_p': 0.95, 'repetition_penalty': 1.0, 'do_sample': True}\n","08/23/2025 16:18:28 - mods.testBench - INFO - Test Bench loaded\n","08/23/2025 16:18:28 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/61910c2db5cbdc9e6a6f37e14aaf00584cc6ad47.incomplete'\n","08/23/2025 16:18:28 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/61910c2db5cbdc9e6a6f37e14aaf00584cc6ad47\n","08/23/2025 16:18:29 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/7b6a500b662a34eb3f0374db856ba4ad7de4c81040571d78dc0d357238930005.incomplete'\n","08/23/2025 16:18:30 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/7b6a500b662a34eb3f0374db856ba4ad7de4c81040571d78dc0d357238930005\n","08/23/2025 16:18:32 - huggingface_hub.file_download - INFO - Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/190d5624dbbc1ad56f2f34c9d58e03fef7e5328b.incomplete'\n","08/23/2025 16:18:32 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/190d5624dbbc1ad56f2f34c9d58e03fef7e5328b\n","08/23/2025 16:18:32 - huggingface_hub.file_download - INFO - Downloading 'chat_template.jinja' to '/root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/e01e3a1bca00ae47bca8326b38cc397729f87481.incomplete'\n","08/23/2025 16:18:32 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/e01e3a1bca00ae47bca8326b38cc397729f87481\n","08/23/2025 16:18:32 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/1c00fc78bd9cf90108046bc433cb34992480f1c1/tokenizer.json\n","08/23/2025 16:18:32 - transformers.tokenization_utils_base - INFO - loading file tokenizer.model from cache at None\n","08/23/2025 16:18:32 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/23/2025 16:18:32 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/1c00fc78bd9cf90108046bc433cb34992480f1c1/special_tokens_map.json\n","08/23/2025 16:18:32 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/1c00fc78bd9cf90108046bc433cb34992480f1c1/tokenizer_config.json\n","08/23/2025 16:18:32 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/1c00fc78bd9cf90108046bc433cb34992480f1c1/chat_template.jinja\n","08/23/2025 16:18:33 - transformers.tokenization_utils_base - INFO - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","08/23/2025 16:18:34 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/89ad848b8bbebe0ae085847b7b2e010a4aa1b408.incomplete'\n","08/23/2025 16:18:34 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/89ad848b8bbebe0ae085847b7b2e010a4aa1b408\n","08/23/2025 16:18:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/1c00fc78bd9cf90108046bc433cb34992480f1c1/config.json\n","08/23/2025 16:18:34 - transformers.configuration_utils - INFO - Model config SmolLM3Config {\n","  \"architectures\": [\n","    \"SmolLM3ForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128012,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 11008,\n","  \"layer_types\": [\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\",\n","    \"full_attention\"\n","  ],\n","  \"max_position_embeddings\": 65536,\n","  \"max_window_layers\": 28,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"smollm3\",\n","  \"no_rope_layer_interval\": 4,\n","  \"no_rope_layers\": [\n","    1,\n","    1,\n","    1,\n","    0,\n","    1,\n","    1,\n","    1,\n","    0,\n","    1,\n","    1,\n","    1,\n","    0,\n","    1,\n","    1,\n","    1,\n","    0,\n","    1,\n","    1,\n","    1,\n","    0,\n","    1,\n","    1,\n","    1,\n","    0,\n","    1,\n","    1,\n","    1,\n","    0,\n","    1,\n","    1,\n","    1,\n","    0,\n","    1,\n","    1,\n","    1,\n","    0\n","  ],\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 36,\n","  \"num_key_value_heads\": 4,\n","  \"pad_token_id\": 128004,\n","  \"pretraining_tp\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 5000000.0,\n","  \"sliding_window\": null,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"use_cache\": false,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 128256\n","}\n","\n","08/23/2025 16:18:35 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors.index.json' to '/root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/ee6c028c012814d385be51c1d537c345c9fa9457.incomplete'\n","08/23/2025 16:18:35 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/ee6c028c012814d385be51c1d537c345c9fa9457\n","08/23/2025 16:18:35 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/1c00fc78bd9cf90108046bc433cb34992480f1c1/model.safetensors.index.json\n","08/23/2025 16:18:36 - huggingface_hub.file_download - INFO - Downloading 'model-00001-of-00002.safetensors' to '/root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/e5eed4113a925264c33e3cdf76cd9f547c06a207ef2270914a7274a2bf685ffd.incomplete'\n","08/23/2025 16:18:36 - huggingface_hub.file_download - INFO - Downloading 'model-00002-of-00002.safetensors' to '/root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/6c3ad90646457295723e4da5ee8afac47006e20407b122df5174693dd8d68a43.incomplete'\n","08/23/2025 16:19:48 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/6c3ad90646457295723e4da5ee8afac47006e20407b122df5174693dd8d68a43\n","08/23/2025 16:21:24 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/blobs/e5eed4113a925264c33e3cdf76cd9f547c06a207ef2270914a7274a2bf685ffd\n","08/23/2025 16:21:24 - transformers.modeling_utils - INFO - Instantiating SmolLM3ForCausalLM model under default dtype torch.float32.\n","08/23/2025 16:21:24 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": 128012,\n","  \"pad_token_id\": 128004,\n","  \"use_cache\": false\n","}\n","\n","08/23/2025 16:21:49 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing SmolLM3ForCausalLM.\n","\n","08/23/2025 16:21:49 - transformers.modeling_utils - INFO - All the weights of SmolLM3ForCausalLM were initialized from the model checkpoint at HuggingFaceTB/SmolLM3-3B.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use SmolLM3ForCausalLM for predictions without further training.\n","08/23/2025 16:21:50 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/1c00fc78bd9cf90108046bc433cb34992480f1c1/generation_config.json\n","08/23/2025 16:21:50 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 128012,\n","  \"pad_token_id\": 128004,\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.95\n","}\n","\n","08/23/2025 16:21:50 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/1c00fc78bd9cf90108046bc433cb34992480f1c1/generation_config.json\n","08/23/2025 16:21:50 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 128012,\n","  \"pad_token_id\": 128004,\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.95\n","}\n","\n","08/23/2025 16:21:50 - mods.testBench - INFO - Results file is expected to have 2 rows.\n","08/23/2025 16:21:51 - mods.testBench - INFO - Starting experiment in TestBench with experiment_id=HuggingFaceTB-SmolLM3-3B-23-082025_16-21-51\n","08/23/2025 16:21:51 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/1c00fc78bd9cf90108046bc433cb34992480f1c1/generation_config.json\n","08/23/2025 16:21:51 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 128012,\n","  \"pad_token_id\": 128004,\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.95\n","}\n","\n","08/23/2025 16:21:51 - mods.testBench - INFO - Ref_row:105 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/23/2025 16:22:10 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/23/2025 16:22:10 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/23/2025 16:22:10 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/23/2025 16:22:10 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/23/2025 16:22:10 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/23/2025 16:22:10 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/23/2025 16:22:10 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/23/2025 16:22:10 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/23/2025 16:22:10 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/23/2025 16:22:10 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/23/2025 16:22:10 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/23/2025 16:22:11 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/23/2025 16:22:11 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/23/2025 16:22:11 - absl - INFO - Using default tokenizer.\n","08/23/2025 16:22:11 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/rouge/default/default_experiment-1-0.arrow\n","08/23/2025 16:22:11 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bleu/default/default_experiment-1-0.arrow\n","08/23/2025 16:22:12 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--HuggingFaceTB--SmolLM3-3B/snapshots/1c00fc78bd9cf90108046bc433cb34992480f1c1/generation_config.json\n","08/23/2025 16:22:12 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": 128012,\n","  \"pad_token_id\": 128004,\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.95\n","}\n","\n","08/23/2025 16:22:12 - mods.testBench - INFO - Ref_row:106 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/23/2025 16:22:27 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/23/2025 16:22:27 - absl - INFO - Using default tokenizer.\n","08/23/2025 16:22:27 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/rouge/default/default_experiment-1-0.arrow\n","08/23/2025 16:22:27 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bleu/default/default_experiment-1-0.arrow\n","08/23/2025 16:22:27 - mods.dataHandler - INFO - Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench/tb-HuggingFaceTB-SmolLM3-3B-23-082025_16-21-51.xlsx\n","08/23/2025 16:22:27 - mods.dataHandler - INFO - Removing tmp file /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench/tmp-HuggingFaceTB-SmolLM3-3B-23-082025_16-21-51.csv\n","08/23/2025 16:22:27 - mods.testBench - INFO - Ending experiment in TestBench with experiment_id=HuggingFaceTB-SmolLM3-3B-23-082025_16-21-51\n","08/23/2025 16:22:28 - __main__ - INFO - reportParamGridSearch time --- 4.246193015575409 minutes ---\n","08/23/2025 16:22:28 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/23/2025 16:22:28 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n"]}],"source":["!cat app/logs/logfile.log"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"i79nllcxdMwy","executionInfo":{"status":"ok","timestamp":1755966377328,"user_tz":-120,"elapsed":3109,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"outputs":[],"source":["!python projectSetup.py"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13175,"status":"ok","timestamp":1755966390511,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"Srd-wh7zg8Qt","outputId":"46f803e7-e3eb-40b4-90b0-8c1994db83d4"},"outputs":[{"output_type":"stream","name":"stderr","text":["08/23/2025 16:26:30 - app.mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=gpt2\n","08/23/2025 16:26:30 - app.mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=gpt2\n","08/23/2025 16:26:30 - app.mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=gpt2\n"]}],"source":["import sys, os\n","import torch\n","from pathlib import Path\n","sys.path.append(os.getcwd())\n","sys.path.append(os.getcwd() + '/app')\n","\n","from app.mods.promptGenerator import PromptGenerator\n","from app.mods.modelLoader import ModelLoader\n","from app.mods.reportGenerator import ReportGenerator\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","torch_dtype = torch.float32 if torch.cuda.is_available() else torch.float32\n","\n","ml = ModelLoader(model_id=\"gpt2\", device=device, torch_dtype=torch_dtype)"]},{"cell_type":"markdown","metadata":{"id":"rfGNcNvTUhdj"},"source":["## microsoft/phi-2\n","It allocates 12 GB in RAM, the extra depends on the number of workers"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"P57w5lE7nDyX","executionInfo":{"status":"ok","timestamp":1755966390520,"user_tz":-120,"elapsed":5,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}},"collapsed":true},"outputs":[],"source":["# !python app/reportParamGridSearch.py --model_id microsoft/phi-2 --non-threaded --prompt_method B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 2 --temperature 1.0 --top_p 0.9 1 --top_k 50 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","metadata":{"id":"vVTm54F1UaHq"},"source":["## HuggingFaceTB/SmolLM3-3B\n","It allocates 14 GB in RAM"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"DdF2u_DS5WaO","collapsed":true,"executionInfo":{"status":"ok","timestamp":1755966390541,"user_tz":-120,"elapsed":9,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"outputs":[],"source":["# !python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM3-3B  --non-threaded --prompt_method A B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","metadata":{"id":"GBWVQWjZUUrm"},"source":["## GPT2-XL\n","It allocates 7.2 GB in RAM"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1755966390550,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"XmhpRs4v6SBP","collapsed":true},"outputs":[],"source":["# !python app/reportParamGridSearch.py --model_id openai-community/gpt2-xl --non-threaded --prompt_method B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 2  --temperature 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 30 50 70 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","source":["## Qwen/Qwen2.5-0.5B-Instruct"],"metadata":{"id":"7XvVPx1EOtm-"}},{"cell_type":"markdown","source":["It allocates 2.7 GB in RAM"],"metadata":{"id":"f-BjaRZzO_Ar"}},{"cell_type":"code","source":["# try:\n","#     !python app/reportParamGridSearch.py --model_id Qwen/Qwen2.5-0.5B-Instruct --non-threaded --prompt_method A B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True --repetition_penalty 1.0\n","# except Exception as e:\n","#     print(e)\n","#     # KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","#     from google.colab import runtime\n","#     runtime.unassign()\n"],"metadata":{"collapsed":true,"id":"JJdiTKC3PCzm","executionInfo":{"status":"ok","timestamp":1755966390617,"user_tz":-120,"elapsed":63,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["## Qwen/Qwen2.5-0.5B-Instruct - TEST SET (idx 81 to 106)"],"metadata":{"id":"zAR-2ByNnf7r"}},{"cell_type":"code","source":["try:\n","    !python app/reportParamGridSearch.py --model_id Qwen/Qwen2.5-0.5B-Instruct --non-threaded --prompt_method C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 81 --end_idx 106  --temperature 1.0 --top_p 0.6 --top_k 50 --max_new_tokens 300 --do_sample True --repetition_penalty 1.0\n","except Exception as e:\n","    print(e)\n","    # KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","    from google.colab import runtime\n","    runtime.unassign()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1755966662506,"user_tz":-120,"elapsed":271897,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}},"outputId":"d8a4279b-90b0-426b-a43a-f4b23cceeb21","id":"H2MfGTZWnf7w"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-08-23 16:26:36.905605: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755966396.925168   14128 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755966396.931085   14128 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755966396.946772   14128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755966396.946795   14128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755966396.946801   14128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755966396.946804   14128 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['Qwen/Qwen2.5-0.5B-Instruct'], 'prompt_method': ['C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [81], 'end_idx': [106], 'temperature': [1.0], 'top_p': [0.6], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True], 'repetition_penalty': [1.0]}\n","08/23/2025 16:27:05 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=Qwen/Qwen2.5-0.5B-Instruct\n","08/23/2025 16:27:05 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=Qwen/Qwen2.5-0.5B-Instruct\n","08/23/2025 16:27:05 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=Qwen/Qwen2.5-0.5B-Instruct\n","Generation parameters: \n","{'temperature': [1.0], 'top_p': [0.6], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True], 'repetition_penalty': [1.0]}\n","Results file is expected to have 26 rows.\n","******* Starting GRID SEARCH ***********\n","******* Starting NOT THREADED PROCESS ************\n","Ref_row:81 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.53it/s]\n","Ref_row:82 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.61it/s]\n","Ref_row:83 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.13it/s]\n","Ref_row:84 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 78.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.98it/s]\n","Ref_row:85 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.10it/s]\n","Ref_row:86 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.16it/s]\n","Ref_row:87 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 93.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.58it/s]\n","Ref_row:88 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.07it/s]\n","Ref_row:89 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.91it/s]\n","Ref_row:90 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 90.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.22it/s]\n","Ref_row:91 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 100.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.73it/s]\n","Ref_row:92 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.44it/s]\n","Ref_row:93 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.77it/s]\n","Ref_row:94 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.33it/s]\n","Ref_row:95 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.45it/s]\n","Ref_row:96 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 88.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.03it/s]\n","Ref_row:97 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.25it/s]\n","Ref_row:98 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.93it/s]\n","Ref_row:99 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 95.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 98.96it/s]\n","Ref_row:100 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.97it/s]\n","Ref_row:101 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.64it/s]\n","Ref_row:102 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.29it/s]\n","Ref_row:103 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 94.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.51it/s]\n","Ref_row:104 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.59it/s]\n","Ref_row:105 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.82it/s]\n","Ref_row:106 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.69it/s]\n","***** Starting statistical analyisis for the experiment_id=Qwen-Qwen2.5-0.5B-Instruct-23-082025_16-27-13 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-Qwen-Qwen2.5-0.5B-Instruct-23-082025_16-27-13.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_C-Qwen-Qwen2.5-0.5B-Instruct-23-082025_16-27-13.xlsx\n","reportParamGridSearch time --- 4.276087959607442 minutes ---\n"]}]},{"cell_type":"markdown","source":["## meta-llama/Llama-3.2-1B-Instruct - TEST SET (idx 81 to 106)"],"metadata":{"id":"-Ilirw7LqXSi"}},{"cell_type":"code","source":["from huggingface_hub import login\n","login(token=userdata.get('HF_TOKEN'))  # insert your Hugging Face token here\n","\n","try:\n","    !python app/reportParamGridSearch.py --model_id meta-llama/Llama-3.2-1B-Instruct --non-threaded --prompt_method C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 81 --end_idx 106  --temperature 1.0 --top_p 0.3 --top_k 50 --max_new_tokens 300 --do_sample True --repetition_penalty 1.0\n","except Exception as e:\n","    print(e)\n","    # KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","    from google.colab import runtime\n","    runtime.unassign()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1755966868016,"user_tz":-120,"elapsed":205505,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}},"outputId":"78f2b11e-7da8-49f6-9973-12e43d8d041c","id":"Hs3-uF_5qXSm"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-08-23 16:31:13.411534: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755966673.432001   15400 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755966673.437985   15400 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755966673.453042   15400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755966673.453067   15400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755966673.453070   15400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755966673.453073   15400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['meta-llama/Llama-3.2-1B-Instruct'], 'prompt_method': ['C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [81], 'end_idx': [106], 'temperature': [1.0], 'top_p': [0.3], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True], 'repetition_penalty': [1.0]}\n","08/23/2025 16:31:32 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=meta-llama/Llama-3.2-1B-Instruct\n","08/23/2025 16:31:32 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=meta-llama/Llama-3.2-1B-Instruct\n","08/23/2025 16:31:32 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=meta-llama/Llama-3.2-1B-Instruct\n","Generation parameters: \n","{'temperature': [1.0], 'top_p': [0.3], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True], 'repetition_penalty': [1.0]}\n","Results file is expected to have 26 rows.\n","******* Starting GRID SEARCH ***********\n","******* Starting NOT THREADED PROCESS ************\n","Ref_row:81 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.41it/s]\n","Ref_row:82 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.41it/s]\n","Ref_row:83 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.97it/s]\n","Ref_row:84 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.10it/s]\n","Ref_row:85 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 103.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.69it/s]\n","Ref_row:86 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.74it/s]\n","Ref_row:87 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.06it/s]\n","Ref_row:88 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.04it/s]\n","Ref_row:89 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.03it/s]\n","Ref_row:90 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.54it/s]\n","Ref_row:91 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.38it/s]\n","Ref_row:92 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.30it/s]\n","Ref_row:93 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 110.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.90it/s]\n","Ref_row:94 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.16it/s]\n","Ref_row:95 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.18it/s]\n","Ref_row:96 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.47it/s]\n","Ref_row:97 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.67it/s]\n","Ref_row:98 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 107.44it/s]\n","Ref_row:99 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 81.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.96it/s]\n","Ref_row:100 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.95it/s]\n","Ref_row:101 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.33it/s]\n","Ref_row:102 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.42it/s]\n","Ref_row:103 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.09it/s]\n","Ref_row:104 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.28it/s]\n","Ref_row:105 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.11it/s]\n","Ref_row:106 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.97it/s]\n","***** Starting statistical analyisis for the experiment_id=meta-llama-Llama-3.2-1B-Instruct-23-082025_16-31-41 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-meta-llama-Llama-3.2-1B-Instruct-23-082025_16-31-41.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_C-meta-llama-Llama-3.2-1B-Instruct-23-082025_16-31-41.xlsx\n","reportParamGridSearch time --- 3.0848061720530193 minutes ---\n"]}]},{"cell_type":"markdown","source":["## meta-llama/Llama-3.2-3B-Instruct - TEST SET (idx 81 to 106)"],"metadata":{"id":"S8irBEIbs38Z"}},{"cell_type":"code","source":["from huggingface_hub import login\n","login(token=userdata.get('HF_TOKEN'))  # insert your Hugging Face token here\n","\n","try:\n","    !python app/reportParamGridSearch.py --model_id meta-llama/Llama-3.2-3B-Instruct --non-threaded --prompt_method C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 81 --end_idx 106  --temperature 1.0 --top_p 0.6 --top_k 50 --max_new_tokens 300 --do_sample True --repetition_penalty 1.0\n","except Exception as e:\n","    print(e)\n","    # KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","    from google.colab import runtime\n","    runtime.unassign()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1755967245932,"user_tz":-120,"elapsed":377904,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}},"outputId":"8c3f0282-8685-4652-dc20-742f67bb7f75","id":"JPHRAuD5snFE"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-08-23 16:34:34.924478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755966874.958099   16342 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755966874.969029   16342 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755966874.993783   16342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755966874.993815   16342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755966874.993823   16342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755966874.993829   16342 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['meta-llama/Llama-3.2-3B-Instruct'], 'prompt_method': ['C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [81], 'end_idx': [106], 'temperature': [1.0], 'top_p': [0.6], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True], 'repetition_penalty': [1.0]}\n","08/23/2025 16:34:54 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=meta-llama/Llama-3.2-3B-Instruct\n","08/23/2025 16:34:54 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=meta-llama/Llama-3.2-3B-Instruct\n","08/23/2025 16:34:54 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=meta-llama/Llama-3.2-3B-Instruct\n","Generation parameters: \n","{'temperature': [1.0], 'top_p': [0.6], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True], 'repetition_penalty': [1.0]}\n","Loading checkpoint shards: 100% 2/2 [00:24<00:00, 12.26s/it]\n","Results file is expected to have 26 rows.\n","******* Starting GRID SEARCH ***********\n","******* Starting NOT THREADED PROCESS ************\n","Ref_row:81 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.11it/s]\n","Ref_row:82 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.96it/s]\n","Ref_row:83 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.26it/s]\n","Ref_row:84 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.71it/s]\n","Ref_row:85 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 111.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.57it/s]\n","Ref_row:86 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.46it/s]\n","Ref_row:87 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.30it/s]\n","Ref_row:88 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.20it/s]\n","Ref_row:89 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.43it/s]\n","Ref_row:90 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.35it/s]\n","Ref_row:91 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.75it/s]\n","Ref_row:92 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 101.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.52it/s]\n","Ref_row:93 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 98.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 102.07it/s]\n","Ref_row:94 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.59it/s]\n","Ref_row:95 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.84it/s]\n","Ref_row:96 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.17it/s]\n","Ref_row:97 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.47it/s]\n","Ref_row:98 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 92.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.23it/s]\n","Ref_row:99 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.83it/s]\n","Ref_row:100 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.66it/s]\n","Ref_row:101 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.16it/s]\n","Ref_row:102 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.95it/s]\n","Ref_row:103 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.05it/s]\n","Ref_row:104 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.70it/s]\n","Ref_row:105 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.53it/s]\n","Ref_row:106 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.99it/s]\n","***** Starting statistical analyisis for the experiment_id=meta-llama-Llama-3.2-3B-Instruct-23-082025_16-35-21 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-meta-llama-Llama-3.2-3B-Instruct-23-082025_16-35-21.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_C-meta-llama-Llama-3.2-3B-Instruct-23-082025_16-35-21.xlsx\n","reportParamGridSearch time --- 6.010721604029338 minutes ---\n"]}]},{"cell_type":"markdown","source":["## HuggingFaceTB/SmolLM3-3B - TEST SET (idx 81 to 106)"],"metadata":{"id":"wFdUKaSntnBZ"}},{"cell_type":"code","source":["try:\n","    !python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM3-3B --non-threaded --prompt_method B --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 81 --end_idx 106  --temperature 1.0 --top_p 0.3 --top_k 50 --max_new_tokens 300 --do_sample True --repetition_penalty 1.0\n","except Exception as e:\n","    print(e)\n","    # KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","    from google.colab import runtime\n","    runtime.unassign()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"outputId":"4c82ee1e-da82-4ea0-87ec-e650b2ce3423","id":"4dnH5DSftnBb","executionInfo":{"status":"ok","timestamp":1755967700028,"user_tz":-120,"elapsed":454092,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-08-23 16:40:51.133792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755967251.154423   18027 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755967251.160668   18027 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755967251.175975   18027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755967251.175999   18027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755967251.176003   18027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755967251.176008   18027 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['HuggingFaceTB/SmolLM3-3B'], 'prompt_method': ['B'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [81], 'end_idx': [106], 'temperature': [1.0], 'top_p': [0.3], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True], 'repetition_penalty': [1.0]}\n","08/23/2025 16:41:11 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","08/23/2025 16:41:11 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","08/23/2025 16:41:11 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","Generation parameters: \n","{'temperature': [1.0], 'top_p': [0.3], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True], 'repetition_penalty': [1.0]}\n","Loading checkpoint shards: 100% 2/2 [00:29<00:00, 14.77s/it]\n","Results file is expected to have 26 rows.\n","******* Starting GRID SEARCH ***********\n","******* Starting NOT THREADED PROCESS ************\n","Ref_row:81 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.18it/s]\n","Ref_row:82 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.05it/s]\n","Ref_row:83 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 103.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.63it/s]\n","Ref_row:84 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.64it/s]\n","Ref_row:85 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.07it/s]\n","Ref_row:86 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.68it/s]\n","Ref_row:87 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.30it/s]\n","Ref_row:88 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.61it/s]\n","Ref_row:89 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.41it/s]\n","Ref_row:90 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 108.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.55it/s]\n","Ref_row:91 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 92.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.02it/s]\n","Ref_row:92 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.88it/s]\n","Ref_row:93 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.77it/s]\n","Ref_row:94 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.69it/s]\n","Ref_row:95 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.36it/s]\n","Ref_row:96 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.05it/s]\n","Ref_row:97 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.12it/s]\n","Ref_row:98 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.86it/s]\n","Ref_row:99 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 89.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.43it/s]\n","Ref_row:100 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 107.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.29it/s]\n","Ref_row:101 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.41it/s]\n","Ref_row:102 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.95it/s]\n","Ref_row:103 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.47it/s]\n","Ref_row:104 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.49it/s]\n","Ref_row:105 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 74.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 92.70it/s]\n","Ref_row:106 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.07it/s]\n","***** Starting statistical analyisis for the experiment_id=HuggingFaceTB-SmolLM3-3B-23-082025_16-41-43 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-HuggingFaceTB-SmolLM3-3B-23-082025_16-41-43.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_B-HuggingFaceTB-SmolLM3-3B-23-082025_16-41-43.xlsx\n","reportParamGridSearch time --- 7.31360832452774 minutes ---\n"]}]},{"cell_type":"markdown","metadata":{"id":"JozIHMbjZGtI"},"source":["# Several models run in parallel\n","In linux command with the & we can run several programs in parallel\n","**Careful it fills GPU RAM quickly if models are > 2 B parameters**\n","\n","**NOTA:** Better use the Colab L4 GPU for charging two models in parallel. The L4 GPU size is 25 GB, so it could admit up to 5B parameters more or less, i.e.:\n","\n","- two models of at most 2.5 B parameters.\n","- Three models of 1 B parameters each\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"yCGd0oBQZbTL","executionInfo":{"status":"ok","timestamp":1755967700037,"user_tz":-120,"elapsed":4,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"outputs":[],"source":["# !python app/reportParamGridSearch.py --model_id microsoft/phi-2 --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True & python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM3-3B --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"j0P1LHHOXp0d","executionInfo":{"status":"ok","timestamp":1755967700395,"user_tz":-120,"elapsed":350,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"outputs":[],"source":["# KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
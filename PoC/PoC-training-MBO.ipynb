{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["9hVgw73hO9eE","3fvH32ZaHEOo","F2m43xRsHKlW"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9d2fb75975f149f1b2a7f7e02a6e5169":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_834628935560405e91003b3150351223","IPY_MODEL_1d9107efc4e949d0a2b9384d60ace9f4","IPY_MODEL_c274e82a329a4e6e8fca7f7137487cfe"],"layout":"IPY_MODEL_66c16833e56e40f18d81e5f45ac28d75"}},"834628935560405e91003b3150351223":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1df76c4cc7ac46c3b456c8188e4e8393","placeholder":"​","style":"IPY_MODEL_9fcbdc1d4f2f430b84f4aec4b5a8be61","value":"Tokenizing &amp; masking: 100%"}},"1d9107efc4e949d0a2b9384d60ace9f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4727c343f2d414398ef819cf4dcba99","max":699,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a78951bd3644f9295215b89fe527039","value":699}},"c274e82a329a4e6e8fca7f7137487cfe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74406139930845adb23bea6c26f50d05","placeholder":"​","style":"IPY_MODEL_b9ca57eae0c949bd9313c88da8939790","value":" 699/699 [00:01&lt;00:00, 503.48 examples/s]"}},"66c16833e56e40f18d81e5f45ac28d75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1df76c4cc7ac46c3b456c8188e4e8393":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fcbdc1d4f2f430b84f4aec4b5a8be61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4727c343f2d414398ef819cf4dcba99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a78951bd3644f9295215b89fe527039":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74406139930845adb23bea6c26f50d05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9ca57eae0c949bd9313c88da8939790":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5c70ad981ca74b53b9bf82ec25a0e5d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_287eb529bbe248e994dac8d19f263b4a","IPY_MODEL_9554edd9226c46a492261f302422c96a","IPY_MODEL_9f1ef326143647a4943cc7054a2b687d"],"layout":"IPY_MODEL_d3a5e940bd9f4247b3047ab44b74411a"}},"287eb529bbe248e994dac8d19f263b4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbee85542c724f8cb0734100be8b5358","placeholder":"​","style":"IPY_MODEL_bb89b8498a9648c0bdef95ca7cf0f539","value":"Tokenizing &amp; masking: 100%"}},"9554edd9226c46a492261f302422c96a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c5dd4a6dcf44902a25f1d1ff8141f62","max":70,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5e35e356703c43f59e7ad64ae5af4565","value":70}},"9f1ef326143647a4943cc7054a2b687d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_708fc1be3e4b4356bf6b253e04fb0ab4","placeholder":"​","style":"IPY_MODEL_721f8ba24ffc483dbcc13ae583bb1dab","value":" 70/70 [00:00&lt;00:00, 341.47 examples/s]"}},"d3a5e940bd9f4247b3047ab44b74411a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbee85542c724f8cb0734100be8b5358":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb89b8498a9648c0bdef95ca7cf0f539":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c5dd4a6dcf44902a25f1d1ff8141f62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e35e356703c43f59e7ad64ae5af4565":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"708fc1be3e4b4356bf6b253e04fb0ab4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"721f8ba24ffc483dbcc13ae583bb1dab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1bddcd8d70346c8b273958adfd5c8c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_647f8049edd746e3a4b3b69d57c21660","IPY_MODEL_1ef617ddb496492d9da30bc39f4aecca","IPY_MODEL_3db8e785fd7e45ea8de15dffcc8f1d7a"],"layout":"IPY_MODEL_c0096cbb73184fbf873fcadfc3b5c0bf"}},"647f8049edd746e3a4b3b69d57c21660":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0b320d94a4c453f9828513871a75845","placeholder":"​","style":"IPY_MODEL_380b81d269a74e00b55e4ac2c8ba8fb3","value":"Truncating train dataset: 100%"}},"1ef617ddb496492d9da30bc39f4aecca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fc060b3181741d5840abb2a1c488744","max":699,"min":0,"orientation":"horizontal","style":"IPY_MODEL_40983bbdbfd14e5ca29696ddbf14b870","value":699}},"3db8e785fd7e45ea8de15dffcc8f1d7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24975f88be964369b346696c1072371a","placeholder":"​","style":"IPY_MODEL_7baf7aad542844a3abc6664ec986f4a5","value":" 699/699 [00:00&lt;00:00, 17120.81 examples/s]"}},"c0096cbb73184fbf873fcadfc3b5c0bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0b320d94a4c453f9828513871a75845":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"380b81d269a74e00b55e4ac2c8ba8fb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0fc060b3181741d5840abb2a1c488744":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40983bbdbfd14e5ca29696ddbf14b870":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"24975f88be964369b346696c1072371a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7baf7aad542844a3abc6664ec986f4a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ddda5f87aab841ce8ebecbf93227ace8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c45fddc4dd44cb1bc3dcd871aa98636","IPY_MODEL_adb2dd99a623442fb1d88923abfc7309","IPY_MODEL_b6bf841659514ef58669971fde235d4f"],"layout":"IPY_MODEL_4bb1f704911447849e0c66b00256fa97"}},"7c45fddc4dd44cb1bc3dcd871aa98636":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f9d09b9c4244560855d2d07afebe29c","placeholder":"​","style":"IPY_MODEL_a6a136298bda43108b7a79e96cfae56a","value":"Truncating eval dataset: 100%"}},"adb2dd99a623442fb1d88923abfc7309":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e06a60caf2a4a629c647c395d65f2ee","max":70,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e1bca00910d74ab2baeb72226a16cc23","value":70}},"b6bf841659514ef58669971fde235d4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1240f2e4fb648b1b2a6ab18ba8462de","placeholder":"​","style":"IPY_MODEL_36f949fe6cf24d3894480e40bc715c52","value":" 70/70 [00:00&lt;00:00, 1620.60 examples/s]"}},"4bb1f704911447849e0c66b00256fa97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f9d09b9c4244560855d2d07afebe29c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6a136298bda43108b7a79e96cfae56a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e06a60caf2a4a629c647c395d65f2ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1bca00910d74ab2baeb72226a16cc23":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1240f2e4fb648b1b2a6ab18ba8462de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36f949fe6cf24d3894480e40bc715c52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea6941737d8145b9b9c501659e420bf8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_88650bc49be3454ebc5bd9eb06f8501e","IPY_MODEL_4529118387174ce985140ad3721d6e1d","IPY_MODEL_95ce068fdec240f1b9bf0cff17889382"],"layout":"IPY_MODEL_e38372e7a300406489f079d5750dcfde"}},"88650bc49be3454ebc5bd9eb06f8501e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c2b64c8e3124a708bf5f7ae7e4e9be6","placeholder":"​","style":"IPY_MODEL_b4708293b9b3453c94c5101da406118a","value":"Tokenizing &amp; masking: 100%"}},"4529118387174ce985140ad3721d6e1d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eab108ebdee44f32a3979c591b000263","max":699,"min":0,"orientation":"horizontal","style":"IPY_MODEL_91fc12cdbde54b5f8ec58a43dce0f6ee","value":699}},"95ce068fdec240f1b9bf0cff17889382":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8425f10735aa47e48b7a0974dd54df12","placeholder":"​","style":"IPY_MODEL_37e1760f990e4540ac1c15077f54252b","value":" 699/699 [00:00&lt;00:00, 862.48 examples/s]"}},"e38372e7a300406489f079d5750dcfde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c2b64c8e3124a708bf5f7ae7e4e9be6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4708293b9b3453c94c5101da406118a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eab108ebdee44f32a3979c591b000263":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91fc12cdbde54b5f8ec58a43dce0f6ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8425f10735aa47e48b7a0974dd54df12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37e1760f990e4540ac1c15077f54252b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9aa40a1d2f91420d8a799aa9ef16c518":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ea216d7e34a0471591ca84fc3c0d8fcf","IPY_MODEL_e86211f7fb334416964dae9281d9a927","IPY_MODEL_18899107ce0040b0909cd4eb465cbd31"],"layout":"IPY_MODEL_ed7f9b388169439da7ab751564db8079"}},"ea216d7e34a0471591ca84fc3c0d8fcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_462f3b06688b4388aa336abde8ef93a5","placeholder":"​","style":"IPY_MODEL_d74661f85e3147afae0011fec7758c6d","value":"Tokenizing &amp; masking: 100%"}},"e86211f7fb334416964dae9281d9a927":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24b47caf925f47b3828e59c9b83e4986","max":70,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7860f8a306d34ab7a7aba51dc5ed2067","value":70}},"18899107ce0040b0909cd4eb465cbd31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09c6b845888743a0afb6cbb0541ada02","placeholder":"​","style":"IPY_MODEL_998ac8edf4a14fa4b2dd3226f396a7fa","value":" 70/70 [00:00&lt;00:00, 655.95 examples/s]"}},"ed7f9b388169439da7ab751564db8079":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"462f3b06688b4388aa336abde8ef93a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d74661f85e3147afae0011fec7758c6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24b47caf925f47b3828e59c9b83e4986":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7860f8a306d34ab7a7aba51dc5ed2067":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09c6b845888743a0afb6cbb0541ada02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"998ac8edf4a14fa4b2dd3226f396a7fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e8b55ff80a9495db5ba2d62d34e52af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d74c9681d914be3a3265519e066be9a","IPY_MODEL_d42ac0e177de4c8488fc33e965cd8fce","IPY_MODEL_0dfaf8f46379455b9f01664dc7a3919c"],"layout":"IPY_MODEL_9f416bf6a7cd437292a997aebb4dfda4"}},"8d74c9681d914be3a3265519e066be9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2467c26d6cd74ffdadf4610866430f30","placeholder":"​","style":"IPY_MODEL_27d55f87b8cd415a8021da1b1f7066dd","value":"Truncating train dataset: 100%"}},"d42ac0e177de4c8488fc33e965cd8fce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f7ea352e86347c889df984a45cdca26","max":699,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0e7376bf2f243a59bd9ef69f977633b","value":699}},"0dfaf8f46379455b9f01664dc7a3919c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dde0863df7148c399d12093f7b21e26","placeholder":"​","style":"IPY_MODEL_cdf80895248f4899a4eff388b6f0e8cd","value":" 699/699 [00:00&lt;00:00, 32256.78 examples/s]"}},"9f416bf6a7cd437292a997aebb4dfda4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2467c26d6cd74ffdadf4610866430f30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27d55f87b8cd415a8021da1b1f7066dd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f7ea352e86347c889df984a45cdca26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0e7376bf2f243a59bd9ef69f977633b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0dde0863df7148c399d12093f7b21e26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdf80895248f4899a4eff388b6f0e8cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cf9f1b7b78242ef8318bd1f6c6bed3d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33f443d4c6af4ce38b29939aa4e79b2e","IPY_MODEL_e1395738da784d5892f9da4d44717f7e","IPY_MODEL_352f6801f7b84a25858cf2f1520e56a2"],"layout":"IPY_MODEL_557ef272a26e4dfea0247b3b8e494d1e"}},"33f443d4c6af4ce38b29939aa4e79b2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23daa801e69546dba6ef8d56a026f060","placeholder":"​","style":"IPY_MODEL_29d959855613445d86c99fd3014775bc","value":"Truncating eval dataset: 100%"}},"e1395738da784d5892f9da4d44717f7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ad306609ae94966814101ac0a77ed1c","max":70,"min":0,"orientation":"horizontal","style":"IPY_MODEL_998f7b6baa4541d7813a29e067067c0d","value":70}},"352f6801f7b84a25858cf2f1520e56a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf1e3522db2b498e915c6ea1b993e1c1","placeholder":"​","style":"IPY_MODEL_0a6d690420eb4fb98130f007ce82d5f8","value":" 70/70 [00:00&lt;00:00, 3736.48 examples/s]"}},"557ef272a26e4dfea0247b3b8e494d1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23daa801e69546dba6ef8d56a026f060":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29d959855613445d86c99fd3014775bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ad306609ae94966814101ac0a77ed1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"998f7b6baa4541d7813a29e067067c0d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bf1e3522db2b498e915c6ea1b993e1c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a6d690420eb4fb98130f007ce82d5f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f3955d7b31940b88a3c20f03cc37d7e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5e73029e2f204b32b26373d976331908","IPY_MODEL_68eed5f556bb48d4b412b5f6ee13a48a","IPY_MODEL_fcd0239e5aa34779854c0a7f95c8bda6"],"layout":"IPY_MODEL_0c63923fb365429c825f05daeadbe052"}},"5e73029e2f204b32b26373d976331908":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdb8ecb14d674c55bf93efa2b29577f4","placeholder":"​","style":"IPY_MODEL_77ea4ab8683d4b86871badafd37bbc3c","value":"README.md: "}},"68eed5f556bb48d4b412b5f6ee13a48a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7064b72f1ba43baa8267e24aa1abbfd","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3a12514a79f48b0a7aef4886bb5b4fb","value":1}},"fcd0239e5aa34779854c0a7f95c8bda6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fc775adf9eb4dff8ca09b975f900726","placeholder":"​","style":"IPY_MODEL_a2821e899a4e431aabd5a4e481350b8e","value":" 1.37k/? [00:00&lt;00:00, 126kB/s]"}},"0c63923fb365429c825f05daeadbe052":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdb8ecb14d674c55bf93efa2b29577f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77ea4ab8683d4b86871badafd37bbc3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7064b72f1ba43baa8267e24aa1abbfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"e3a12514a79f48b0a7aef4886bb5b4fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fc775adf9eb4dff8ca09b975f900726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2821e899a4e431aabd5a4e481350b8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfe1b34c5f514a0dac2a787a16644257":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8179daaf591a411cbaedf086dae0bb1f","IPY_MODEL_01b641d5e2f9491e986a201e0f8d598e","IPY_MODEL_1d66d0949a294c9ab623357cafd373cf"],"layout":"IPY_MODEL_2f39e91e1d714a299fba0cd582429c87"}},"8179daaf591a411cbaedf086dae0bb1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b4edf81ec094ce2a4045cd8b4bf4dda","placeholder":"​","style":"IPY_MODEL_46788614d5e44feab2e5bae05376c314","value":"Generating train split: 100%"}},"01b641d5e2f9491e986a201e0f8d598e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e596172de8be404abc489858d02873fd","max":629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96a943beffe04005bc5e6a4cbddfcda9","value":629}},"1d66d0949a294c9ab623357cafd373cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c93edc81cc1b40bfa10ccfe646373aba","placeholder":"​","style":"IPY_MODEL_7b077a04d4d74c5e8fd4c52beddecad2","value":" 629/629 [00:00&lt;00:00, 25734.69 examples/s]"}},"2f39e91e1d714a299fba0cd582429c87":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b4edf81ec094ce2a4045cd8b4bf4dda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46788614d5e44feab2e5bae05376c314":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e596172de8be404abc489858d02873fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96a943beffe04005bc5e6a4cbddfcda9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c93edc81cc1b40bfa10ccfe646373aba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b077a04d4d74c5e8fd4c52beddecad2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1620ea556544a6c8acfd78a7125f62c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f99c5c813c41427e86cdaa0f53b476ba","IPY_MODEL_2de32f9c6c7e4e4992005102f8d998ab","IPY_MODEL_da2d28de28df4d498cf242685f53f631"],"layout":"IPY_MODEL_10f19fd7dad24d5e9cae26ecf83184fc"}},"f99c5c813c41427e86cdaa0f53b476ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_513c9adcb8584fefb605754ca9b4979a","placeholder":"​","style":"IPY_MODEL_b683683cf7324940ab8a15b0b2d156d3","value":"Generating eval split: 100%"}},"2de32f9c6c7e4e4992005102f8d998ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79adbf86e69440e18d3481d5491a41fe","max":70,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d859c82d6b7d4e7bb4d94faab95a07dc","value":70}},"da2d28de28df4d498cf242685f53f631":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59a626cbd9c44595bea8f1871b215f19","placeholder":"​","style":"IPY_MODEL_4cf95c02e28344eb9948a5a363e7af79","value":" 70/70 [00:00&lt;00:00, 4061.21 examples/s]"}},"10f19fd7dad24d5e9cae26ecf83184fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"513c9adcb8584fefb605754ca9b4979a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b683683cf7324940ab8a15b0b2d156d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"79adbf86e69440e18d3481d5491a41fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d859c82d6b7d4e7bb4d94faab95a07dc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"59a626cbd9c44595bea8f1871b215f19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cf95c02e28344eb9948a5a363e7af79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ec67e019e1b49e79a5905fc7fdd933a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fafc44a2b2994802b5629afa0cb2c00c","IPY_MODEL_f19be4d7853a4715aa8354a011f94753","IPY_MODEL_294876a233304acd98608c0fbcf0591e"],"layout":"IPY_MODEL_3d42e0de4451401c93586ef042e87e16"}},"fafc44a2b2994802b5629afa0cb2c00c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2a43aa1d3864932a88324dfae1fdf2d","placeholder":"​","style":"IPY_MODEL_2766e48d0ceb4cfca1fc10ead9940e25","value":"Generating test split: 100%"}},"f19be4d7853a4715aa8354a011f94753":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44b8544d0fe042c28e76ca86c3dfdce2","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_042cb6e5374c4226823122bfcdd23268","value":100}},"294876a233304acd98608c0fbcf0591e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b2dfd811f374b93b3507530d38142f0","placeholder":"​","style":"IPY_MODEL_830e29a9e3154c408b1427ddcbc054d5","value":" 100/100 [00:00&lt;00:00, 6224.76 examples/s]"}},"3d42e0de4451401c93586ef042e87e16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2a43aa1d3864932a88324dfae1fdf2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2766e48d0ceb4cfca1fc10ead9940e25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44b8544d0fe042c28e76ca86c3dfdce2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"042cb6e5374c4226823122bfcdd23268":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b2dfd811f374b93b3507530d38142f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"830e29a9e3154c408b1427ddcbc054d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee5ad9316c8543409ea0d8d622a3c9d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45fa41cdd1f445439fc71814b71ce336","IPY_MODEL_6dfb8092a8ac4f588d7940260b931447","IPY_MODEL_fd54eceff7fd41acb5d2173394aadec0"],"layout":"IPY_MODEL_0a61ff8086cc4bf590d84e210b7c3c96"}},"45fa41cdd1f445439fc71814b71ce336":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13e1f3a18fb94d56a15220b726723f7d","placeholder":"​","style":"IPY_MODEL_68e0f21d83bf409a94cc40f9a1ad90df","value":"Filter: 100%"}},"6dfb8092a8ac4f588d7940260b931447":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5d499ed227b4f07b1b98f6debcbc7b4","max":629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d5c0462214d049de84fdcb373ff4cd4e","value":629}},"fd54eceff7fd41acb5d2173394aadec0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b07e844235f4a6f9fff1453937f426a","placeholder":"​","style":"IPY_MODEL_c299e31505f54e0d8d0ca0d743784ad0","value":" 629/629 [00:00&lt;00:00, 29109.11 examples/s]"}},"0a61ff8086cc4bf590d84e210b7c3c96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13e1f3a18fb94d56a15220b726723f7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68e0f21d83bf409a94cc40f9a1ad90df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5d499ed227b4f07b1b98f6debcbc7b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5c0462214d049de84fdcb373ff4cd4e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b07e844235f4a6f9fff1453937f426a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c299e31505f54e0d8d0ca0d743784ad0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3678f31c03454a2e88c3ea79000e2894":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6251b141f11f4e6fabf5f5ebc978bc6e","IPY_MODEL_cf2c1f560392431db9cacbbf98880c53","IPY_MODEL_b91764e78ef44daea319152308875330"],"layout":"IPY_MODEL_62e17a5c178d4b4580147afd83e9cbe2"}},"6251b141f11f4e6fabf5f5ebc978bc6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9faca9bba92435db1c015df89160cbe","placeholder":"​","style":"IPY_MODEL_25a2e054a95341049e0aa660ca6c9100","value":"Filter: 100%"}},"cf2c1f560392431db9cacbbf98880c53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e230a7b4dab14e44bbbba24baa7536e9","max":70,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cfd6edcaaf3d42d2b83489ac23dc504a","value":70}},"b91764e78ef44daea319152308875330":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d7d783233604ffb99f88a4dc1429c89","placeholder":"​","style":"IPY_MODEL_090c231acc0d4580b41c31737f905486","value":" 70/70 [00:00&lt;00:00, 4331.94 examples/s]"}},"62e17a5c178d4b4580147afd83e9cbe2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9faca9bba92435db1c015df89160cbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25a2e054a95341049e0aa660ca6c9100":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e230a7b4dab14e44bbbba24baa7536e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfd6edcaaf3d42d2b83489ac23dc504a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2d7d783233604ffb99f88a4dc1429c89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"090c231acc0d4580b41c31737f905486":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34221b33cc6342b1974d41224c2313ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1abba2b6dead401f948c489d9dbfc314","IPY_MODEL_f0ec01458309413c9918d8793e6dca1f","IPY_MODEL_adfe24d773234e64b3bcf04e0a92dea9"],"layout":"IPY_MODEL_755d4bb30cf04a54b980d2cc5221333d"}},"1abba2b6dead401f948c489d9dbfc314":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c05d83ba95f4b42a3211039155ac40f","placeholder":"​","style":"IPY_MODEL_2f20a9bed21e496b99f3d67ead3d085c","value":"Building demo prompts + tokenizing: 100%"}},"f0ec01458309413c9918d8793e6dca1f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_320c2405de9542bdbeed61ce8b4e2502","max":629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_85085fe742304e4296465b3a7b456521","value":629}},"adfe24d773234e64b3bcf04e0a92dea9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3020ec3f1504c48a556ae735d0daded","placeholder":"​","style":"IPY_MODEL_1d48424b99d34ebcad586a864b30dce4","value":" 629/629 [00:01&lt;00:00, 521.42 examples/s]"}},"755d4bb30cf04a54b980d2cc5221333d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c05d83ba95f4b42a3211039155ac40f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f20a9bed21e496b99f3d67ead3d085c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"320c2405de9542bdbeed61ce8b4e2502":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85085fe742304e4296465b3a7b456521":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a3020ec3f1504c48a556ae735d0daded":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d48424b99d34ebcad586a864b30dce4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ae6a5cd7f7544b39993e0074a989609":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c5e0b54492b49a29d51f71b2acf3397","IPY_MODEL_2ba0bcaa3f2741d99b4992346dd6043b","IPY_MODEL_e338c8db649741da85aa4ac4042d5e68"],"layout":"IPY_MODEL_d9772f9508f545f2a1a9548e3f6bf42c"}},"1c5e0b54492b49a29d51f71b2acf3397":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8a019cca42e4e7fbad796905e0a3fd6","placeholder":"​","style":"IPY_MODEL_04ed73601af84e8cb1bfac9c0c6e0caf","value":"Building demo prompts + tokenizing: 100%"}},"2ba0bcaa3f2741d99b4992346dd6043b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0c964dcbc0e47018d05ac186a890c65","max":70,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2cd0d447d174fcfa2935113fee5c46a","value":70}},"e338c8db649741da85aa4ac4042d5e68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b5455e65f7f4dc2809f68bd23c33c42","placeholder":"​","style":"IPY_MODEL_d52ee59bfa934b7c850964e086910232","value":" 70/70 [00:00&lt;00:00, 445.12 examples/s]"}},"d9772f9508f545f2a1a9548e3f6bf42c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8a019cca42e4e7fbad796905e0a3fd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04ed73601af84e8cb1bfac9c0c6e0caf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0c964dcbc0e47018d05ac186a890c65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2cd0d447d174fcfa2935113fee5c46a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b5455e65f7f4dc2809f68bd23c33c42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d52ee59bfa934b7c850964e086910232":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a410a2e9f883484a8f84910315a71a81":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2d80875d41e94d9581d6ba6044616af3","IPY_MODEL_43968543077b4910aab421e56d36de2b","IPY_MODEL_137137c3ed244bf98d7d639a6eee89d7"],"layout":"IPY_MODEL_12e147e080c54929bf79a586ebd3a949"}},"2d80875d41e94d9581d6ba6044616af3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_456089ff792d4aa98ba3bb24d2c1aa72","placeholder":"​","style":"IPY_MODEL_ff6a6972a668411985fc3dd66e6e8fc8","value":"Truncating train dataset: 100%"}},"43968543077b4910aab421e56d36de2b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9939fec4506d47bf83e640ef804b7bf3","max":629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a1f69d118950406e92a7fa40542eca08","value":629}},"137137c3ed244bf98d7d639a6eee89d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c922645a595407bbecfb3524ffd2ce2","placeholder":"​","style":"IPY_MODEL_52a89939eccd4fdcb45c8b76c843f32c","value":" 629/629 [00:00&lt;00:00, 22731.89 examples/s]"}},"12e147e080c54929bf79a586ebd3a949":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"456089ff792d4aa98ba3bb24d2c1aa72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff6a6972a668411985fc3dd66e6e8fc8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9939fec4506d47bf83e640ef804b7bf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1f69d118950406e92a7fa40542eca08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7c922645a595407bbecfb3524ffd2ce2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52a89939eccd4fdcb45c8b76c843f32c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6752ad14c46845798b4d4020983d1095":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2f16014482354aa998975952ebb5b194","IPY_MODEL_d74eb6eaebd74e4c91b839814a9489af","IPY_MODEL_3bffd23b965f4c6db389c6142247d09a"],"layout":"IPY_MODEL_0c0db42ad7784ac0935c2dc0e8680315"}},"2f16014482354aa998975952ebb5b194":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb46eaf1702443ddadd5ba501107ddd6","placeholder":"​","style":"IPY_MODEL_b86194217e19459ead9b146e3cf8ec74","value":"Truncating eval dataset: 100%"}},"d74eb6eaebd74e4c91b839814a9489af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_297ff762be66413d8a32de669d531d20","max":70,"min":0,"orientation":"horizontal","style":"IPY_MODEL_132adf59220942cc97a43a45e4aabb56","value":70}},"3bffd23b965f4c6db389c6142247d09a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f623871eeb87428a8161f9e801182d93","placeholder":"​","style":"IPY_MODEL_e3accf199a9c4ed99294164f6bcf3c1d","value":" 70/70 [00:00&lt;00:00, 3583.26 examples/s]"}},"0c0db42ad7784ac0935c2dc0e8680315":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb46eaf1702443ddadd5ba501107ddd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b86194217e19459ead9b146e3cf8ec74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"297ff762be66413d8a32de669d531d20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"132adf59220942cc97a43a45e4aabb56":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f623871eeb87428a8161f9e801182d93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3accf199a9c4ed99294164f6bcf3c1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd7347b1fef64ed2800fe78b4eb2ca21":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_450e99a9840844cd874f093c96c7c72c","IPY_MODEL_85c46d970cb84846947254af56e908bb","IPY_MODEL_167621dfe72a4c87ab78e9484449204d"],"layout":"IPY_MODEL_09ba44c732f0421ead65ee868abb1503"}},"450e99a9840844cd874f093c96c7c72c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76bce8aa02cd458ba21f2b2afde354cd","placeholder":"​","style":"IPY_MODEL_904ae34ae22f493ba76cf32f3f43b20a","value":"Processing Files (1 / 1)                : 100%"}},"85c46d970cb84846947254af56e908bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9c7e39f383240a0bc250ff6d15d9037","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a631efc69a346e78f2da5698397ede4","value":1}},"167621dfe72a4c87ab78e9484449204d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dfacef2f66648c7baa9deadc8655518","placeholder":"​","style":"IPY_MODEL_c65d0e74180a4d828396ea43ca483a1b","value":"  352MB /  352MB, 60.6MB/s  "}},"09ba44c732f0421ead65ee868abb1503":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76bce8aa02cd458ba21f2b2afde354cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"904ae34ae22f493ba76cf32f3f43b20a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9c7e39f383240a0bc250ff6d15d9037":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"6a631efc69a346e78f2da5698397ede4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8dfacef2f66648c7baa9deadc8655518":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c65d0e74180a4d828396ea43ca483a1b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1e3b4045ce04a7ba5f5316d88cb7cb6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12fe40417c554b57bbf7ed5fb7201bd5","IPY_MODEL_3099f441a0a74fa1bca8cd241c8c2f40","IPY_MODEL_9dd4bd663a864af1b19b9bf9935607c2"],"layout":"IPY_MODEL_b27a5627d5244030a21c388020427fc7"}},"12fe40417c554b57bbf7ed5fb7201bd5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_996f290b188644eead15e2b74215abc9","placeholder":"​","style":"IPY_MODEL_f7a91f7fb0344525a1aaece9235e28ce","value":"New Data Upload                         : 100%"}},"3099f441a0a74fa1bca8cd241c8c2f40":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7c4a3cf4dbf4d6f94bb0fe322070109","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e5c46453fdbc4087844f90c13b5b9767","value":1}},"9dd4bd663a864af1b19b9bf9935607c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0930859dbc084bdab8301499ca8c7016","placeholder":"​","style":"IPY_MODEL_87932f18913c4ef4a396e060bc7f2500","value":" 64.9MB / 64.9MB, 11.2MB/s  "}},"b27a5627d5244030a21c388020427fc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"996f290b188644eead15e2b74215abc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7a91f7fb0344525a1aaece9235e28ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7c4a3cf4dbf4d6f94bb0fe322070109":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"e5c46453fdbc4087844f90c13b5b9767":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0930859dbc084bdab8301499ca8c7016":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87932f18913c4ef4a396e060bc7f2500":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21837e48aab94b2ba1d7f22afe9cd2dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_49a704bca81841e2818e591b4f6a7a5d","IPY_MODEL_155e64e593774f138bde469b60f9686d","IPY_MODEL_1b078da7e375483b865a3da68b4cb7cb"],"layout":"IPY_MODEL_7956413cbe49424294756b74f5bb5f98"}},"49a704bca81841e2818e591b4f6a7a5d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c79cf3d46a845338ed5c809050f5c63","placeholder":"​","style":"IPY_MODEL_dbf9cb7b141546b3b8536598e0863a41","value":"  ...para_qlora/merged/model.safetensors: 100%"}},"155e64e593774f138bde469b60f9686d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57a1eb2e1c664f9cb47be75d981589ff","max":351722820,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7d5db0328ea94753ade163a5362fdc72","value":351722820}},"1b078da7e375483b865a3da68b4cb7cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a6cebe50e3446b38dc3599c1eb8aa4f","placeholder":"​","style":"IPY_MODEL_6f16d59ab376463fa1f212160c6e2cef","value":"  352MB /  352MB            "}},"7956413cbe49424294756b74f5bb5f98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c79cf3d46a845338ed5c809050f5c63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbf9cb7b141546b3b8536598e0863a41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57a1eb2e1c664f9cb47be75d981589ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d5db0328ea94753ade163a5362fdc72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a6cebe50e3446b38dc3599c1eb8aa4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f16d59ab376463fa1f212160c6e2cef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g7ehcE6hOQKd","outputId":"b17a463e-60a3-4882-8283-fab2ced39be5","executionInfo":{"status":"ok","timestamp":1756401439492,"user_tz":-120,"elapsed":17678,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from google.colab import userdata\n","github_token = userdata.get('zbotta_token')\n","\n","token = github_token\n","username = \"zbotta\"\n","repo = 'reportingAgent'\n","%cd /content/drive/MyDrive/GitHub/{repo}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9EgSGSJOVWU","outputId":"9a0404f8-7490-422e-cd16-a3560d27eb97","executionInfo":{"status":"ok","timestamp":1756401440941,"user_tz":-120,"elapsed":1443,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}]},{"cell_type":"code","source":["!git config --global user.name \"zbotta\"\n","!git config --global user.email \"zbotta@proton.me\"\n","!git pull\n","!git checkout dev"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7QaVVbhpJ1q","executionInfo":{"status":"ok","timestamp":1756296463240,"user_tz":-120,"elapsed":1402,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"5fe770b5-d88b-430d-948f-a8a77d37604d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n","M\tPoC/PoC-training-MBO.ipynb\n","M\tapp/datasets/training/smollm2_360m_onepara_lora/adapter/adapter_config.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-20/adapter_config.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-20/special_tokens_map.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-20/tokenizer.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-20/tokenizer_config.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-20/trainer_state.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-20/vocab.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-22/adapter_config.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-22/special_tokens_map.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-22/tokenizer.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-22/tokenizer_config.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-22/trainer_state.json\n","D\tapp/datasets/training/smollm2_360m_onepara_lora/checkpoint-22/vocab.json\n","Already on 'dev'\n","Your branch is ahead of 'origin/dev' by 4 commits.\n","  (use \"git push\" to publish your local commits)\n"]}]},{"cell_type":"markdown","source":["# Testing models < 1B"],"metadata":{"id":"9hVgw73hO9eE"}},{"cell_type":"code","source":["!pip -q install -U \"transformers>=4.43\" \"accelerate>=0.33\" bitsandbytes"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PxfkShoHOUV-","executionInfo":{"status":"ok","timestamp":1755610286609,"user_tz":-120,"elapsed":118737,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"74c2ebfc-1dcd-4c94-9ea0-d72b24bb3ca5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["## Qwen 2.5-0.5B-Instruct"],"metadata":{"id":"iJ4X9JEhPBgY"}},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch, re\n","\n","MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n","\n","bnb_cfg = BitsAndBytesConfig(\n","    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, trust_remote_code=True)\n","if tok.pad_token_id is None:\n","    tok.pad_token = tok.eos_token\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_ID, quantization_config=bnb_cfg, device_map=\"auto\",\n","    torch_dtype=torch.float16, trust_remote_code=True\n",")\n","\n","SYSTEM_INSTR = (\n","  \"You are an incident-report generator.\\n\"\n","  \"Language: {lang_directive}\\n\"\n","  \"Write ONE SINGLE PARAGRAPH that includes ALL facts provided: what happened, when, where, who, how, why (root cause), and contingency/corrective actions. \"\n","  \"Constraints: neutral factual tone; no bullet points, no headings, no lists, no JSON; \"\n","  \"do NOT invent details; include only information given; output must be a single line with no line breaks; \"\n","  \"preserve numbers, times, names, and proper nouns; limit length to {max_chars} characters.\"\n",")\n","\n","def extract_lang(user_text:str):\n","    # Optional inline directive, e.g. \"Language: French\"\n","    m = re.search(r\"(?i)\\bLanguage\\s*:\\s*([A-Za-zÀ-ÿ \\-]+)\", user_text)\n","    return m.group(1).strip() if m else None\n","\n","def build_prompt(user_text, max_chars=400, lang=\"auto\"):\n","    inline = extract_lang(user_text)\n","    if inline:\n","        lang_directive = f\"write in {inline}\"\n","    elif lang and lang.lower() != \"auto\":\n","        lang_directive = f\"write in {lang}\"\n","    else:\n","        lang_directive = \"match the dominant language of the INPUT\"\n","\n","    return (\n","        SYSTEM_INSTR.format(lang_directive=lang_directive, max_chars=max_chars)\n","        + \"\\n\\nINPUT:\\n\" + user_text.strip()\n","        + f\"\\n\\nOUTPUT (single paragraph, ≤{max_chars} chars):\"\n","    )\n","\n","def _one_line(s: str) -> str:\n","    s = s.replace(\"\\n\", \" \")\n","    return re.sub(r\"\\s+\", \" \", s).strip()\n","\n","def _clip_paragraph(s: str, max_chars: int) -> str:\n","    if len(s) <= max_chars: return s\n","    clipped = s[:max_chars]\n","    end = max(clipped.rfind(\".\"), clipped.rfind(\"!\"), clipped.rfind(\"?\"))\n","    return clipped[:end+1] if end > 50 else clipped  # prefer a sentence end\n","\n","def generate_event_report(user_text, max_chars=400, max_new_tokens=260,\n","                          temperature=0.0, top_p=1.0, lang=\"auto\"):\n","    prompt = build_prompt(user_text, max_chars=max_chars, lang=lang)\n","    messages = [{\"role\":\"user\",\"content\":prompt}]\n","    input_ids = tok.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n","\n","    out = model.generate(\n","        input_ids, max_new_tokens=max_new_tokens,\n","        do_sample=(temperature>0), temperature=temperature, top_p=top_p,\n","        eos_token_id=tok.eos_token_id\n","    )\n","    gen_ids = out[0, input_ids.shape[-1]:]\n","    text = tok.decode(gen_ids, skip_special_tokens=True)\n","    text = _one_line(text)\n","    return _clip_paragraph(text, int(max_chars))\n","\n","\n"],"metadata":{"id":"ArswDhaUdJsG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tests several languages\n","\n","As Qwen model is multilingual, we can make a test of the output when the a language directive is done.\n","\n","This could be interesting to include in the APP deployment."],"metadata":{"id":"uS82CYPkx0LA"}},{"cell_type":"markdown","source":["#### ENGLISH"],"metadata":{"id":"xfug7wS8x2qh"}},{"cell_type":"code","source":["# Smoke test (English, auto)\n","example = \"\"\"What: Incorrect pH adjustment in buffer preparation\n","When: June 10, 2025, 9:15 AM\n","Where: Formulation Area, Production Building 2\n","Who: Rahul Mehta, Process Technician\n","How: pH meter not calibrated before use\n","Why: Technician skipped calibration step due to time pressure\n","ContingencyActions: Buffer batch discarded, technician retrained, equipment calibration logs reviewed\"\"\"\n","print(generate_event_report(example, lang=\"auto\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IY8HiLNgx6tn","executionInfo":{"status":"ok","timestamp":1755615557694,"user_tz":-120,"elapsed":13689,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"9476f4d7-a07b-4d62-c29a-d9a9bb12d39c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["On June 10, 2025, at 9:15 AM, during the process of preparing a buffer solution for a production batch, an incorrect pH adjustment was made. The pH meter had not been calibrated before its use, leading to an uncontrolled pH level. This oversight occurred after the technician had already started the preparation process without checking the calibration status.\n"]}]},{"cell_type":"markdown","source":["#### FRENCH"],"metadata":{"id":"uETmG4JWx5Ve"}},{"cell_type":"code","source":["# Smoke test with your 5W1H-style input:\n","example = \"\"\"What: Ajustement incorrect du pH lors de la préparation du tampon\n","When: 10 juin 2025, 9 h 15\n","Where: Zone de formulation, Bâtiment de production 2\n","Who: Rahul Mehta, technicien de procédé\n","How: pH-mètre non étalonné avant utilisation\n","Why: Le technicien a sauté l’étape d’étalonnage par manque de temps\n","ContingencyActions : Lot de tampons éliminé, technicien formé à nouveau, journaux d’étalonnage des équipements examinés\"\"\"\n","print(generate_event_report(example, temperature=0.0, lang=\"FRENCH\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xV2WqypAx6_Y","executionInfo":{"status":"ok","timestamp":1755615586006,"user_tz":-120,"elapsed":17439,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"55ee39b9-b288-4395-a568-a5c5071c2aee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Le pH incorrecte lors de la préparation du tampon a été détecté en juillet 2025, au sein de la zone de formation de la Bâtiment de Production 2, dans le Bâtiment de Production 2. L'ajout de pH-mètres n'était pas effectué avant cette utilisation. Le technicien de procédé, Rahul Mehta, s'est fait sauter l'étape d'étalonnage par manque de temps.\n"]}]},{"cell_type":"markdown","source":["#### SPANISH"],"metadata":{"id":"zWe3KCXNzfpY"}},{"cell_type":"code","source":["example = \"\"\"What: Ajuste incorrecto del pH en la preparación de la solución tampón\n","When: 10 de junio de 2025, 9:15 a. m.\n","Where: Área de Formulación, Edificio de Producción 2\n","Who: Rahul Mehta, Técnico de Procesos\n","How: El medidor de pH no se calibró antes de su uso\n","Why: El técnico omitió el paso de calibración por falta de tiempo\n","ContingencyActions : Se descartó el lote de solución tampón, se capacitó al técnico y se revisaron los registros de calibración del equipo\"\"\"\n","print(generate_event_report(example, temperature=0.0, lang=\"SPANISH\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IzkiIrY6zfIT","executionInfo":{"status":"ok","timestamp":1755615612065,"user_tz":-120,"elapsed":11024,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"8e330f65-46cf-49e4-d509-2c4fa3c20355"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Un ajuste incorrecto del pH en la preparación de la solución tampón ocurrió el 10 de junio de 2025, a las 9:15 a. m., en el área de Formulación del edificio de producción 2. El medidor de pH no se había calibrado antes de su uso. El técnico Rahul Mehta, un técnico de procesos, omitió el paso de calibración por falta de tiempo. La causa fue la falta de tiempo para realizar la calificación correcta.\n"]}]},{"cell_type":"code","source":["!pip install evaluate sentence_transformers numpy bert_score rouge_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34e0NWzeiCZG","executionInfo":{"status":"ok","timestamp":1755610472759,"user_tz":-120,"elapsed":5863,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"f19ca2ea-f1f9-41c1-ebc3-a617dbfa623b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n","Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (5.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n","Collecting rouge_score\n","  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.34.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.55.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.16.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.3.0)\n","Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.14.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.7)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=23294bdcc1c45770cc05ac0f21d137877906e92c73c07d67a150ccd752b633c9\n","  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}]},{"cell_type":"code","source":["REF = \"On June 10, 2025, at 9:15 AM in the Formulation Area (Production Building 2), technician Rahul Mehta used a non-calibrated pH meter to adjust the buffer, leading to an incorrect pH. The calibration step was skipped due to time pressure. The buffer batch was discarded, Rahul was retrained, and calibration logs were reviewed to prevent recurrence.\"\n","PRED = \"On June 10, 2025, the buffer preparation process at the Production Building 2 of the Formulation Area encountered an incorrect pH adjustment. The pH meter had not been calibrated before its use, leading to an unadjusted pH value. This oversight resulted in a significant deviation from the desired pH range, causing a critical safety hazard.\"\n","PRED2 = \"On June 10, 2025, at 9:15 AM, the buffer preparation process for batch number 4667 failed due to incorrect pH adjustment in the buffer preparation area of the production building. The technician, Rahul Mehta, had been tasked with preparing a buffer solution, but he had not performed a pH correction step as per his calibration schedule.\"\n","PRED3 = \"On June 10, 2025, at 9:15 AM in the production area of Building 2, the buffer preparation team conducted batch #16, a solution containing sodium hydroxide, under the supervision of Master Technician Rahul Mehta, on process control measures. Initially, they expected pH readings within the specified range of 3.8 to 4.3. After checking, they noticed that the pH meters were uncalibrated.\"\n","PRED4 = \"On June 10, 2025, at 9:15 AM, the buffer preparation process at the Production Building 2 of the Formulation Area encountered an incorrect pH adjustment in the buffer solution. The pH meter had not been calibrated before its use, leading to an uncontrolled pH level. This oversight resulted in a significant deviation from the desired pH range, causing a potential safety hazard.\"\n","#at 9:15 AM,\n","import sys, os\n","from pathlib import Path\n","sys.path.append(os.getcwd())\n","sys.path.append(os.getcwd() + '/app')\n","\n","from app.mods.metricsEvaluator import MetricsEvaluator\n","\n","me = MetricsEvaluator()"],"metadata":{"id":"XW-pNV37dW62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["me.set_cross_encoder_score(REF, [PRED])\n","print(me.get_cross_encoder_score())\n","me.set_cross_encoder_score(REF, [PRED2])\n","print(me.get_cross_encoder_score())\n","me.set_cross_encoder_score(REF, [PRED3])\n","print(me.get_cross_encoder_score())\n","me.set_cross_encoder_score(REF, [PRED4])\n","print(me.get_cross_encoder_score())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0ITAp1Ar7Cp","executionInfo":{"status":"ok","timestamp":1755615705039,"user_tz":-120,"elapsed":112,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"b4d11518-858e-4dcb-c7f6-efca215a5018"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.]\n","[1.]\n","[1.]\n","[1.]\n"]}]},{"cell_type":"code","source":["me.set_bi_encoder_score(REF, [PRED], is_test_bench=False)\n","print(me.get_bi_encoder_score())\n","me.set_bi_encoder_score(REF, [PRED2])\n","print(me.get_bi_encoder_score())\n","me.set_bi_encoder_score(REF, [PRED3])\n","print(me.get_bi_encoder_score())\n","me.set_bi_encoder_score(REF, [PRED4])\n","print(me.get_bi_encoder_score())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6sEh30lAiuI6","executionInfo":{"status":"ok","timestamp":1755614179566,"user_tz":-120,"elapsed":85,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"510ead90-9c06-44f1-ee7f-5485a6664de5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.        0.7809284]\n","[1.         0.73692465]\n","[1.         0.72478765]\n","[1.        0.7777794]\n"]}]},{"cell_type":"code","source":["!python -V"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_HyboresflJ","executionInfo":{"status":"ok","timestamp":1755614309810,"user_tz":-120,"elapsed":178,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"2229f897-1647-4907-ac51-f6f08bf9fdf8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.11.13\n"]}]},{"cell_type":"code","source":["!pip -q install -r requirements_colab.txt\n","!pip install --upgrade torch torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":866},"id":"xs5Rm0jyxGWW","executionInfo":{"status":"ok","timestamp":1755614854464,"user_tz":-120,"elapsed":17854,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"e10cbf21-1698-4a82-fe71-177544c5218d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Collecting torchvision\n","  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.83)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.90)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.3.90)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.8.93)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.90)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.93)\n","Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1.3)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (75.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchvision\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.21.0+cu124\n","    Uninstalling torchvision-0.21.0+cu124:\n","      Successfully uninstalled torchvision-0.21.0+cu124\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torchvision-0.23.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torchvision"]},"id":"1560b2c3eac54c21a8eab145c32ea7b5"}},"metadata":{}}]},{"cell_type":"markdown","source":["# Grid Search on Tiny Models"],"metadata":{"id":"dJ1cStGxHAjJ"}},{"cell_type":"markdown","source":["## HuggingFaceTB/SmolLM2-360M-Instruct & HuggingFaceTB/SmolLM2-135M-Instruct\n","\n"],"metadata":{"id":"3fvH32ZaHEOo"}},{"cell_type":"code","source":["!python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM2-135M-Instruct  --non-threaded --prompt_method A B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True & python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM2-360M-Instruct  --non-threaded --prompt_method A B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCHjelWnGpP-","executionInfo":{"status":"ok","timestamp":1755658015432,"user_tz":-120,"elapsed":37828389,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"71c80291-e1be-4f8a-c37d-0a2718ccb304"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.23it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.22it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:44:48 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 653 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...Why?  Why?  Why?  Why? ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.06it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.21it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:45:20 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1636 [type=json_invalid, input_value='{\"title\": \"What was the ...t was the contamination', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.07it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:45:23 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1424 [type=json_invalid, input_value='{ \"title\": \"Report on th...wing information: what,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.98it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 53.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.31it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.67it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.96it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.67it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:46:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1211 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...of batch LQX-100 during', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 39.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.68it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.88it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:46:03 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1691 [type=json_invalid, input_value='{ \"title\": \"Report on In...ncident was impacted by', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.94it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.27it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 78.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.76it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.24it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.25it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 69.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.71it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.24it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.11it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.28it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 82.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.98it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:46:47 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1578 [type=json_invalid, input_value='{ \"title\": \"Report on Br... back to the laboratory', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 54.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.85it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:47:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1050 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...rce assessment, March 3', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 42.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.36it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.48it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:47:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1427 [type=json_invalid, input_value='{ \"title\": \"Report on ev... container. Both of the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.84it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:47:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 370 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...00000000000000000000000', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.33it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.36it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:47:55 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 319 [type=json_invalid, input_value='{ \"title\": \"Report on Ev...-00-00-00-00-00-00-00-0', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 80.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.61it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.54it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 75.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.01it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:48:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1507 [type=json_invalid, input_value='{\"title\": \"What was the ...days. The investigation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 72.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.68it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 74.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.67it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.05it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.52it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.46it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 79.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.04it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:49:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1317 [type=json_invalid, input_value='{\"title\": \"What was the ...product sample after 09', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 59.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.69it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.99it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.75it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:49:10 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1525 [type=json_invalid, input_value='{ \"title\": \"Report on th...e samples were found to', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 56.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.07it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.56it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.60it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.69it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:49:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 319 [type=json_invalid, input_value='{ \"title\": \"Report on Ev...-00-00-00-00-00-00-00-0', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 45.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.12it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.76it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.56it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.45it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.75it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.05it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:50:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1609 [type=json_invalid, input_value='{ \"title\": \"Report on In...r was informed that the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 25.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.78it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.30it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 92.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.39it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.73it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 82.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.00it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.92it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.68it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.75it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.82it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.60it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 111.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.52it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.33it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 57.64it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:51:55 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1006 [type=json_invalid, input_value='{\"title\": \"What was the ...or?  Why was the error?', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 60.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.22it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.84it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.71it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.09it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:52:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1006 [type=json_invalid, input_value='{\"title\": \"What was the ...or?  Why was the error?', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 73.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.69it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.35it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.31it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.71it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:53:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 397 [type=json_invalid, input_value='{\"title\": \"What was the ...:20 16:20 16:20 16:20 1', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 63.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.41it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.71it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 87.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.81it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.86it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.62it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:53:36 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1002 [type=json_invalid, input_value='{\"title\": \"Documentation...en, and when, and when,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 64.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.05it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.81it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.23it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.14it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.33it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.41it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.52it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.29it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.09it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.12it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:54:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1796 [type=json_invalid, input_value='{\"title\": \"What was the ...h production record was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 38.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.50it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.69it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.58it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.27it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.72it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 86.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.11it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.74it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.46it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.53it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.24it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 89.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.07it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.68it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.22it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.29it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.19it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 96.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.42it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.87it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.98it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.77it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.85it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:55:13 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1251 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...e error corrected?  Why', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 82.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.64it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.54it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.55it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.99it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.61it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.90it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:55:46 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1249 [type=json_invalid, input_value='{\"title\": \"What was the ... the human error?  What', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 31.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.98it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.00it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 73.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.43it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.24it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.70it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.66it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:56:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 484 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...discovery) 2025-01-15 1', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.37it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.07it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 73.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.98it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.17it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.32it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.12it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 82.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.45it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:57:02 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1095 [type=json_invalid, input_value='{\"title\": \"What was the ...ch manufacturing record', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 64.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.31it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 91.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.88it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.26it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.09it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 67.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.83it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.48it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 32.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.13it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:57:48 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1311 [type=json_invalid, input_value='{ \"title\": \"documentatio...nted or downloaded on a', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches:   0% 0/1 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 42.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.73it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.38it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.07it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.82it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 86.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.48it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.58it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 75.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.62it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 36.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.92it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.27it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.90it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.15it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.07it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.42it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.17it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.99it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.04it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.44it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.41it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 73.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.04it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:59:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1064 [type=json_invalid, input_value='{ \"title\": \"Report on pH...dition of the acid/base', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 60.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.22it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 68.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.70it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 43.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.14it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.22it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.70it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.09it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.17it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 00:59:49 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1343 [type=json_invalid, input_value='{ \"title\": \"Report on pH....  The deviation report', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 56.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.84it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 71.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.91it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 43.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.80it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.97it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.74it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.79it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.66it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.17it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:00:52 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1298 [type=json_invalid, input_value='{\"title\": \"What was the ...e consequence?  Why was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 40.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.34it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.49it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.43it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.09it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.05it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:01:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1298 [type=json_invalid, input_value='{\"title\": \"What was the ...e consequence?  Why was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 38.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.32it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.89it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.77it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:01:57 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1491 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...r the malfunction? What', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 47.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.58it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.54it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.13it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.55it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:02:31 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1491 [type=json_invalid, input_value='{\"title\": \"What was the ...stigation, and what was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 61.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.12it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.75it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.28it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.44it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 74.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.88it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.13it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:03:20 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1582 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...deviation investigation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 48.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.80it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.70it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.20it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.67it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.04it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.09it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.68it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.83it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.39it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.35it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.73it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.29it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:04:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1063 [type=json_invalid, input_value='{\"title\": \"report\", \"rep...s 20250610, why: faulty', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 75.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.46it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.02it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.68it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.27it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.07it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.99it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.83it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:04:37 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 391 [type=json_invalid, input_value='{\"title\": \"What was the ...03:00-07:30, 03:00-07:3', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 83.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.37it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.93it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:05:08 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 363 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...44444444444444444444444', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.73it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:05:12 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1607 [type=json_invalid, input_value='{\"title\": \"What was the ... then inspected for any', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 63.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.47it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 53.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.23it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.89it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.07it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.38it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:05:59 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1571 [type=json_invalid, input_value='{ \"title\": \"Report on HV...ule, and Peter Carlson,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 37.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.48it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:06:07 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 899 [type=json_invalid, input_value='{\"title\": \"What was the ...corded at 03:00 and 07:', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 77.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.73it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:06:33 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1615 [type=json_invalid, input_value='{ \"title\": \"Report on HV...y deviation action. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 50.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.55it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.28it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:06:39 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1165 [type=json_invalid, input_value='{\"title\": \"What was the ...nt? The report is here.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 33.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.09it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.21it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.11it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.66it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.13it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:07:11 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1520 [type=json_invalid, input_value='{ \"title\": \"Report on HV...iation was initiated by', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.90it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.46it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.09it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.08it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:07:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1574 [type=json_invalid, input_value='{ \"title\": \"Report on HV...ated in response to the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.14it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.91it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.33it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.38it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.89it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.69it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:08:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1714 [type=json_invalid, input_value='{ \"title\": \"Report on HV...er for further analysis', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.75it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 80.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.22it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.80it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.04it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.27it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.97it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.11it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.28it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:09:20 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1478 [type=json_invalid, input_value='{\"title\":\"Report on HVAC...o the missing scheduled', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 58.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.76it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 80.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.28it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 83.96it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.69it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.38it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.35it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:09:55 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 694 [type=json_invalid, input_value='{\"title\": \"equipment cal...scovery), when: April 1', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 61.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.36it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.71it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.95it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.24it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:10:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1298 [type=json_invalid, input_value='{\"title\": \"equipment cal... approved recalibration', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 47.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.96it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 75.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.68it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.54it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.35it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:11:02 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1103 [type=json_invalid, input_value='{\"title\": \"What: Equipme...er Wong, QC Manager –', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 55.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.28it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.76it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.37it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.36it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 53.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.58it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 43.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.75it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 64.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.43it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:11:34 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 694 [type=json_invalid, input_value='{\"title\": \"equipment cal...scovery), when: April 1', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.31it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.59it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.08it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:12:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1821 [type=json_invalid, input_value='{ \"title\": \"Report on co...on source assessment is', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 40.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.65it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:12:23 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 694 [type=json_invalid, input_value='{\"title\": \"equipment cal...scovery), when: April 1', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 34.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.39it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.91it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:12:56 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 318 [type=json_invalid, input_value='{\"title\": \"A\", \"report\":...00000000000000000000000', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.57it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.66it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 82.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.84it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:13:05 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 504 [type=json_invalid, input_value='{ \"title\": \"Report on co...\\\\\"  \\\\\"\\\\\"  \\\\\"\\\\\"  \\\\', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 33.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.90it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 81.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.03it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 74.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.94it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 78.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.31it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.99it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.09it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.33it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 79.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.13it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.39it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.21it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 57.92it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.71it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.97it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.57it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.90it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.72it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.36it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 60.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.56it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.05it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:14:49 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1152 [type=json_invalid, input_value='{\"title\": \"Equipment Cal...port: QC Testing Report', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.31it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:14:51 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1817 [type=json_invalid, input_value='{ \"title\": \"Report on co...rce assessment is being', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 64.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.57it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.45it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.55it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 56.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.47it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.83it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:15:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1465 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...The calibration was re-', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 62.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.69it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:15:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1751 [type=json_invalid, input_value='{ \"title\": \"Report on co...n. The contamination is', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.48it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.50it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 49.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.72it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.87it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:16:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1269 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...n opened (Ref: DEV-2024', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.37it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.32it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.64it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:16:49 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1498 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...ration was re-evaluated', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 42.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.43it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 81.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.99it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.65it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.70it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:17:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1465 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...The calibration was re-', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 53.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.22it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.51it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.28it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:18:05 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1715 [type=json_invalid, input_value='{ \"title\": \"Contaminatio...tected during the batch', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 60.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.09it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:18:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1404 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ... re-calibrated, and the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.18it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 80.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.62it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:18:38 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1408 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...s re-calibrated and ret', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.53it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:18:55 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1641 [type=json_invalid, input_value='{ \"title\": \"Contaminatio...atch. The contamination', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.96it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 83.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.44it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:19:10 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1495 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...formed for the affected', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 47.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.25it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.88it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.23it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.65it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.84it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:19:51 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 536 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...00000000000000000000000', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.01it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.46it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.38it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.63it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.09it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.76it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.32it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.98it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 57.18it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.17it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.90it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 55.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.91it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:20:55 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 451 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo... (when) 2024-02-20 22:1', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 59.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.95it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:21:20 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 491 [type=json_invalid, input_value='{\"title\":\"Documentation ...621579 Batch No. 250621', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 32.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.06it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:21:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 381 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo... 2024-0239 2024-0239 20', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.18it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.87it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.74it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.09it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.61it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 66.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.91it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.17it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.00it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.76it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.15it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 57.42it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.97it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.68it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.89it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.79it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.26it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.75it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 73.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.93it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.30it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 73.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.95it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.99it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.30it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.16it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 80.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.68it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.27it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:23:03 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 392 [type=json_invalid, input_value='{\"title\": \"Failure of St...02-39, 2024-02-40, 2024', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 62.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.47it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.09it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.29it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.04it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:23:41 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 710 [type=json_invalid, input_value='{\"title\": \"Failure of St...scovery), when: May 20,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 61.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.93it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.35it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.26it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:23:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1409 [type=json_invalid, input_value='{ \"title\": \"Report on Ba...ction: Correct document', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 119.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.21it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.68it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.69it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.35it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.68it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.43it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.27it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:24:28 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 856 [type=json_invalid, input_value='{ \"title\": \"Accurate Des...still). Batch 818288 is', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 62.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.65it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.51it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:24:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1217 [type=json_invalid, input_value='{\"title\": \"Failure of st...ed; the autoclave cycle', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.00it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.46it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.80it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.50it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.42it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.70it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.32it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.25it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.66it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.58it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.61it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.78it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.30it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 89.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.93it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.98it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.96it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.94it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.07it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.07it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.86it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:27:01 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 647 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ....0.2.3: ERROR: [5000000', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 62.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.31it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:27:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1358 [type=json_invalid, input_value='{ \"title\": \"Documentatio... the fact that the same', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.90it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.88it/s]\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.25it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 76.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.90it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 73.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.81it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 36.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.41it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.35it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.07it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:27:46 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1469 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...erator retraining? What', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.76it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:28:13 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1562 [type=json_invalid, input_value='{ \"title\": \"Report on Te...gation was initiated to', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 62.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.48it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:28:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1226 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...appened.  What happened', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.93it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:28:47 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 757 [type=json_invalid, input_value='{\"title\":\"Temperature ex... Cooler 3 2025-02-10-03', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 70.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.50it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:28:53 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1248 [type=json_invalid, input_value='{\"title\": \"What happened...ed, and why it happened', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 36.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.98it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.44it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.23it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.30it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.16it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:29:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 920 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...d why, and why, and why', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.69it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.70it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 53.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.99it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:30:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1526 [type=json_invalid, input_value='{ \"title\": \"Report on Te...g system malfunctioning', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.99it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:30:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1803 [type=json_invalid, input_value='{\"title\": \"What was the ... training, the operator', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 40.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 57.38it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:30:36 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1435 [type=json_invalid, input_value='{ \"title\": \"Report on Te...M-89A. The material was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 87.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.79it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:30:54 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1418 [type=json_invalid, input_value='{\"title\": \"What was the ... document?  How was the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.37it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.26it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.10it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:31:30 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1539 [type=json_invalid, input_value='{\"title\": \"What was the ...omaly detected; anomaly', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.25it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:31:33 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1587 [type=json_invalid, input_value='{ \"title\": \"Report on Te...t further contamination', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 118.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.12it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.35it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 94.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.44it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.28it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:32:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1600 [type=json_invalid, input_value='{ \"title\": \"Report on Te...ontingency actions. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 48.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.11it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.95it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:32:19 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1394 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...gency actions should be', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 85.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.86it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.83it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.18it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.24it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.97it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:32:47 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1638 [type=json_invalid, input_value='{ \"title\": \"Report on Te...leted and the deviation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.24it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.28it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:33:13 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 489 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...overy) 2024-06-05 07:30', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 49.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.06it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.92it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.86it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.00it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:33:53 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1166 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...ining? 21. What was the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 62.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.15it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.42it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.16it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:33:58 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 593 [type=json_invalid, input_value='{ \"title\": \"Report on Te...\\\"\\\\\"\\\\\"  \\\\\"\\\\\"\\\\\"\\\\\" ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.93it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.01it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.17it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.88it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.27it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 79.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.35it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.28it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.38it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.73it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.95it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.75it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.62it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.39it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.56it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.30it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:35:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 812 [type=json_invalid, input_value='{ \"title\": \"Data\" ,\"repo...$  $$$$  $$$$  $$$$  $$', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.98it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.22it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.98it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.68it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.00it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.88it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 88.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.89it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.56it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.83it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.71it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:36:53 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 2064 [type=json_invalid, input_value='{\"title\": \"What was the ...n stopped. The operator', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 53.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.01it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.54it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.91it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:37:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1510 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo... was the reason for the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 64.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.21it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.41it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.87it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.43it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.79it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.93it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.33it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:38:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 601 [type=json_invalid, input_value='{\"title\": \"Report on the...T_HYDRATE_IN_DETECTION_', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.86it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.77it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.05it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 82.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.06it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.27it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.41it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 89.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.85it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.42it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:38:53 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1332 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo...scription?  Description', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.48it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.98it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.18it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.98it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:39:35 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1496 [type=json_invalid, input_value='{\"title\": \"What was the ...customer notification? ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 83.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.03it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:39:46 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1557 [type=json_invalid, input_value='{ \"title\": \"Report on Eq...lts showed no deviation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 80.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.79it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 89.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.30it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:40:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 472 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo...5:50 (discovery) 2024-0', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.86it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.74it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 28.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.20it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 81.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.53it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:40:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1475 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...ason for the reason for', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 61.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.69it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.41it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.37it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.62it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.24it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.27it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 90.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.79it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.44it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.55it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.94it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:41:31 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1829 [type=json_invalid, input_value='{ \"title\": \"Report on Eq...deviation investigation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.26it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.37it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.79it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.14it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.12it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:42:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1563 [type=json_invalid, input_value='{ \"title\": \"Report on Eq...retested. The deviation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 42.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.36it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.01it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.16it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:42:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1779 [type=json_invalid, input_value='{ \"title\": \"Report on Eq...ation investigation was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 63.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.18it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:42:55 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1461 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...ver. The correction was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.92it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.98it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:43:14 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1644 [type=json_invalid, input_value='{ \"title\": \"Report on Eq... was found to be minor.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 77.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.30it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.81it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.66it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.03it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.67it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.69it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:43:49 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1202 [type=json_invalid, input_value='{ \"title\": \"Report on Eq... 3, 2024. The deviation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 111.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.48it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.96it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.36it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.15it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.42it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 81.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.58it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:44:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1421 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...e HVAC failure caused a', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 51.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.15it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:44:31 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1627 [type=json_invalid, input_value='{ \"title\": \"Report on Eq...tion in the QC process,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 62.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.86it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.72it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.06it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.18it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.68it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.06it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:45:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1510 [type=json_invalid, input_value='{ \"title\": \"Report on Eq...tigation to investigate', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 73.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.32it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:45:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 387 [type=json_invalid, input_value='{\"title\": \"What: unexpec...0 20:00 20:00 20:00 20:', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.33it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.70it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.55it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.32it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.95it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 79.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.87it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:46:16 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1380 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...he cause of the failure', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 47.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.83it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.52it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.21it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:46:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1581 [type=json_invalid, input_value='{ \"title\": \"Calibration ... results reviewed, test', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 61.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.02it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.32it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.12it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.68it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:47:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 667 [type=json_invalid, input_value='{\"title\": \"Unexpected Fa...024 at 03:00 (discovery', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 82.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.33it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.43it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.17it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:47:28 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1581 [type=json_invalid, input_value='{ \"title\": \"Calibration ... results reviewed, test', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.32it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.83it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:47:48 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1167 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo...task? What was the goal', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.97it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.79it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 91.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.97it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.79it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.55it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 86.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.04it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.51it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:48:37 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1403 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...f the temperature spike', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.34it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.70it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.25it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.00it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:49:10 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1541 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...aulty thermostat, which', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 55.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.97it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.42it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.13it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.00it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.07it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 87.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.44it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:49:55 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1602 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...le from the chamber and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 61.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.75it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.43it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.87it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:50:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1453 [type=json_invalid, input_value='{ \"title\": \"Report on Fa...or malfunctioned during', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 52.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.41it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:50:37 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1520 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...e temperature deviation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 70.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.63it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:51:01 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 512 [type=json_invalid, input_value='{ \"title\": \"Report on Fa...\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 38.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.43it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:51:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1406 [type=json_invalid, input_value='{\"title\": \"Unexpected fa...is, Study Lead, and Dr.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.25it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.32it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.84it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.20it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:51:51 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1541 [type=json_invalid, input_value='{ \"title\": \"Report: Fail...tive action to be taken', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.34it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:51:57 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1408 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...ntified as the cause of', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 73.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.79it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.47it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.62it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.61it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.85it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.83it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.73it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:52:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 321 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...00000000000000000-00000', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 119.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.29it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.10it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.19it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.02it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.82it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:53:19 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 842 [type=json_invalid, input_value='{\"title\": \"What was the ... RM-VT203? What was the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 47.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.32it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.36it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.37it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.58it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:53:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1338 [type=json_invalid, input_value='{ \"title\": \"Report on Fa...e batch was quarantined', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.46it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.79it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 53.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.37it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.44it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.48it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.59it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.17it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:54:37 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1508 [type=json_invalid, input_value='{ \"title\": \"Report on Fa...on was initiated by the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 72.00it/s]\n","Batches:   0% 0/1 [00:00<?, ?it/s]08/20/2025 01:54:38 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1467 [type=json_invalid, input_value='{\"title\": \"What was the ...eviation investigation?', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 58.56it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.34it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:55:10 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1599 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...lier notification? What', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 56.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.32it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:55:11 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1491 [type=json_invalid, input_value='{ \"title\": \"Report on Fa.... The report is concise', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.28it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.81it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.07it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.81it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.05it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.59it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.01it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.04it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 65.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.30it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.82it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:55:59 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1373 [type=json_invalid, input_value='{ \"title\": \"Report of th...this case, please paste', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.36it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.82it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.56it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:56:33 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 529 [type=json_invalid, input_value='{\"title\": \"Report on the...10:30 (Discovery), when', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.73it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 86.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.09it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.74it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.54it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.12it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.53it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.82it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.07it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.38it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.22it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.83it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.24it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 73.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.95it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.65it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.29it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.04it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.20it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.70it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.61it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:58:26 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 721 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...</br>  </br>  </br>  </', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.79it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.16it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.57it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.05it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 89.34it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.36it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.57it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 86.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.18it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 01:59:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1424 [type=json_invalid, input_value='{\"title\": \"Not correct, ...h will go through. They', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 36.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.43it/s]\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.63it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.95it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 49.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.28it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.76it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:00:02 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1337 [type=json_invalid, input_value='{ \"title\": \"Report on De...\\'s name and the reason', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 44.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.97it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:00:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 574 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo...t 13:15 (Discovery) 202', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.46it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.54it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.55it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.75it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.43it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 89.52it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.76it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.47it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 67.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.23it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.04it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.40it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:01:01 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 323 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo...4000%; 4100%; 4200%; 43', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 41.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.74it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.80it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:01:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1487 [type=json_invalid, input_value='{ \"title\": \"Report on As...e taken. An Operator is', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 63.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.80it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.97it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:01:36 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1747 [type=json_invalid, input_value='{\"title\": \"Sample miside...ted labeling procedures', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 62.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.00it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.75it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.98it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.24it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.14it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.47it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.40it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.15it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.37it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.85it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 70.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.68it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.60it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.85it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 66.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.91it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:02:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1482 [type=json_invalid, input_value='{\"title\": \"What was the ...he labeling procedures.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 80.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.17it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 76.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.79it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.51it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.22it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.58it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.92it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.24it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.48it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.74it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.10it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.46it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.44it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.70it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:03:52 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1413 [type=json_invalid, input_value='{\"title\": \"Sample Miside...mistake in the labeling', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 89.01it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.12it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.91it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.67it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:04:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1216 [type=json_invalid, input_value='{\"title\":\"Sample misiden...rective measures; Other', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 64.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.52it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.11it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.25it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.86it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.00it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.62it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.62it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.73it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.16it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.73it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:05:35 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1350 [type=json_invalid, input_value='{ \"title\": \"Deviation in...ic Processing Suite G A', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.78it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.73it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 74.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.70it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 74.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.08it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.52it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.40it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.49it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.83it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.95it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.70it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.70it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.46it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.67it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.03it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.78it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.79it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.98it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.73it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.41it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.15it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 75.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.68it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.15it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.28it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.98it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.02it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 81.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.84it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.08it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:07:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 733 [type=json_invalid, input_value='{\"title\": \"What is the s...* * * * * * * * * * * *', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 52.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.47it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.74it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.69it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:07:55 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 952 [type=json_invalid, input_value='{\"title\": \"What was the ... lost? 21.  Why was the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 83.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.79it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.81it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.31it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.58it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.16it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.01it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.28it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.97it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.22it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.55it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.36it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.25it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.59it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.13it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.66it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.09it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:08:49 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1404 [type=json_invalid, input_value='{ \"title\": \"Report on Mi...t was not deviated from', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 32.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.40it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.79it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.71it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 75.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.48it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.55it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.19it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 53.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.35it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.73it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:09:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1402 [type=json_invalid, input_value='{ \"title\": \"Report on th...nitiated. 18. Deviation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.42it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.69it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.44it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.85it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.70it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:10:03 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1456 [type=json_invalid, input_value='{ \"title\": \"Report on mi...aking corrective action', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.87it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.26it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.22it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.90it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 82.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.47it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.78it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.80it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.47it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:10:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 485 [type=json_invalid, input_value='{\"title\": \"Failed to rec...58158158158158158158158', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.74it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.52it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.15it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.37it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.60it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.92it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.96it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.83it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 90.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.45it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.71it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.94it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.51it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.42it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.70it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:11:49 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1737 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo...upervisor\\'s supervisor', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.43it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.69it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.60it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.78it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:12:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 430 [type=json_invalid, input_value='{\"title\": \"What\", \"repor...-07-19 20:00:00.000.000', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 76.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.72it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 89.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.09it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.45it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.83it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.79it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.44it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 67.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.58it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:13:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 879 [type=json_invalid, input_value='{\"title\": \"The cleanup r...when, when, when, when,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 55.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.48it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.88it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.49it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:13:07 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1521 [type=json_invalid, input_value='{ \"title\": \"Report on un...ility impact assessment', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 81.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.93it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 81.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.30it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.13it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 96.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.05it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.68it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.77it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.02it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.77it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:13:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1455 [type=json_invalid, input_value='{ \"title\": \"Report on un...s from the chamber, log', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 26.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.51it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.16it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 80.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.17it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 89.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.84it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.31it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.12it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:14:16 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1614 [type=json_invalid, input_value='{ \"title\": \"Report on Un...iation was initiated as', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 73.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.61it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 49.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.34it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.07it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.66it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:14:39 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1802 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...ation investigation was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 58.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.05it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:15:02 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 658 [type=json_invalid, input_value='{ \"title\": \"Report on un...\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.12it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.65it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.94it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:15:13 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 528 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ....S.B. (Ref: DEV-2024-03', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.88it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.25it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 80.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.96it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 75.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.99it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:15:56 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1583 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...rminated. The batch was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.75it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:15:57 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 809 [type=json_invalid, input_value='{\"title\": \"Detailed Repo...\\\\n\\\\\\\\\\\\\\\\n\\\\\\\\\\\\\\\\n\\\\', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.51it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 49.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.45it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:16:31 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1448 [type=json_invalid, input_value='{ \"title\": \"Report on un... the initial assessment', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 63.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.09it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:16:38 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1935 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...tion was initiated. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.06it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:17:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1452 [type=json_invalid, input_value='{ \"title\": \"Report on un...as a result of a faulty', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.91it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 31.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.65it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:17:11 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 590 [type=json_invalid, input_value='{\"title\": \"Wrong tablet ...-03-19; 2024-03-19; 202', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 74.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.26it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.00it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.97it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.17it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.52it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:17:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1538 [type=json_invalid, input_value='{ \"title\": \"Report on un...actions are as follows:', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 41.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.57it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.74it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:18:16 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 891 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo...tion?  Action?  Action?', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 63.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.24it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.95it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:18:48 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 891 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo...tion?  Action?  Action?', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.92it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:19:07 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1546 [type=json_invalid, input_value='{ \"title\": \"Report on un... Technician, during the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.36it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.87it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:19:20 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1161 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo... report?  Action report', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.18it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.62it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:19:54 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 891 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo...tion?  Action?  Action?', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.98it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:19:56 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 617 [type=json_invalid, input_value='{ \"title\": \"Report on un...\" \\\\\"\\\\\"\\\\\"\\\\\" \\\\\"\\\\\"\\\\', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.83it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.06it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.57it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 67.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.42it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 49.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.84it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.60it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:20:35 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1205 [type=json_invalid, input_value='{\"title\":\"What was the a...ging Line, Action: Oper', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 40.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.26it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.86it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.81it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:21:08 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1156 [type=json_invalid, input_value='{\"title\": \"What was the ...use of the spillage was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.93it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:21:32 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1690 [type=json_invalid, input_value='{ \"title\": \"Detailed rep...he deviation was logged', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 25.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.97it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:21:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 360 [type=json_invalid, input_value='{\"title\": \"Accidental Sp...00000000000000000000000', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 48.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.71it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.98it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.76it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.65it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 73.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.42it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.27it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.05it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.90it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.98it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.31it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.05it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.56it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.82it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.90it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:23:02 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 576 [type=json_invalid, input_value='{\"title\": \"What?\", \"repo...port filed) 2024-08-03 ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.26it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.00it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.70it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.48it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.29it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.35it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.65it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:23:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 974 [type=json_invalid, input_value='{ \"title\": \"Description ...osis\\' }, { \\'fieldCode', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 64.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.96it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 78.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.27it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.33it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 76.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.81it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.97it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.10it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.31it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.61it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:24:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 366 [type=json_invalid, input_value='{\"title\":\"Report\",\"repor...MZ-10JY-PMZ-10JY-PMZ-10', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.92it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.43it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.49it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.23it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.69it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.39it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.05it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.32it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.83it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.14it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.54it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.17it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.06it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.65it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.96it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 82.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.53it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.88it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.70it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.15it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.44it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.99it/s]\n","***** Starting statistical analyisis for the experiment_id=HuggingFaceTB-SmolLM2-135M-Instruct-19-082025_16-19-07 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-HuggingFaceTB-SmolLM2-135M-Instruct-19-082025_16-19-07.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_A-HuggingFaceTB-SmolLM2-135M-Instruct-19-082025_16-19-07.xlsx\n","08/20/2025 02:26:21 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1441 [type=json_invalid, input_value='{ \"title\": \"Report on th... given in one paragraph', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 34.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.44it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_B-HuggingFaceTB-SmolLM2-135M-Instruct-19-082025_16-19-07.xlsx\n","Batches: 100% 1/1 [00:00<00:00, 60.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.48it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.25it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_C-HuggingFaceTB-SmolLM2-135M-Instruct-19-082025_16-19-07.xlsx\n","reportParamGridSearch time --- 609.7248634417851 minutes ---\n","Batches: 100% 1/1 [00:00<00:00, 77.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.81it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 109.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.07it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:26:58 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1149 [type=json_invalid, input_value='{ \"title\": \"Report on ev...y, contingency actions?', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 62.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.89it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.67it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:27:16 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 611 [type=json_invalid, input_value='{ \"title\": \"Report on ev...\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 114.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.71it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 87.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.64it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.73it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.23it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 89.34it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 111.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 104.18it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.63it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.70it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 86.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.49it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.82it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.04it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 104.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.01it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 92.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.87it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 107.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.44it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 110.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.25it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:29:11 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 462 [type=json_invalid, input_value='{ \"title\": \"Report on Mi...15. 13:15. 13:15. 13:15', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 130.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.50it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.73it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 73.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.44it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:29:51 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 462 [type=json_invalid, input_value='{ \"title\": \"Report on Mi...15. 13:15. 13:15. 13:15', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 111.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.35it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 118.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.05it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 78.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.80it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:30:13 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1482 [type=json_invalid, input_value='{ \"title\": \"Report on Mi... the sample was sent to', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 81.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.99it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.60it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.46it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.73it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.47it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 77.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.04it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:31:12 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1399 [type=json_invalid, input_value='{ \"title\": \"Report on Mi...misidentification.  The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 127.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.31it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:31:31 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1049 [type=json_invalid, input_value='{ \"title\": \"Report on Mi...mple was retested again', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 108.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.32it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:31:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1244 [type=json_invalid, input_value='{ \"title\": \"Report on Mi...mple was actually CTX-3', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 120.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.34it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.02it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:32:20 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1491 [type=json_invalid, input_value='{ \"title\": \"Report on Mi...th a different organism', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 124.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.21it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.02it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:32:42 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1347 [type=json_invalid, input_value='{ \"title\": \"Report on Mi...mple was actually CTX-3', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 118.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.83it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.30it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 111.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.25it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.36it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.79it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 109.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.68it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 108.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.75it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.62it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 111.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.34it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.15it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 111.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.00it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.14it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.54it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.62it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 108.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.32it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.25it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.86it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 99.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.61it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 116.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.31it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 117.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.97it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.56it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 113.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.17it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.99it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:35:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1241 [type=json_invalid, input_value='{ \"title\": \"Report on Da... on March 30, 2024. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 90.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.72it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 118.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.04it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.37it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 91.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.92it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.15it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.34it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 113.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.37it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:35:54 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1580 [type=json_invalid, input_value='{ \"title\": \"Report on Da...essful and the data was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 103.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.63it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:36:13 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1469 [type=json_invalid, input_value='{ \"title\": \"Report on Da...ion started. The report', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 78.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.30it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:36:32 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1380 [type=json_invalid, input_value='{ \"title\": \"Report on Da... working to implement a', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 117.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.97it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:36:51 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1410 [type=json_invalid, input_value='{ \"title\": \"Report on Da... is well-structured and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 92.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.53it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 103.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.76it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 107.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.02it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:37:26 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 995 [type=json_invalid, input_value='{ \"title\": \"Report on th... and Dev team, QA team,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 113.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.81it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.31it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 110.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 98.60it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.44it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:37:54 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 527 [type=json_invalid, input_value='{ \"title\": \"Report for t... <br>  <br>  <br>  <br>', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 112.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.62it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.01it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 92.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.32it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.28it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.52it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.24it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.46it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 107.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.12it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.35it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 87.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.29it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.53it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.57it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 104.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.99it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.64it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.38it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.24it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.40it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.67it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.50it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 94.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.96it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 103.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 106.12it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.22it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 79.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.85it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.31it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.55it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.97it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.06it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.19it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.92it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 96.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.79it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 75.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.76it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:40:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1222 [type=json_invalid, input_value='{ \"title\": \"Report gener...ided in the event is as', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 121.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.95it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:40:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1482 [type=json_invalid, input_value='{ \"title\": \"Report on MQ...tion was conducted, and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 125.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.43it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.80it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.41it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.54it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 91.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.62it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.85it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:41:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1829 [type=json_invalid, input_value='{ \"title\": \"Cleaning pro... and revised to include', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 116.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.31it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 108.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.24it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:41:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1412 [type=json_invalid, input_value='{ \"title\": \"Detailed rep...ng was performed by the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 123.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.99it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.57it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.78it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.20it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:41:59 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1255 [type=json_invalid, input_value='{ \"title\": \"Wrong cleani..., cleaning SOP updated,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 118.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.04it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 109.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.94it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.20it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.25it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.64it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 109.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.91it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.10it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 102.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.42it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 108.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.56it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.13it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 109.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.93it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:42:57 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1508 [type=json_invalid, input_value='{ \"title\": \"Report on Au...sue. The report is also', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 118.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.86it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.17it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.34it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.70it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:43:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1268 [type=json_invalid, input_value='{ \"title\": \"Report on th...for you. I will provide', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 118.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.82it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.99it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 91.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.30it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.39it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.85it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 78.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.55it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.81it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.36it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/20/2025 02:44:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1286 [type=json_invalid, input_value='{ \"title\": \"Report for A...ored during the cleanup', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 98.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 99.58it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.62it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.05it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.28it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 80.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.83it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 103.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.28it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 104.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 102.54it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.28it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 108.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.91it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.37it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.47it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 105.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 92.98it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.58it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.42it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 90.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.83it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 111.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.77it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.01it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 111.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.40it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.89it/s]\n","***** Starting statistical analyisis for the experiment_id=HuggingFaceTB-SmolLM2-360M-Instruct-19-082025_16-19-13 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-HuggingFaceTB-SmolLM2-360M-Instruct-19-082025_16-19-13.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_A-HuggingFaceTB-SmolLM2-360M-Instruct-19-082025_16-19-13.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_B-HuggingFaceTB-SmolLM2-360M-Instruct-19-082025_16-19-13.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_C-HuggingFaceTB-SmolLM2-360M-Instruct-19-082025_16-19-13.xlsx\n","reportParamGridSearch time --- 630.0947401245435 minutes ---\n"]}]},{"cell_type":"markdown","source":["## Qwen/Qwen2.5-0.5B-Instruct"],"metadata":{"id":"F2m43xRsHKlW"}},{"cell_type":"code","source":["!python app/reportParamGridSearch.py --model_id Qwen/Qwen2.5-0.5B-Instruct --non-threaded --max_workers  4 --prompt_method B C --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 2  --temperature 0.7 1.3 --top_p 0.3 0.9 --top_k 50 --max_new_tokens 300 --do_sample True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"awksT-7GOYkm","executionInfo":{"status":"ok","timestamp":1755615472782,"user_tz":-120,"elapsed":166748,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"162e6922-ad9c-40ab-e6ee-0eafd363c962"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-08-19 14:55:13.273844: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755615313.308470   23223 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755615313.320179   23223 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755615313.348724   23223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755615313.348754   23223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755615313.348761   23223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755615313.348768   23223 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['Qwen/Qwen2.5-0.5B-Instruct'], 'prompt_method': ['B', 'C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [1], 'end_idx': [2], 'temperature': [0.7, 1.3], 'top_p': [0.3, 0.9], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","08/19/2025 14:55:34 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=Qwen/Qwen2.5-0.5B-Instruct\n","08/19/2025 14:55:34 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=Qwen/Qwen2.5-0.5B-Instruct\n","08/19/2025 14:55:34 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=Qwen/Qwen2.5-0.5B-Instruct\n","Generation parameters: \n","{'temperature': [0.7, 1.3], 'top_p': [0.3, 0.9], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","Results file is expected to have 16 rows.\n","******* Starting GRID SEARCH ***********\n","******* Starting NOT THREADED PROCESS ************\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 65.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.33it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 117.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.24it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 100.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.94it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 107.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.10it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 114.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.02it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 119.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.10it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 107.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.51it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 110.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.11it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 116.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.61it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 69.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.84it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 42.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.07it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 120.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.63it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 105.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.96it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 114.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.72it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 78.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.54it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.1}\n","Batches: 100% 1/1 [00:00<00:00, 108.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.63it/s]\n","***** Starting statistical analyisis for the experiment_id=Qwen-Qwen2.5-0.5B-Instruct-19-082025_14-55-37 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-Qwen-Qwen2.5-0.5B-Instruct-19-082025_14-55-37.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_B-Qwen-Qwen2.5-0.5B-Instruct-19-082025_14-55-37.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_C-Qwen-Qwen2.5-0.5B-Instruct-19-082025_14-55-37.xlsx\n","reportParamGridSearch time --- 2.5005958318710326 minutes ---\n"]}]},{"cell_type":"code","source":["# KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","\n","from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"54e-108TIfvA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training SmolLM2-360M-Instruct\n"],"metadata":{"id":"6JPNWTIgSq91"}},{"cell_type":"markdown","source":["## Plot training loss and metrics function\n"],"metadata":{"id":"XOoLyFvTLZji"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","import os\n","# =========================\n","# Post-training: PNG GRAPHS + CSV\n","# =========================\n","\n","def plot_training_results(out_dir, _trainer):\n","  \"\"\" loads trainer log history and plots results and exports images \"\"\"\n","  logs = pd.DataFrame(_trainer.state.log_history)\n","  if logs.empty:\n","      print(\"No logs recorded. Did logging_steps/eval_steps run?\")\n","  else:\n","      os.makedirs(out_dir, exist_ok=True)\n","      log_csv = os.path.join(out_dir, \"training_logs.csv\")\n","      logs.to_csv(log_csv, index=False)\n","      print(\"Saved raw logs to:\", log_csv)\n","\n","      def safe_exp(x):\n","          try: return float(np.exp(x))\n","          except Exception: return np.nan\n","\n","      train_df = logs[logs[\"loss\"].notna()][[\"step\",\"loss\"]].copy()\n","      eval_df  = logs[logs[\"eval_loss\"].notna()][[\"step\",\"eval_loss\"]].copy()\n","      lr_df    = logs[logs[\"learning_rate\"].notna()][[\"step\",\"learning_rate\"]].copy()\n","\n","      if not eval_df.empty: eval_df[\"eval_perplexity\"] = eval_df[\"eval_loss\"].map(safe_exp)\n","      if not train_df.empty: train_df[\"perplexity\"] = train_df[\"loss\"].map(safe_exp)\n","\n","      def smooth(y, k=5):\n","          s = pd.Series(y); return s.rolling(k, min_periods=1, center=True).mean().values\n","\n","      if not train_df.empty:\n","          plt.figure(figsize=(7,4))\n","          plt.plot(train_df[\"step\"], train_df[\"loss\"], label=\"train loss\")\n","          plt.plot(train_df[\"step\"], smooth(train_df[\"loss\"], 7), label=\"train loss (smoothed)\")\n","          plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.title(\"Training Loss\")\n","          plt.legend(); plt.tight_layout()\n","          plt.savefig(os.path.join(out_dir, \"train_loss.png\")); plt.show()\n","\n","      if not eval_df.empty:\n","          plt.figure(figsize=(7,4))\n","          plt.plot(eval_df[\"step\"], eval_df[\"eval_loss\"], label=\"eval loss\")\n","          plt.plot(eval_df[\"step\"], smooth(eval_df[\"eval_loss\"], 3), label=\"eval loss (smoothed)\")\n","          plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.title(\"Eval Loss\")\n","          plt.legend(); plt.tight_layout()\n","          plt.savefig(os.path.join(out_dir, \"eval_loss.png\")); plt.show()\n","\n","          plt.figure(figsize=(7,4))\n","          plt.plot(eval_df[\"step\"], eval_df[\"eval_perplexity\"], label=\"eval perplexity\")\n","          plt.xlabel(\"Step\"); plt.ylabel(\"Perplexity\"); plt.title(\"Eval Perplexity\")\n","          plt.legend(); plt.tight_layout()\n","          plt.savefig(os.path.join(out_dir, \"eval_perplexity.png\")); plt.show()\n","\n","      if not lr_df.empty:\n","          plt.figure(figsize=(7,4))\n","          plt.plot(lr_df[\"step\"], lr_df[\"learning_rate\"], label=\"learning rate\")\n","          plt.xlabel(\"Step\"); plt.ylabel(\"LR\"); plt.title(\"Learning Rate Schedule\")\n","          plt.legend(); plt.tight_layout()\n","          plt.savefig(os.path.join(out_dir, \"learning_rate.png\")); plt.show()\n","\n","      if not eval_df.empty:\n","          best_row = eval_df.loc[eval_df[\"eval_loss\"].idxmin()]\n","          print(f\"Best eval loss: {best_row['eval_loss']:.4f} at step {int(best_row['step'])} \"\n","                f\"(perplexity ~ {best_row['eval_perplexity']:.2f})\")\n","      if not train_df.empty:\n","          print(f\"Final train loss: {train_df['loss'].iloc[-1]:.4f} at step {int(train_df['step'].iloc[-1])}\")\n"],"metadata":{"id":"YhLqJyveLdy0","executionInfo":{"status":"ok","timestamp":1756405360536,"user_tz":-120,"elapsed":75,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}}},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":["## Training to minimize eval loss (NLL)"],"metadata":{"id":"6zKLlvE2Szh4"}},{"cell_type":"markdown","source":["### Generic prompt"],"metadata":{"id":"M66VtIqi7XXn"}},{"cell_type":"code","source":["!pip -q install -U \"transformers>=4.43\" \"accelerate>=0.33\" \"datasets>=2.20\" \\\n","  \"trl>=0.9.6\" peft bitsandbytes evaluate"],"metadata":{"id":"YuyEHSLVri01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip -q install wandb"],"metadata":{"id":"tFUuTTkBFERL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","import wandb\n","IS_WANDB = False\n","if IS_WANDB:\n","  os.environ[\"WANDB_PROJECT\"] = \"accident-reporter\"\n","  os.environ[\"WANDB_WATCH\"] = \"false\"          # don't auto-log gradients\n","  os.environ[\"WANDB_SILENT\"] = \"true\"\n","  from google.colab import userdata\n","  wand_db_token = userdata.get('wandb_token')\n","  wandb.login(key=wand_db_token)  # paste token (or set WANDB_API_KEY env var)\n","else:\n","  os.environ[\"WANDB_DISABLED\"] = \"true\""],"metadata":{"id":"S2r7j3KgAmBi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === ONE-CELL QLoRA TRAINER: SmolLM2-360M-Instruct with trl.SFTConfig (no char clipping) ===\n","# Colab tip: Runtime -> Change runtime type -> GPU (T4)\n","\n","import os, re, json, torch\n","from datasets import load_dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, EarlyStoppingCallback\n","from peft import LoraConfig\n","from trl import SFTTrainer, SFTConfig\n","\n","MODEL_ID = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n","TRAINING_DIR = \"app/datasets/training\"\n","OUT_DIR  = TRAINING_DIR + \"/smollm2_360m_onepara_lora\"\n","\n","# 4-bit QLoRA base (tiny VRAM/RAM footprint)\n","bnb = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n","if tok.pad_token_id is None:\n","    tok.pad_token = tok.eos_token\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_ID,\n","    quantization_config=bnb,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")\n","model.config.use_cache = False  # needed for grad checkpointing\n","# LoRA config (light but effective for ~500 rows)\n","\n","# -------- Autodetect LoRA target modules (robust across archs) --------\n","def guess_lora_targets(m):\n","    names = set()\n","    for n, mod in m.named_modules():\n","        if isinstance(mod, torch.nn.Linear):\n","            names.add(n.split(\".\")[-1])\n","    # Prefer common llama/mistral-ish names if present\n","    preferred = [x for x in [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",\n","                              \"wi\",\"wo\",\"wq\",\"wk\",\"wv\",\"out_proj\",\"fc_in\",\"fc_out\"] if x in names]\n","    if preferred:\n","        return preferred\n","    # Fallback: all linear leaf names except lm_head\n","    return sorted(list({n for n in names if n != \"lm_head\"}))\n","\n","target_modules = guess_lora_targets(model)\n","if not target_modules:\n","    raise RuntimeError(\"Could not find any target modules for LoRA — aborting to avoid no-grad training.\")\n","\n","# --- Load LoRa config  ---\n","\n","peft_cfg = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n","    task_type=\"CAUSAL_LM\"\n",")\n","model_lora_check = get_peft_model(model, peft_cfg)\n","model_lora_check.print_trainable_parameters()\n","\n","ds = load_dataset(\"json\", data_files={\"train\":TRAINING_DIR+\"/train.jsonl\",\"eval\":TRAINING_DIR+\"/eval.jsonl\"})\n","\n","print(f\"DS_1: {ds}\")\n","\n","\n","def one_line(s: str) -> str:\n","    s = str(s).replace(\"\\n\",\" \")\n","    return re.sub(r\"\\s+\",\" \", s).strip()\n","\n","INSTR = (\n","  \"Write ONE SINGLE PARAGRAPH in English that includes ALL given facts: what happened, when, where, who, how, why \"\n","  \"(root cause), and contingency/corrective actions. Neutral tone. No bullet points, no headings, no lists, no JSON. \"\n","  \"DO NOT invent details. Output must be a single line (no line breaks).\"\n",")\n","RESP_TMPL = \"### Response:\\n\"  # SFTTrainer will mask everything before this marker as prompt\n","MAX_LEN = 1024\n","\n","def find_subsequence(xs: list[int], ys: list[int]) -> int:\n","    \"\"\"Return start index of ys inside xs, or -1 if not found.\"\"\"\n","    n, m = len(xs), len(ys)\n","    if m == 0 or m > n: return -1\n","    for i in range(n - m + 1):\n","        if xs[i:i+m] == ys:\n","            return i\n","    return -1\n","\n","def tokenize_and_mask(example: dict) -> dict:\n","    # Build full prompt -> \"### Instruction ... INPUT ... ### Response:\\n + target\"\n","    text_in  = one_line(example[\"input\"])\n","    text_out = one_line(example[\"target\"])\n","    full = f\"### Instruction:\\n{INSTR}\\n\\nINPUT:\\n{text_in}\\n\\n{RESP_TMPL}{text_out}\"\n","\n","    enc = tok(\n","        full,\n","        truncation=True,\n","        max_length=MAX_LEN,\n","        padding=False,             # pad later in collator\n","        return_tensors=None\n","    )\n","    input_ids = enc[\"input_ids\"]\n","    labels    = input_ids.copy()\n","\n","    # Locate response template and mask everything before the end of it\n","    rt_ids = tok(RESP_TMPL, add_special_tokens=False)[\"input_ids\"]\n","    start = find_subsequence(input_ids, rt_ids)\n","    if start == -1:\n","        # If marker not found (rare after truncation), skip supervision on whole sample\n","        labels[:] = [-100] * len(labels)\n","    else:\n","        # Mask up to the end of the template tokens\n","        cut = start + len(rt_ids)\n","        labels[:cut] = [-100] * cut\n","\n","    return {\n","        \"input_ids\": input_ids,\n","        \"attention_mask\": enc[\"attention_mask\"],\n","        \"labels\": labels\n","    }\n","\n","ds_tok = ds.map(tokenize_and_mask, remove_columns=ds[\"train\"].column_names, desc=\"Tokenizing & masking\")\n","\n","# print(f\"DS_2: {ds_tok}\")\n","\n","ds_tok = ds_tok.remove_columns([c for c in ds_tok[\"train\"].column_names\n","                                if c not in (\"input_ids\",\"attention_mask\",\"labels\")])\n","\n","# Make sure the dataset yields torch tensors with those keys\n","ds_tok.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n","# print(f\"DS_3: {ds_tok}\")\n","\n","# Simple collator: pad inputs and labels to max length in batch\n","class CausalLMPadCollator:\n","    def __init__(self, tokenizer, label_pad_id=-100):\n","        self.tok = tokenizer\n","        self.label_pad_id = label_pad_id\n","\n","    def __call__(self, features: list[dict]) -> dict[str, torch.Tensor]:\n","        max_len = max(len(f[\"input_ids\"]) for f in features)\n","        input_ids, attn, labels = [], [], []\n","        for f in features:\n","            pad = max_len - len(f[\"input_ids\"])\n","            input_ids.append(f[\"input_ids\"] + [self.tok.pad_token_id] * pad)\n","            attn.append(f[\"attention_mask\"] + [0] * pad)\n","            labels.append(f[\"labels\"] + [self.label_pad_id] * pad)\n","        return {\n","            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n","            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n","            \"labels\": torch.tensor(labels, dtype=torch.long),\n","        }\n","\n","collator = CausalLMPadCollator(tok)\n","\n","# --- SFTConfig (replaces TrainingArguments) ---\n","sft_cfg = SFTConfig(\n","    output_dir=OUT_DIR,\n","    num_train_epochs=4,                       # 2 epochs is plenty for ~500 rows\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=16,           # eff batch ~= 64\n","    gradient_checkpointing=False,             #Turn off checkpointing (needs a bit more VRAM on T4, but SmolLM2-360M QLoRA usually fits):\n","    learning_rate=1.5e-4,\n","    lr_scheduler_type=\"cosine\",\n","    warmup_steps=3,\n","    load_best_model_at_end=True,\n","    # warmup_ratio = 0.03, # not a good idea if our data is short\n","    eval_strategy=\"epoch\", # \"steps\"\n","    # eval_steps=100,\n","    save_strategy=\"epoch\", # \"steps\"\n","    # save_steps=100,\n","    metric_for_best_model=\"eval_loss\",\n","    greater_is_better=False,\n","    save_total_limit=2,\n","    logging_steps=1,\n","    fp16=True,                                # T4-friendly\n","    optim=\"paged_adamw_8bit\",\n","    max_grad_norm=0.5,\n","    max_length = MAX_LEN,                      # handled by SFTTrainer when set here\n","    # dataset_text_field=\"text\", # Removed as data is already tokenized\n","    packing=False,\n","    remove_unused_columns=False,           # important for pre-tokenized inputs\n","    report_to=\"none\"  # wandb\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=sft_cfg,                             # <-- using SFTConfig\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n","    train_dataset=ds_tok[\"train\"], # Use the tokenized dataset\n","    eval_dataset=ds_tok[\"eval\"],   # Use the tokenized dataset\n","    data_collator = collator,\n","    peft_config=peft_cfg,\n",")\n","trainer.train()\n","final_metrics = trainer.evaluate()      # logs eval_loss into state.log_history\n","print(\"Final eval:\", final_metrics)\n","\n","# Save LoRA adapter\n","adapter_dir = f\"{OUT_DIR}/adapter\"\n","trainer.model.save_pretrained(adapter_dir)\n","tok.save_pretrained(adapter_dir)\n","print(\"Saved LoRA adapter to:\", adapter_dir)\n","\n","# ---------------------- Inference (no char clipping) ----------------------\n","# @torch.no_grad()\n","# def build_infer_prompt(user_text: str) -> str:\n","#     return \"### Instruction:\\n\" + INSTR + \"\\n\\nINPUT:\\n\" + one_line(user_text) + f\"\\n\\n{RESP_TMPL}\"\n","\n","# @torch.no_grad()\n","# def generate_one_paragraph(user_text: str, max_new_tokens: int = 220,\n","#                            temperature: float = 0.0, top_p: float = 1.0) -> str:\n","#     prompt = build_infer_prompt(user_text)\n","#     ids = tok(prompt, return_tensors=\"pt\").to(trainer.model.device)\n","#     out = trainer.model.generate(\n","#         **ids,\n","#         max_new_tokens=max_new_tokens,\n","#         do_sample=(temperature>0),\n","#         temperature=temperature,\n","#         top_p=top_p,\n","#         eos_token_id=tok.eos_token_id\n","#     )\n","#     gen = tok.decode(out[0], skip_special_tokens=True).split(RESP_TMPL, 1)[-1]\n","#     return one_line(gen)  # single line, but no length clipping\n","\n","# # Quick check on a couple eval samples\n","# eval_split = load_dataset(\"json\", data_files={\"eval\":TRAINING_DIR+\"/eval.jsonl\"})[\"eval\"] # Load from the correct directory\n","# for i in range(min(3, len(eval_split))):\n","#     print(\"-\", generate_one_paragraph(eval_split[i][\"input\"]))\n","\n","# Log metrics in wandb\n","# wandb.log({\n","#   \"eval/paragraphness\": no_breaks / N,      # % with no '\\n'\n","#   \"eval/<=400_chars\": within_len / N,\n","#   \"eval/slot_coverage\": slot_cov,           # if you compute it\n","# })\n","# del meval\n","\n","plot_training_results(OUT_DIR,trainer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":851,"referenced_widgets":["ea6941737d8145b9b9c501659e420bf8","88650bc49be3454ebc5bd9eb06f8501e","4529118387174ce985140ad3721d6e1d","95ce068fdec240f1b9bf0cff17889382","e38372e7a300406489f079d5750dcfde","5c2b64c8e3124a708bf5f7ae7e4e9be6","b4708293b9b3453c94c5101da406118a","eab108ebdee44f32a3979c591b000263","91fc12cdbde54b5f8ec58a43dce0f6ee","8425f10735aa47e48b7a0974dd54df12","37e1760f990e4540ac1c15077f54252b","9aa40a1d2f91420d8a799aa9ef16c518","ea216d7e34a0471591ca84fc3c0d8fcf","e86211f7fb334416964dae9281d9a927","18899107ce0040b0909cd4eb465cbd31","ed7f9b388169439da7ab751564db8079","462f3b06688b4388aa336abde8ef93a5","d74661f85e3147afae0011fec7758c6d","24b47caf925f47b3828e59c9b83e4986","7860f8a306d34ab7a7aba51dc5ed2067","09c6b845888743a0afb6cbb0541ada02","998ac8edf4a14fa4b2dd3226f396a7fa","0e8b55ff80a9495db5ba2d62d34e52af","8d74c9681d914be3a3265519e066be9a","d42ac0e177de4c8488fc33e965cd8fce","0dfaf8f46379455b9f01664dc7a3919c","9f416bf6a7cd437292a997aebb4dfda4","2467c26d6cd74ffdadf4610866430f30","27d55f87b8cd415a8021da1b1f7066dd","5f7ea352e86347c889df984a45cdca26","b0e7376bf2f243a59bd9ef69f977633b","0dde0863df7148c399d12093f7b21e26","cdf80895248f4899a4eff388b6f0e8cd","2cf9f1b7b78242ef8318bd1f6c6bed3d","33f443d4c6af4ce38b29939aa4e79b2e","e1395738da784d5892f9da4d44717f7e","352f6801f7b84a25858cf2f1520e56a2","557ef272a26e4dfea0247b3b8e494d1e","23daa801e69546dba6ef8d56a026f060","29d959855613445d86c99fd3014775bc","1ad306609ae94966814101ac0a77ed1c","998f7b6baa4541d7813a29e067067c0d","bf1e3522db2b498e915c6ea1b993e1c1","0a6d690420eb4fb98130f007ce82d5f8"]},"id":"yd8qW0laSvS8","executionInfo":{"status":"error","timestamp":1756404031452,"user_tz":-120,"elapsed":16359,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"4e297bc5-0f12-43ea-d75e-400cf091c628"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 4,341,760 || all params: 366,162,880 || trainable%: 1.1857\n","DS_1: DatasetDict({\n","    train: Dataset({\n","        features: ['input', 'target'],\n","        num_rows: 699\n","    })\n","    eval: Dataset({\n","        features: ['input', 'target'],\n","        num_rows: 70\n","    })\n","})\n"]},{"output_type":"display_data","data":{"text/plain":["Tokenizing & masking:   0%|          | 0/699 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea6941737d8145b9b9c501659e420bf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Tokenizing & masking:   0%|          | 0/70 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aa40a1d2f91420d8a799aa9ef16c518"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Truncating train dataset:   0%|          | 0/699 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e8b55ff80a9495db5ba2d62d34e52af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Truncating eval dataset:   0%|          | 0/70 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cf9f1b7b78242ef8318bd1f6c6bed3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 2/44 : < :, Epoch 0.09/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-765797357.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mpeft_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpeft_cfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m )\n\u001b[0;32m--> 196\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0mfinal_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# logs eval_loss into state.log_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final eval:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2239\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2580\u001b[0m                     )\n\u001b[1;32m   2581\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2582\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2584\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_activation_offload_context\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3795\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3796\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3798\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/trl/trainer/sft_trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    856\u001b[0m         \"\"\"\n\u001b[1;32m    857\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m         (loss, outputs) = super().compute_loss(\n\u001b[0m\u001b[1;32m    859\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3882\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3883\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3884\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3885\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3886\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1848\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1851\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 460\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m                         \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Restore original forward methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    391\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;31m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;31m# The reason is that in some cases, an error can occur that backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_are_functorch_transforms_active\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/utils.py\u001b[0m in \u001b[0;36munwrap_dead_wrappers\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# NB: doesn't use tree_map_only for performance reasons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     result = tuple(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0munwrap_if_dead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# NB: doesn't use tree_map_only for performance reasons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     result = tuple(\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0munwrap_if_dead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["### Prompt with demo (prompt D as ref)\n"],"metadata":{"id":"DRf5VTZJ7bAj"}},{"cell_type":"code","source":["!pip -q install -U \"transformers>=4.43\" \"accelerate>=0.33\" \"datasets>=2.20\" peft bitsandbytes trl"],"metadata":{"id":"n4rNEaRxGRoc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756400069976,"user_tz":-120,"elapsed":3137,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"560c06db-6238-4fe0-edb1-0dcff78c8a81"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/511.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m501.8/511.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["#### Load training function and plot function"],"metadata":{"id":"4H79z9CCmlLH"}},{"cell_type":"code","source":["# ==== ONE-CELL COLAB: wrap QLoRA + demo-prompt SFT into a function that returns the trainer ====\n","# Colab: Runtime > Change runtime type > GPU (T4)\n","import os, re, numpy as np, torch\n","from dataclasses import dataclass\n","from typing import Dict, List, Tuple\n","from datasets import load_dataset, DatasetDict\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, EarlyStoppingCallback\n","from peft import LoraConfig, get_peft_model\n","from trl import SFTTrainer, SFTConfig\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","# =========================\n","# Post-training: PNG GRAPHS + CSV\n","# =========================\n","\n","def plot_training_results(out_dir, _trainer):\n","  \"\"\" loads trainer log history and plots results and exports images \"\"\"\n","  logs = pd.DataFrame(_trainer.state.log_history)\n","  if logs.empty:\n","      print(\"No logs recorded. Did logging_steps/eval_steps run?\")\n","  else:\n","      os.makedirs(out_dir, exist_ok=True)\n","      log_csv = os.path.join(out_dir, \"training_logs.csv\")\n","      logs.to_csv(log_csv, index=False)\n","      print(\"Saved raw logs to:\", log_csv)\n","\n","      def safe_exp(x):\n","          try: return float(np.exp(x))\n","          except Exception: return np.nan\n","\n","      train_df = logs[logs[\"loss\"].notna()][[\"step\",\"loss\"]].copy()\n","      eval_df  = logs[logs[\"eval_loss\"].notna()][[\"step\",\"eval_loss\"]].copy()\n","      lr_df    = logs[logs[\"learning_rate\"].notna()][[\"step\",\"learning_rate\"]].copy()\n","\n","      if not eval_df.empty: eval_df[\"eval_perplexity\"] = eval_df[\"eval_loss\"].map(safe_exp)\n","      if not train_df.empty: train_df[\"perplexity\"] = train_df[\"loss\"].map(safe_exp)\n","\n","      def smooth(y, k=5):\n","          s = pd.Series(y); return s.rolling(k, min_periods=1, center=True).mean().values\n","\n","      if not train_df.empty:\n","          plt.figure(figsize=(7,4))\n","          plt.plot(train_df[\"step\"], train_df[\"loss\"], label=\"train loss\")\n","          plt.plot(train_df[\"step\"], smooth(train_df[\"loss\"], 7), label=\"train loss (smoothed)\")\n","          plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.title(\"Training Loss\")\n","          plt.legend(); plt.tight_layout()\n","          plt.savefig(os.path.join(out_dir, \"train_loss.png\")); plt.show()\n","\n","      if not eval_df.empty:\n","          plt.figure(figsize=(7,4))\n","          plt.plot(eval_df[\"step\"], eval_df[\"eval_loss\"], label=\"eval loss\")\n","          plt.plot(eval_df[\"step\"], smooth(eval_df[\"eval_loss\"], 3), label=\"eval loss (smoothed)\")\n","          plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.title(\"Eval Loss\")\n","          plt.legend(); plt.tight_layout()\n","          plt.savefig(os.path.join(out_dir, \"eval_loss.png\")); plt.show()\n","\n","          plt.figure(figsize=(7,4))\n","          plt.plot(eval_df[\"step\"], eval_df[\"eval_perplexity\"], label=\"eval perplexity\")\n","          plt.xlabel(\"Step\"); plt.ylabel(\"Perplexity\"); plt.title(\"Eval Perplexity\")\n","          plt.legend(); plt.tight_layout()\n","          plt.savefig(os.path.join(out_dir, \"eval_perplexity.png\")); plt.show()\n","\n","      if not lr_df.empty:\n","          plt.figure(figsize=(7,4))\n","          plt.plot(lr_df[\"step\"], lr_df[\"learning_rate\"], label=\"learning rate\")\n","          plt.xlabel(\"Step\"); plt.ylabel(\"LR\"); plt.title(\"Learning Rate Schedule\")\n","          plt.legend(); plt.tight_layout()\n","          plt.savefig(os.path.join(out_dir, \"learning_rate.png\")); plt.show()\n","\n","      if not eval_df.empty:\n","          best_row = eval_df.loc[eval_df[\"eval_loss\"].idxmin()]\n","          print(f\"Best eval loss: {best_row['eval_loss']:.4f} at step {int(best_row['step'])} \"\n","                f\"(perplexity ~ {best_row['eval_perplexity']:.2f})\")\n","      if not train_df.empty:\n","          print(f\"Final train loss: {train_df['loss'].iloc[-1]:.4f} at step {int(train_df['step'].iloc[-1])}\")\n","\n","\n","def train_demo_qlora(\n","    MODEL_ID: str = \"HuggingFaceTB/SmolLM2-360M-Instruct\",\n","    OUT_DIR: str = \"smollm2_demo_qlora\",\n","    DEMO_PROB: float = 1.0,\n","    DATASET_ID = \"zBotta/traffic-accidents-reports-800\",\n","    sft_cfg: SFTConfig,\n","):\n","    \"\"\"\n","    Fine-tune MODEL_ID with QLoRA on DATASET_ID using a demo-wired prompt.\n","    Dataset format: splits 'train' and 'eval', each row has:\n","      - 'input': string \"What: ..., When: ..., Where: ..., Who: ..., How: ..., Why: ..., ContingencyActions: ...\"\n","      - 'target': one-paragraph report (string)\n","    Returns: SFTTrainer (trained)\n","    \"\"\"\n","    MAX_LEN   = 1024\n","    RESP_TMPL = \"### Response:\\n\"\n","    INSTR = (\n","      \"\"\"\n","      You are a reporting agent.\n","      You task is to create a report when provided the what, when, why, who, how and where questions about the events.\n","      You are also given information about the contingency actions regarding the event.\n","\n","      Guidelines:\n","      - Generate only one report given the informations about the event\n","      - Generate the report as text in one paragraph\n","      - It is important to focus on accuracy and coherence when generating the report so that the description content matches the information provided (what, when, where, who, how , why, contingency actions).\n","        If an information is not provided in (what, when, where, who, how , why, contingency actions), it must not be part of the generated text description.\n","      \"\"\"\n","    )\n","\n","    def one_line(s: str) -> str:\n","        return re.sub(r\"\\s+\",\" \", str(s).replace(\"\\n\",\" \")).strip()\n","\n","    # ---- Load dataset (must have train/eval with 'input' and 'target') ----\n","    raw_any = load_dataset(DATASET_ID)\n","    assert \"train\" in raw_any and \"eval\" in raw_any, \"Dataset must have 'train' and 'eval' splits.\"\n","    ds_raw = DatasetDict(train=raw_any[\"train\"], eval=raw_any[\"eval\"])\n","\n","    print(f\"train len before validation: {len(ds_raw['train'])}\")\n","    print(f\"eval  len before validation: {len(ds_raw['eval'])}\")\n","\n","    def valid_row(rec: Dict) -> bool:\n","        return bool(str(rec.get(\"input\",\"\")).strip()) and bool(str(rec.get(\"target\",\"\")).strip())\n","\n","    ds_raw = DatasetDict(\n","        train=ds_raw[\"train\"].filter(valid_row),\n","        eval =ds_raw[\"eval\"].filter(valid_row)\n","    )\n","\n","    print(f\"train len after validation: {len(ds_raw['train'])}\")\n","    print(f\"eval  len after validation: {len(ds_raw['eval'])}\")\n","\n","    # Build demo pool from TRAIN\n","    demo_pool: List[Tuple[str,str]] = [(one_line(ex[\"input\"]), one_line(ex[\"target\"])) for ex in ds_raw[\"train\"]]\n","\n","    # ---- Prompt builders ----\n","    def build_prompt_with_demo(demo_in: str, demo_out: str, current_in: str) -> str:\n","        return (\n","            \"### Instruction:\\n\" + INSTR + \"\\n\\n\" +\n","            \"### Input-example:\\n\" + demo_in + \"\\n\\n\" +\n","            \"### Output-example:\\n\" + one_line(demo_out) + \"\\n\\n\" +\n","            \"### Input:\\n\" + current_in + \"\\n\\n\" +\n","            RESP_TMPL\n","        )\n","\n","    def build_prompt_no_demo(current_in: str) -> str:\n","        return (\n","            \"### Instruction:\\n\" + INSTR + \"\\n\\n\" +\n","            \"### Input:\\n\" + current_in + \"\\n\\n\" +\n","            RESP_TMPL\n","        )\n","\n","# ------------------ Tokenizer ------------------\n","    tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n","    if tok.pad_token_id is None: tok.pad_token = tok.eos_token\n","\n","    # Map rows -> prompt first (kept), then target fills the remaining budget\n","    def add_demo_and_tokenize(example, idx):\n","        cur_in  = one_line(example[\"input\"])\n","        cur_out = one_line(example[\"target\"])\n","\n","        use_demo = (np.random.rand() < DEMO_PROB) and (len(demo_pool) > 1)\n","        if use_demo:\n","            demo_idx = (idx + 1) % len(demo_pool)  # simple different row\n","            demo_in, demo_out = demo_pool[demo_idx]\n","            prompt = build_prompt_with_demo(demo_in, demo_out, cur_in)\n","        else:\n","            prompt = build_prompt_no_demo(cur_in)\n","\n","        # 1) tokenize prompt first (so the response marker is always inside)\n","        prom = tok(prompt, add_special_tokens=True, truncation=True, max_length=MAX_LEN, padding=False)\n","        prompt_ids = prom[\"input_ids\"]; attn_prompt = prom[\"attention_mask\"]\n","\n","        # If prompt alone overflows, fall back to no-demo minimal prompt\n","        if len(prompt_ids) >= MAX_LEN - 4:\n","            prompt = prompt_no_demo(cur_in)\n","            prom = tok(prompt, add_special_tokens=True, truncation=True, max_length=MAX_LEN, padding=False)\n","            prompt_ids = prom[\"input_ids\"]; attn_prompt = prom[\"attention_mask\"]\n","\n","        # 2) tokenize target to fill remaining budget\n","        budget = MAX_LEN - len(prompt_ids)\n","        targ = tok(cur_out, add_special_tokens=False, truncation=True, max_length=max(1, budget), padding=False)\n","        target_ids = targ[\"input_ids\"][:max(0, budget)]\n","\n","        # assemble & ensure ≥1 supervised token\n","        input_ids = prompt_ids + target_ids\n","        attention_mask = attn_prompt + [1]*len(target_ids)\n","        labels = [-100]*len(prompt_ids) + target_ids\n","        if not target_ids:  # ensure graph has grad\n","            eos = tok.eos_token_id\n","            input_ids += [eos]; attention_mask += [1]; labels += [eos]\n","\n","        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n","\n","\n","    ds_tok = ds_raw.map(\n","        add_demo_and_tokenize,\n","        with_indices=True,\n","        remove_columns=ds_raw[\"train\"].column_names,\n","        desc=\"Building demo prompts + tokenizing\"\n","    )\n","\n","    ds_tok = ds_tok.remove_columns([c for c in ds_tok[\"train\"].column_names\n","                                if c not in (\"input_ids\",\"attention_mask\",\"labels\")])\n","\n","    # Make sure the dataset yields torch tensors with those keys\n","    ds_tok.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n","\n","    # ---- QLoRA base model ----\n","    bnb4 = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_compute_dtype=torch.float16\n","    )\n","    model = AutoModelForCausalLM.from_pretrained(\n","        MODEL_ID,\n","        quantization_config=bnb4,\n","        torch_dtype=torch.float16,\n","        device_map=\"auto\",\n","    )\n","    model.config.use_cache = False  # better with gradient checkpointing\n","\n","    # LoRA adapters\n","\n","    # -------- Autodetect LoRA target modules (robust across archs) --------\n","    def guess_lora_targets(m):\n","        names = set()\n","        for n, mod in m.named_modules():\n","            if isinstance(mod, torch.nn.Linear):\n","                names.add(n.split(\".\")[-1])\n","        # Prefer common llama/mistral-ish names if present\n","        preferred = [x for x in [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\",\n","                                 \"wi\",\"wo\",\"wq\",\"wk\",\"wv\",\"out_proj\",\"fc_in\",\"fc_out\"] if x in names]\n","        if preferred:\n","            return preferred\n","        # Fallback: all linear leaf names except lm_head\n","        return sorted(list({n for n in names if n != \"lm_head\"}))\n","\n","    target_modules = guess_lora_targets(model)\n","    if not target_modules:\n","        raise RuntimeError(\"Could not find any target modules for LoRA — aborting to avoid no-grad training.\")\n","\n","    # --- Load LoRa config  ---\n","\n","    lora_cfg = LoraConfig(\n","        r=8, lora_alpha=16, lora_dropout=0.05,\n","        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n","        task_type=\"CAUSAL_LM\"\n","    )\n","    model_lora_check = get_peft_model(model, lora_cfg)\n","    model_lora_check.print_trainable_parameters()\n","\n","    # ---- Collator ----\n","    @dataclass\n","    class CausalLMPadCollator:\n","        pad_id: int\n","        def __call__(self, feats: List[Dict]):\n","            maxlen = max(len(f[\"input_ids\"]) for f in feats)\n","            def pad(seq, v): return seq + [v]*(maxlen-len(seq))\n","            return {\n","                \"input_ids\": torch.tensor([pad(f[\"input_ids\"], self.pad_id) for f in feats]),\n","                \"attention_mask\": torch.tensor([pad(f[\"attention_mask\"], 0) for f in feats]),\n","                \"labels\": torch.tensor([pad(f[\"labels\"], -100) for f in feats]),\n","            }\n","    collator = CausalLMPadCollator(tok.pad_token_id)\n","\n","    # ---- Trainer (TRL SFT) ----\n","\n","\n","    trainer = SFTTrainer(\n","        model=model,\n","        args=sft_cfg,\n","        peft_config=lora_cfg,\n","        train_dataset=ds_tok[\"train\"],\n","        eval_dataset=ds_tok[\"eval\"],\n","        data_collator=collator,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n","    )\n","\n","    trainer.train()\n","\n","    # Save LoRA adapter\n","    adapter_dir = f\"{OUT_DIR}/adapter\"\n","    os.makedirs(adapter_dir, exist_ok=True)\n","    model.save_pretrained(adapter_dir)\n","    tok.save_pretrained(adapter_dir)\n","    print(\"Saved LoRA adapter to:\", adapter_dir)\n","\n","    # Try to merge for standalone export (may be unsupported with 4-bit base in some envs)\n","    try:\n","        merged = trainer.model.merge_and_unload()\n","        merged_dir = f\"{OUT_DIR}/merged\"\n","        os.makedirs(merged_dir, exist_ok=True)\n","        merged.save_pretrained(merged_dir, safe_serialization=True)\n","        tok.save_pretrained(merged_dir)\n","        print(\"Merged model saved to:\", merged_dir)\n","    except Exception as e:\n","        print(\"Skip merging (common with 4-bit base):\", e)\n","\n","    return trainer\n","\n","# --- Example usage (uncomment to run) ---\n","# trainer = train_demo_qlora(\n","#     MODEL_ID=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n","#     OUT_DIR=\"smollm2_360m_demo_qlora\",\n","#     DEMO_PROB=1.0\n","# )\n"],"metadata":{"id":"WMMX6-NmmD8I","executionInfo":{"status":"ok","timestamp":1756409690189,"user_tz":-120,"elapsed":123,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}}},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":["#### Execute function with given model training ID - 800 rows"],"metadata":{"id":"AlPi9LVZmw5A"}},{"cell_type":"code","source":["# Login to HG to download dataset and push trained model\n","from huggingface_hub import login\n","from google.colab import userdata\n","hf_token = userdata.get('hf_token')\n","login(token=hf_token)\n","\n","TRAINING_DIR = \"app/datasets/training\"\n","out_dir  = TRAINING_DIR + \"/smollm2_360m_demo_1para_qlora\"\n","\n","sft_cfg = SFTConfig(\n","    output_dir=OUT_DIR,\n","    num_train_epochs=EPOCHS,                    # 2–3 usually enough\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=16,        # eff batch ~64\n","    learning_rate=1.2e-4,\n","    lr_scheduler_type=\"cosine\",\n","    warmup_steps=3,                        # explicit small warmup for tiny runs\n","    logging_steps=5,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=2,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"eval_loss\",\n","    greater_is_better=False,\n","    fp16=True,\n","    gradient_checkpointing=False,             #Turn off checkpointing (needs a bit more VRAM on T4, but SmolLM2-360M QLoRA usually fits):\n","    optim=\"paged_adamw_8bit\",\n","    max_length=MAX_LEN,\n","    remove_unused_columns=False,           # keep tokenized fields\n","    report_to=\"none\", #wandb\n",")\n","\n","trainer = train_demo_qlora(\n","    MODEL_ID=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n","    OUT_DIR=out_dir,\n","    DEMO_PROB=1.0,\n","    DATASET_ID=\"zBotta/traffic-accidents-reports-800\",\n","    sft_cfg=sft_cfg,\n",")\n","plot_training_results(out_dir,trainer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9f3955d7b31940b88a3c20f03cc37d7e","5e73029e2f204b32b26373d976331908","68eed5f556bb48d4b412b5f6ee13a48a","fcd0239e5aa34779854c0a7f95c8bda6","0c63923fb365429c825f05daeadbe052","bdb8ecb14d674c55bf93efa2b29577f4","77ea4ab8683d4b86871badafd37bbc3c","d7064b72f1ba43baa8267e24aa1abbfd","e3a12514a79f48b0a7aef4886bb5b4fb","0fc775adf9eb4dff8ca09b975f900726","a2821e899a4e431aabd5a4e481350b8e","cfe1b34c5f514a0dac2a787a16644257","8179daaf591a411cbaedf086dae0bb1f","01b641d5e2f9491e986a201e0f8d598e","1d66d0949a294c9ab623357cafd373cf","2f39e91e1d714a299fba0cd582429c87","7b4edf81ec094ce2a4045cd8b4bf4dda","46788614d5e44feab2e5bae05376c314","e596172de8be404abc489858d02873fd","96a943beffe04005bc5e6a4cbddfcda9","c93edc81cc1b40bfa10ccfe646373aba","7b077a04d4d74c5e8fd4c52beddecad2","e1620ea556544a6c8acfd78a7125f62c","f99c5c813c41427e86cdaa0f53b476ba","2de32f9c6c7e4e4992005102f8d998ab","da2d28de28df4d498cf242685f53f631","10f19fd7dad24d5e9cae26ecf83184fc","513c9adcb8584fefb605754ca9b4979a","b683683cf7324940ab8a15b0b2d156d3","79adbf86e69440e18d3481d5491a41fe","d859c82d6b7d4e7bb4d94faab95a07dc","59a626cbd9c44595bea8f1871b215f19","4cf95c02e28344eb9948a5a363e7af79","6ec67e019e1b49e79a5905fc7fdd933a","fafc44a2b2994802b5629afa0cb2c00c","f19be4d7853a4715aa8354a011f94753","294876a233304acd98608c0fbcf0591e","3d42e0de4451401c93586ef042e87e16","e2a43aa1d3864932a88324dfae1fdf2d","2766e48d0ceb4cfca1fc10ead9940e25","44b8544d0fe042c28e76ca86c3dfdce2","042cb6e5374c4226823122bfcdd23268","2b2dfd811f374b93b3507530d38142f0","830e29a9e3154c408b1427ddcbc054d5","ee5ad9316c8543409ea0d8d622a3c9d2","45fa41cdd1f445439fc71814b71ce336","6dfb8092a8ac4f588d7940260b931447","fd54eceff7fd41acb5d2173394aadec0","0a61ff8086cc4bf590d84e210b7c3c96","13e1f3a18fb94d56a15220b726723f7d","68e0f21d83bf409a94cc40f9a1ad90df","b5d499ed227b4f07b1b98f6debcbc7b4","d5c0462214d049de84fdcb373ff4cd4e","5b07e844235f4a6f9fff1453937f426a","c299e31505f54e0d8d0ca0d743784ad0","3678f31c03454a2e88c3ea79000e2894","6251b141f11f4e6fabf5f5ebc978bc6e","cf2c1f560392431db9cacbbf98880c53","b91764e78ef44daea319152308875330","62e17a5c178d4b4580147afd83e9cbe2","f9faca9bba92435db1c015df89160cbe","25a2e054a95341049e0aa660ca6c9100","e230a7b4dab14e44bbbba24baa7536e9","cfd6edcaaf3d42d2b83489ac23dc504a","2d7d783233604ffb99f88a4dc1429c89","090c231acc0d4580b41c31737f905486","34221b33cc6342b1974d41224c2313ac","1abba2b6dead401f948c489d9dbfc314","f0ec01458309413c9918d8793e6dca1f","adfe24d773234e64b3bcf04e0a92dea9","755d4bb30cf04a54b980d2cc5221333d","9c05d83ba95f4b42a3211039155ac40f","2f20a9bed21e496b99f3d67ead3d085c","320c2405de9542bdbeed61ce8b4e2502","85085fe742304e4296465b3a7b456521","a3020ec3f1504c48a556ae735d0daded","1d48424b99d34ebcad586a864b30dce4","4ae6a5cd7f7544b39993e0074a989609","1c5e0b54492b49a29d51f71b2acf3397","2ba0bcaa3f2741d99b4992346dd6043b","e338c8db649741da85aa4ac4042d5e68","d9772f9508f545f2a1a9548e3f6bf42c","e8a019cca42e4e7fbad796905e0a3fd6","04ed73601af84e8cb1bfac9c0c6e0caf","c0c964dcbc0e47018d05ac186a890c65","b2cd0d447d174fcfa2935113fee5c46a","4b5455e65f7f4dc2809f68bd23c33c42","d52ee59bfa934b7c850964e086910232","a410a2e9f883484a8f84910315a71a81","2d80875d41e94d9581d6ba6044616af3","43968543077b4910aab421e56d36de2b","137137c3ed244bf98d7d639a6eee89d7","12e147e080c54929bf79a586ebd3a949","456089ff792d4aa98ba3bb24d2c1aa72","ff6a6972a668411985fc3dd66e6e8fc8","9939fec4506d47bf83e640ef804b7bf3","a1f69d118950406e92a7fa40542eca08","7c922645a595407bbecfb3524ffd2ce2","52a89939eccd4fdcb45c8b76c843f32c","6752ad14c46845798b4d4020983d1095","2f16014482354aa998975952ebb5b194","d74eb6eaebd74e4c91b839814a9489af","3bffd23b965f4c6db389c6142247d09a","0c0db42ad7784ac0935c2dc0e8680315","eb46eaf1702443ddadd5ba501107ddd6","b86194217e19459ead9b146e3cf8ec74","297ff762be66413d8a32de669d531d20","132adf59220942cc97a43a45e4aabb56","f623871eeb87428a8161f9e801182d93","e3accf199a9c4ed99294164f6bcf3c1d"]},"id":"yqUQV1SXmK4P","executionInfo":{"status":"ok","timestamp":1756411865903,"user_tz":-120,"elapsed":2147823,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"328d3b3d-89f0-43cc-dd9a-7a8fa08f7baf"},"execution_count":70,"outputs":[{"output_type":"display_data","data":{"text/plain":["README.md: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f3955d7b31940b88a3c20f03cc37d7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/629 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfe1b34c5f514a0dac2a787a16644257"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating eval split:   0%|          | 0/70 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1620ea556544a6c8acfd78a7125f62c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ec67e019e1b49e79a5905fc7fdd933a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["train len before validation: 629\n","eval  len before validation: 70\n"]},{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/629 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee5ad9316c8543409ea0d8d622a3c9d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/70 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3678f31c03454a2e88c3ea79000e2894"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["train len after validation: 629\n","eval  len after validation: 70\n"]},{"output_type":"display_data","data":{"text/plain":["Building demo prompts + tokenizing:   0%|          | 0/629 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34221b33cc6342b1974d41224c2313ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Building demo prompts + tokenizing:   0%|          | 0/70 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ae6a5cd7f7544b39993e0074a989609"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["trainable params: 4,341,760 || all params: 366,162,880 || trainable%: 1.1857\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Truncating train dataset:   0%|          | 0/629 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a410a2e9f883484a8f84910315a71a81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Truncating eval dataset:   0%|          | 0/70 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6752ad14c46845798b4d4020983d1095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [150/150 35:05, Epoch 15/15]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.330900</td>\n","      <td>1.313430</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.197100</td>\n","      <td>1.208333</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.131600</td>\n","      <td>1.125862</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>1.053300</td>\n","      <td>1.066727</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>1.021300</td>\n","      <td>1.027761</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.970400</td>\n","      <td>1.003372</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.935400</td>\n","      <td>0.984088</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.940100</td>\n","      <td>0.971106</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.915400</td>\n","      <td>0.961902</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.885500</td>\n","      <td>0.955039</td>\n","    </tr>\n","    <tr>\n","      <td>11</td>\n","      <td>0.895000</td>\n","      <td>0.950263</td>\n","    </tr>\n","    <tr>\n","      <td>12</td>\n","      <td>0.883300</td>\n","      <td>0.947767</td>\n","    </tr>\n","    <tr>\n","      <td>13</td>\n","      <td>0.878500</td>\n","      <td>0.946352</td>\n","    </tr>\n","    <tr>\n","      <td>14</td>\n","      <td>0.880400</td>\n","      <td>0.945937</td>\n","    </tr>\n","    <tr>\n","      <td>15</td>\n","      <td>0.869600</td>\n","      <td>0.945796</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved LoRA adapter to: app/datasets/training/smollm2_360m_demo_1para_qlora/adapter\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Merged model saved to: app/datasets/training/smollm2_360m_demo_1para_qlora/merged\n","Saved raw logs to: app/datasets/training/smollm2_360m_demo_1para_qlora/training_logs.csv\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 700x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 700x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 700x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 700x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Best eval loss: 0.9458 at step 150 (perplexity ~ 2.57)\n","Final train loss: 0.8696 at step 150\n"]}]},{"cell_type":"markdown","source":["#### Push trained model to HF"],"metadata":{"id":"kd577093-Tyq"}},{"cell_type":"code","source":["repo_id = \"zBotta/smollm2-accident-reporter-360m\"\n","\n","from huggingface_hub import HfApi, create_repo, upload_folder, login\n","merged_dir = f\"{out_dir}/merged\"\n","from google.colab import userdata\n","hf_token = userdata.get('hf_token')\n","\n","login(token=hf_token)\n","\n","api = HfApi(token=hf_token)\n","# api.create_repo(repo_id, private=False, repo_type=\"model\")\n","upload_folder(folder_path=merged_dir, repo_id=repo_id, repo_type=\"model\")\n","print(\"Pushed:\", repo_id)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":131,"referenced_widgets":["cd7347b1fef64ed2800fe78b4eb2ca21","450e99a9840844cd874f093c96c7c72c","85c46d970cb84846947254af56e908bb","167621dfe72a4c87ab78e9484449204d","09ba44c732f0421ead65ee868abb1503","76bce8aa02cd458ba21f2b2afde354cd","904ae34ae22f493ba76cf32f3f43b20a","c9c7e39f383240a0bc250ff6d15d9037","6a631efc69a346e78f2da5698397ede4","8dfacef2f66648c7baa9deadc8655518","c65d0e74180a4d828396ea43ca483a1b","b1e3b4045ce04a7ba5f5316d88cb7cb6","12fe40417c554b57bbf7ed5fb7201bd5","3099f441a0a74fa1bca8cd241c8c2f40","9dd4bd663a864af1b19b9bf9935607c2","b27a5627d5244030a21c388020427fc7","996f290b188644eead15e2b74215abc9","f7a91f7fb0344525a1aaece9235e28ce","b7c4a3cf4dbf4d6f94bb0fe322070109","e5c46453fdbc4087844f90c13b5b9767","0930859dbc084bdab8301499ca8c7016","87932f18913c4ef4a396e060bc7f2500","21837e48aab94b2ba1d7f22afe9cd2dd","49a704bca81841e2818e591b4f6a7a5d","155e64e593774f138bde469b60f9686d","1b078da7e375483b865a3da68b4cb7cb","7956413cbe49424294756b74f5bb5f98","6c79cf3d46a845338ed5c809050f5c63","dbf9cb7b141546b3b8536598e0863a41","57a1eb2e1c664f9cb47be75d981589ff","7d5db0328ea94753ade163a5362fdc72","5a6cebe50e3446b38dc3599c1eb8aa4f","6f16d59ab376463fa1f212160c6e2cef"]},"id":"X_v3WEl9-V9j","executionInfo":{"status":"ok","timestamp":1756412141484,"user_tz":-120,"elapsed":11488,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"2adbc758-a3b1-4663-c5b1-17d3a7f596f7"},"execution_count":72,"outputs":[{"output_type":"display_data","data":{"text/plain":["Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd7347b1fef64ed2800fe78b4eb2ca21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["New Data Upload                         : |          |  0.00B /  0.00B            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1e3b4045ce04a7ba5f5316d88cb7cb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  ...para_qlora/merged/model.safetensors:  14%|#4        | 50.3MB /  352MB            "],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21837e48aab94b2ba1d7f22afe9cd2dd"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Pushed: zBotta/smollm2-accident-reporter-360m\n"]}]},{"cell_type":"markdown","source":["#### Execute function with given model training ID - 5k rows"],"metadata":{"id":"Llb78zoNLams"}},{"cell_type":"code","source":["# Login to HG to download dataset and push trained model\n","from huggingface_hub import login\n","from google.colab import userdata\n","hf_token = userdata.get('hf_token')\n","login(token=hf_token)\n","\n","TRAINING_DIR = \"app/datasets/training\"\n","out_dir  = TRAINING_DIR + \"/smollm2_360m_demo_1para_qlora_5k\"\n","\n","trainer = train_demo_qlora(\n","    MODEL_ID=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n","    OUT_DIR=out_dir,\n","    DEMO_PROB=1.0,\n","    DATASET_ID=\"zBotta/traffic-accidents-reports-5k\",\n","    EPOCHS=15,\n",")\n","plot_training_results(out_dir,trainer)"],"metadata":{"id":"0lU1Ao7VLe3_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Push trained model to HF"],"metadata":{"id":"snNWydHWLwk_"}},{"cell_type":"code","source":["repo_id = \"zBotta/smollm2-accident-reporter-360m-5k\"\n","\n","from huggingface_hub import HfApi, create_repo, upload_folder, login\n","merged_dir = f\"{out_dir}/merged\"\n","from google.colab import userdata\n","hf_token = userdata.get('hf_token')\n","\n","login(token=hf_token)\n","\n","api = HfApi(token=hf_token)\n","# api.create_repo(repo_id, private=False, repo_type=\"model\")\n","upload_folder(folder_path=merged_dir repo_id=repo_id, repo_type=\"model\")\n","print(\"Pushed:\", repo_id)"],"metadata":{"id":"QW_hzSfwLykN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Free up VRAM"],"metadata":{"id":"VNr_RRTQK6ol"}},{"cell_type":"code","source":["# Free VRAM after training (safe cleanup)\n","import gc, torch\n","\n","def free_vram(trainer=None, *extra_objs):\n","    # 1) ask Accelerate to release buffers (if present)\n","    try:\n","        if trainer is not None and hasattr(trainer, \"accelerator\"):\n","            trainer.accelerator.wait_for_everyone()\n","            trainer.accelerator.free_memory()\n","    except Exception as e:\n","        print(\"accelerator.free_memory() skipped:\", e)\n","\n","    # 2) move big models to CPU so CUDA refs can drop\n","    for obj in (trainer,)+extra_objs if trainer is not None else extra_objs:\n","        try:\n","            m = getattr(obj, \"model\", None) or obj\n","            if hasattr(m, \"to\"):\n","                m.to(\"cpu\")\n","        except Exception:\n","            pass\n","\n","    # 3) drop strong references\n","    for name in list(globals().keys()):\n","        # keep notebook/system internals\n","        if name.startswith((\"__\", \"_\", \"In\", \"Out\")):\n","            continue\n","        try:\n","            val = globals()[name]\n","            if hasattr(val, \"to\") or hasattr(val, \"parameters\"):  # models, trainers, tokenizers (sometimes hold buffers)\n","                globals()[name] = None\n","        except Exception:\n","            pass\n","\n","    # 4) garbage-collect Python objects and release CUDA caches\n","    gc.collect()\n","    try:\n","        torch.cuda.empty_cache()\n","        torch.cuda.ipc_collect()\n","        torch.cuda.synchronize()\n","    except Exception:\n","        pass\n","\n","# Delete trainer data\n","free_vram(trainer)                   # if you only have the trainer\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"1-l2Tcm675RL","executionInfo":{"status":"error","timestamp":1756409637489,"user_tz":-120,"elapsed":60,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"26219232-93ba-4225-fd0c-1977fa0959a7"},"execution_count":68,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'merged' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1665212472.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# EXAMPLES (uncomment the ones you have):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mfree_vram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# if you created a merged model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;31m# free_vram(trainer)                   # if you only have the trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# free_vram(None, model, tok)          # if you kept separate references\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'merged' is not defined"]}]},{"cell_type":"markdown","source":["## Testing ce_sim/be_sim as metric for training\n","\n","We are going to:\n","- use the existing **MetricsEvaluator** class\n","- Create a compute_metrics function to calculate be_sim and ce_sim on each generated text at the end of the epoch\n","- **NOTA**: It is really slow, we are going to keep with loss eval\n","\n"],"metadata":{"id":"iqqxQbw9-BlL"}},{"cell_type":"code","source":["!pip -q install sentence_transformers bert-score evaluate rouge_score"],"metadata":{"id":"Pmyu9kETCcNp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pred = \"On June 10, 2025, at 9:15 AM in the Formulation Area (Production Building 2), technician Rahul Mehta used a non-calibrated pH meter to adjust the buffer, leading to an incorrect pH.\"\n","ref = \"In the Formulation Area I had a problem with the ph\"\n","meval.set_cross_encoder_score(ref, [pred], )\n","print(meval.get_cross_encoder_score())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ENA4xh10blkO","executionInfo":{"status":"ok","timestamp":1756298664813,"user_tz":-120,"elapsed":244,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"36d3a29f-0c3a-40fd-fe1d-16e5733b412e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.3631974 1.0000001 0.3631974]\n"]}]},{"cell_type":"code","source":["from sentence_transformers.cross_encoder import CrossEncoder\n","import numpy as np\n","import re\n","from app.mods.metricsEvaluator import MetricsEvaluator\n","\n","meval = MetricsEvaluator()"],"metadata":{"id":"BU94BCUbCi2W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Training for maximizing be_sim"],"metadata":{"id":"mWz4_vSD-ZMV"}},{"cell_type":"code","source":["# === ONE-CELL QLoRA TRAINER: SmolLM2-360M-Instruct with trl.SFTConfig (no char clipping) ===\n","# Colab tip: Runtime -> Change runtime type -> GPU (T4)\n","\n","import os, re, json, torch\n","from datasets import load_dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, EarlyStoppingCallback\n","from peft import LoraConfig\n","from trl import SFTTrainer, SFTConfig\n","\n","MODEL_ID = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n","TRAINING_DIR = \"app/datasets/training\"\n","OUT_DIR  = TRAINING_DIR + \"/smollm2_360m_onepara_lora\"\n","\n","# 4-bit QLoRA base (tiny VRAM/RAM footprint)\n","bnb = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_compute_dtype=torch.float16\n",")\n","\n","tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n","if tok.pad_token_id is None:\n","    tok.pad_token = tok.eos_token\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_ID,\n","    quantization_config=bnb,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")\n","model.config.use_cache = False  # needed for grad checkpointing\n","\n","# LoRA config (light but effective for ~500 rows)\n","peft_cfg = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","ds = load_dataset(\"json\", data_files={\"train\":TRAINING_DIR+\"/train.jsonl\",\"eval\":TRAINING_DIR+\"/eval.jsonl\"})\n","\n","print(f\"DS_1: {ds}\")\n","\n","\n","def one_line(s: str) -> str:\n","    s = str(s).replace(\"\\n\",\" \")\n","    return re.sub(r\"\\s+\",\" \", s).strip()\n","\n","INSTR = (\n","  \"Write ONE SINGLE PARAGRAPH in English that includes ALL given facts: what happened, when, where, who, how, why \"\n","  \"(root cause), and contingency/corrective actions. Neutral tone. No bullet points, no headings, no lists, no JSON. \"\n","  \"DO NOT invent details. Output must be a single line (no line breaks).\"\n",")\n","RESP_TMPL = \"### Response:\\n\"  # SFTTrainer will mask everything before this marker as prompt\n","MAX_LEN = 1024\n","\n","def find_subsequence(xs: list[int], ys: list[int]) -> int:\n","    \"\"\"Return start index of ys inside xs, or -1 if not found.\"\"\"\n","    n, m = len(xs), len(ys)\n","    if m == 0 or m > n: return -1\n","    for i in range(n - m + 1):\n","        if xs[i:i+m] == ys:\n","            return i\n","    return -1\n","\n","def tokenize_and_mask(example: dict) -> dict:\n","    # Build full prompt -> \"### Instruction ... INPUT ... ### Response:\\n + target\"\n","    text_in  = one_line(example[\"input\"])\n","    text_out = one_line(example[\"target\"])\n","    full = f\"### Instruction:\\n{INSTR}\\n\\nINPUT:\\n{text_in}\\n\\n{RESP_TMPL}{text_out}\"\n","\n","    enc = tok(\n","        full,\n","        truncation=True,\n","        max_length=MAX_LEN,\n","        padding=False,             # pad later in collator\n","        return_tensors=None\n","    )\n","    input_ids = enc[\"input_ids\"]\n","    labels    = input_ids.copy()\n","\n","    # Locate response template and mask everything before the end of it\n","    rt_ids = tok(RESP_TMPL, add_special_tokens=False)[\"input_ids\"]\n","    start = find_subsequence(input_ids, rt_ids)\n","    if start == -1:\n","        # If marker not found (rare after truncation), skip supervision on whole sample\n","        labels[:] = [-100] * len(labels)\n","    else:\n","        # Mask up to the end of the template tokens\n","        cut = start + len(rt_ids)\n","        labels[:cut] = [-100] * cut\n","\n","    return {\n","        \"input_ids\": input_ids,\n","        \"attention_mask\": enc[\"attention_mask\"],\n","        \"labels\": labels\n","    }\n","\n","ds_tok = ds.map(tokenize_and_mask, remove_columns=ds[\"train\"].column_names, desc=\"Tokenizing & masking\")\n","\n","# print(f\"DS_2: {ds_tok}\")\n","\n","ds_tok = ds_tok.remove_columns([c for c in ds_tok[\"train\"].column_names\n","                                if c not in (\"input_ids\",\"attention_mask\",\"labels\")])\n","\n","# Make sure the dataset yields torch tensors with those keys\n","ds_tok.set_format(type=\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n","# print(f\"DS_3: {ds_tok}\")\n","\n","# Simple collator: pad inputs and labels to max length in batch\n","class CausalLMPadCollator:\n","    def __init__(self, tokenizer, label_pad_id=-100):\n","        self.tok = tokenizer\n","        self.label_pad_id = label_pad_id\n","\n","    def __call__(self, features: list[dict]) -> dict[str, torch.Tensor]:\n","        max_len = max(len(f[\"input_ids\"]) for f in features)\n","        input_ids, attn, labels = [], [], []\n","        for f in features:\n","            pad = max_len - len(f[\"input_ids\"])\n","            input_ids.append(f[\"input_ids\"] + [self.tok.pad_token_id] * pad)\n","            attn.append(f[\"attention_mask\"] + [0] * pad)\n","            labels.append(f[\"labels\"] + [self.label_pad_id] * pad)\n","        return {\n","            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n","            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n","            \"labels\": torch.tensor(labels, dtype=torch.long),\n","        }\n","\n","collator = CausalLMPadCollator(tok)\n","\n","\n","# Keep raw targets so compute_metrics can use them\n","eval_ref_texts = [one_line(ex[\"target\"]) for ex in ds[\"eval\"]]  # BEFORE tokenization\n","\n","def make_compute_ce_sim(meval: MetricsEvaluator,\n","                        tokenizer,\n","                        eval_ref_texts,\n","                        use_be: bool = True,     # compute bi-encoder similarity\n","                        use_ce: bool = False,    # compute cross-encoder similarity (slower)\n","                        max_new_tokens=300,\n","                        batch_size=8):\n","    @torch.inference_mode()\n","    def compute_ce_sim(_eval_pred):  # ignore logits; we generate here\n","        model.eval()\n","        preds = []\n","        # Need to regenerate text from input prompts, not decode raw predictions which may have -100s\n","        # The prompt needs to be built from the original input text, not the tokenized version.\n","        # Get the original input texts from the evaluation dataset\n","        refs = [ex[\"input\"] for ex in ds[\"eval\"]]\n","\n","        for i in range(0, len(refs), batch_size):\n","            batch_inputs = refs[i:i+batch_size]\n","            prompts = [f\"### Instruction:\\n{INSTR}\\n\\nINPUT:\\n{one_line(inp)}\\n\\n{RESP_TMPL}\"\n","                       for inp in batch_inputs]\n","\n","            ids = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n","            out = model.generate(**ids, max_new_tokens=max_new_tokens, do_sample=False,\n","                                 eos_token_id=tokenizer.eos_token_id)\n","            texts = tokenizer.batch_decode(out, skip_special_tokens=True)\n","            preds.extend([one_line(t.split(RESP_TMPL,1)[-1]) for t in texts])\n","\n","        ce_vals, be_vals = [], []\n","        if use_be:\n","          for ref, pred in zip(refs, preds):\n","              # compare_all_texts=False → only ref↔pred; is_test_bench=True → drop ref↔ref\n","              meval.set_bi_encoder_score(ref, [pred], compare_all_texts=False, is_test_bench=True)\n","              be = meval.get_bi_encoder_score()\n","              be_vals.append(float(np.asarray(be).reshape(-1)[0]))\n","\n","        if use_ce:\n","            for ref, pred in zip(refs, preds):\n","                meval.set_cross_encoder_score(ref, [pred])\n","                ce = meval.get_cross_encoder_score()\n","                ce_vals.append(float(np.asarray(ce).reshape(-1)[0]))\n","\n","        # score in small batches to save memory\n","        # scores = []\n","        # for p, r in zip(preds, eval_ref_texts):\n","        #     meval.set_cross_encoder_score(r, [p])\n","        #     score = float(meval.get_cross_encoder_score()[0]) # Corrected: get the scalar score\n","        #     scores.append(score)\n","        # return {\"ce_sim\": float(np.mean(scores)),\n","        #         \"ce_sim_std\": float(np.std(scores))}\n","\n","        metrics = {}\n","        if use_be:\n","            metrics[\"be_sim_mean\"] = float(np.mean(be_vals)) if be_vals else 0.0\n","            metrics[\"be_sim_std\"]  = float(np.std(be_vals))  if be_vals else 0.0\n","        if use_ce:\n","            metrics[\"ce_sim_mean\"] = float(np.mean(ce_vals)) if ce_vals else 0.0\n","            metrics[\"ce_sim_std\"]  = float(np.std(ce_vals))  if ce_vals else 0.0\n","        return metrics\n","    return compute_ce_sim\n","\n","compute_metrics = make_compute_ce_sim(meval, tok, eval_ref_texts, batch_size=4)\n","\n","# --- SFTConfig (replaces TrainingArguments) ---\n","sft_cfg = SFTConfig(\n","    output_dir=OUT_DIR,\n","    num_train_epochs=4,                       # 2 epochs is plenty for ~500 rows\n","    per_device_train_batch_size=4,\n","    gradient_accumulation_steps=16,           # eff batch ~= 64\n","    gradient_checkpointing=False,             #Turn off checkpointing (needs a bit more VRAM on T4, but SmolLM2-360M QLoRA usually fits):\n","    learning_rate=1.5e-4,\n","    lr_scheduler_type=\"cosine\",\n","    warmup_steps=3,\n","    eval_strategy=\"epoch\", # \"steps\"\n","    save_strategy=\"epoch\", # \"steps\"\n","    metric_for_best_model= \"be_sim_mean\",#\"ce_sim\",  # Cross-encoding similarity\n","    greater_is_better=True,   # Maximize metric\n","    save_total_limit=2,\n","    logging_steps=1,\n","    fp16=True,                                # T4-friendly\n","    optim=\"paged_adamw_8bit\",\n","    max_grad_norm=0.5,\n","    max_length =MAX_LEN,                      # handled by SFTTrainer when set here\n","    packing=False,\n","    remove_unused_columns=False,           # important for pre-tokenized inputs\n","    report_to=\"none\"  # wandb\n",")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=sft_cfg,                             # <-- using SFTConfig\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n","    train_dataset=ds_tok[\"train\"], # Use the tokenized dataset\n","    eval_dataset=ds_tok[\"eval\"],   # Use the tokenized dataset\n","    data_collator = collator,\n","    peft_config=peft_cfg,\n","    compute_metrics=compute_metrics, # custom ce_sim and be_sim\n",")\n","trainer.train()\n","final_metrics = trainer.evaluate()      # logs eval_loss into state.log_history\n","print(\"Final eval:\", final_metrics)\n","\n","# Save LoRA adapter\n","adapter_dir = f\"{OUT_DIR}/adapter\"\n","trainer.model.save_pretrained(adapter_dir)\n","tok.save_pretrained(adapter_dir)\n","print(\"Saved LoRA adapter to:\", adapter_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["9d2fb75975f149f1b2a7f7e02a6e5169","834628935560405e91003b3150351223","1d9107efc4e949d0a2b9384d60ace9f4","c274e82a329a4e6e8fca7f7137487cfe","66c16833e56e40f18d81e5f45ac28d75","1df76c4cc7ac46c3b456c8188e4e8393","9fcbdc1d4f2f430b84f4aec4b5a8be61","c4727c343f2d414398ef819cf4dcba99","9a78951bd3644f9295215b89fe527039","74406139930845adb23bea6c26f50d05","b9ca57eae0c949bd9313c88da8939790","5c70ad981ca74b53b9bf82ec25a0e5d5","287eb529bbe248e994dac8d19f263b4a","9554edd9226c46a492261f302422c96a","9f1ef326143647a4943cc7054a2b687d","d3a5e940bd9f4247b3047ab44b74411a","bbee85542c724f8cb0734100be8b5358","bb89b8498a9648c0bdef95ca7cf0f539","9c5dd4a6dcf44902a25f1d1ff8141f62","5e35e356703c43f59e7ad64ae5af4565","708fc1be3e4b4356bf6b253e04fb0ab4","721f8ba24ffc483dbcc13ae583bb1dab","d1bddcd8d70346c8b273958adfd5c8c4","647f8049edd746e3a4b3b69d57c21660","1ef617ddb496492d9da30bc39f4aecca","3db8e785fd7e45ea8de15dffcc8f1d7a","c0096cbb73184fbf873fcadfc3b5c0bf","c0b320d94a4c453f9828513871a75845","380b81d269a74e00b55e4ac2c8ba8fb3","0fc060b3181741d5840abb2a1c488744","40983bbdbfd14e5ca29696ddbf14b870","24975f88be964369b346696c1072371a","7baf7aad542844a3abc6664ec986f4a5","ddda5f87aab841ce8ebecbf93227ace8","7c45fddc4dd44cb1bc3dcd871aa98636","adb2dd99a623442fb1d88923abfc7309","b6bf841659514ef58669971fde235d4f","4bb1f704911447849e0c66b00256fa97","2f9d09b9c4244560855d2d07afebe29c","a6a136298bda43108b7a79e96cfae56a","9e06a60caf2a4a629c647c395d65f2ee","e1bca00910d74ab2baeb72226a16cc23","f1240f2e4fb648b1b2a6ab18ba8462de","36f949fe6cf24d3894480e40bc715c52"]},"id":"2CriIc9F-bFR","executionInfo":{"status":"error","timestamp":1756306060987,"user_tz":-120,"elapsed":1682356,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"50f09b9e-7bf4-4811-ea4a-90be6c998827"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DS_1: DatasetDict({\n","    train: Dataset({\n","        features: ['input', 'target'],\n","        num_rows: 699\n","    })\n","    eval: Dataset({\n","        features: ['input', 'target'],\n","        num_rows: 70\n","    })\n","})\n"]},{"output_type":"display_data","data":{"text/plain":["Tokenizing & masking:   0%|          | 0/699 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d2fb75975f149f1b2a7f7e02a6e5169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Tokenizing & masking:   0%|          | 0/70 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c70ad981ca74b53b9bf82ec25a0e5d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Truncating train dataset:   0%|          | 0/699 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1bddcd8d70346c8b273958adfd5c8c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Truncating eval dataset:   0%|          | 0/70 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddda5f87aab841ce8ebecbf93227ace8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='23' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [23/44 17:55 < 17:55, 0.02 it/s, Epoch 2/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Be Sim Mean</th>\n","      <th>Be Sim Std</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.295400</td>\n","      <td>1.322015</td>\n","      <td>0.748928</td>\n","      <td>0.118401</td>\n","    </tr>\n","  </tbody>\n","</table><p>\n","    <div>\n","      \n","      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9/9 00:04]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1863432576.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# custom ce_sim and be_sim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m )\n\u001b[0;32m--> 236\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0mfinal_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# logs eval_loss into state.log_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Final eval:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2239\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2698\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   2699\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2700\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3135\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3137\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3138\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4253\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4254\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4255\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4256\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4542\u001b[0m             \u001b[0meval_set_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"losses\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_losses\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"loss\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4543\u001b[0m             \u001b[0meval_set_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_inputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"inputs\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4544\u001b[0;31m             metrics = self.compute_metrics(\n\u001b[0m\u001b[1;32m   4545\u001b[0m                 \u001b[0mEvalPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_set_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4546\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1863432576.py\u001b[0m in \u001b[0;36mcompute_ce_sim\u001b[0;34m(_eval_pred)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             out = model.generate(**ids, max_new_tokens=max_new_tokens, do_sample=False,\n\u001b[0m\u001b[1;32m    162\u001b[0m                                  eos_token_id=tokenizer.eos_token_id)\n\u001b[1;32m    163\u001b[0m             \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2616\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2617\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2618\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2619\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3599\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3600\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3601\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3603\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 460\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    461\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1081\u001b[0m                         \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Restore original forward methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_forward\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonkey_patched_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    391\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    290\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;31m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;31m# The reason is that in some cases, an error can occur that backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;31m# 1. Dequantize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# 2. MatmulnN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdequantize_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;31m# 3. Save state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m         \u001b[0mabsmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdequantize_blockwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    984\u001b[0m         \u001b[0mabsmax\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mabsmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mdequantize_blockwise\u001b[0;34m(A, quant_state, absmax, code, out, blocksize, nested)\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m     return torch.ops.bitsandbytes.dequantize_blockwise.default(\n\u001b[0m\u001b[1;32m    709\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mabsmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;31m# that are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_P\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;31m# Use positional-only argument to avoid naming collision with aten ops arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    927\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/library.py\u001b[0m in \u001b[0;36mfunc_no_dynamo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_disable_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc_no_dynamo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/backends/cuda/ops.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(A, absmax, code, blocksize, dtype)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsmax\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0m_dequantize_blockwise_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocksize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/backends/cuda/ops.py\u001b[0m in \u001b[0;36m_dequantize_blockwise_impl\u001b[0;34m(A, absmax, code, blocksize, dtype, out)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0m_get_tensor_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         )\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36m_get_tensor_stream\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_tensor_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;31m# We use the raw stream for performance reasons.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getCurrentRawStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["# Data treatment and HF push dataset"],"metadata":{"id":"5mrZwgYmALzx"}},{"cell_type":"markdown","source":["### Importing and treating Excel Dataset"],"metadata":{"id":"Sdkl-q9gSw6c"}},{"cell_type":"code","source":["!pip -q install pandas openpyxl"],"metadata":{"id":"QKrfg07FTsGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert Excel -> train/eval datasets for one-paragraph report SFT\n","# Requirements: pandas, openpyxl\n","# In Colab: !pip -q install pandas openpyxl\n","\n","import pandas as pd, re, json, os\n","from sklearn.model_selection import train_test_split\n","\n","DATASET_ID = \"5k\" #800\n","\n","# === user settings ===\n","excel_path = f\"app/datasets/training/training_traffic_accident_reports_{DATASET_ID}.xlsx\"\n","test_excel_path = \"app/datasets/traffic_accident_reports_collection.xlsx\"\n","sheet_name = \"TRAFFIC_ACCIDENT\"                  # or \"Sheet1\"\n","max_chars = 700                 # target paragraph limit\n","train_frac = 0.9\n","random_state = 42\n","out_dir = \"app/datasets/training\"\n","os.makedirs(out_dir, exist_ok=True)\n","\n","# Map flexible headers to canonical keys (lower-case, no spaces)\n","colmap = {\n","    \"what\": \"what\",\n","    \"when\": \"when\",\n","    \"where\": \"where\",\n","    \"who\":  \"who\",\n","    \"how\":  \"how\",\n","    \"why\":  \"why\",\n","    \"contingencyactions\": \"contingency_actions\",\n","    \"contingency actions\": \"contingency_actions\",\n","    \"report\": \"reference_report\",\n","    \"reference_report\": \"reference_report\",\n","}\n","\n","def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n","    new_cols = {}\n","    for c in df.columns:\n","        key = re.sub(r\"\\s+\", \" \", str(c)).strip().lower()\n","        key_nospace = key.replace(\" \", \"\")\n","\n","        # General mapping\n","        if key in colmap:\n","            new_cols[c] = colmap[key]\n","        elif key_nospace in colmap:\n","            new_cols[c] = colmap[key_nospace]\n","        else:\n","            new_cols[c] = key_nospace  # keep something sensible\n","    return df.rename(columns=new_cols)\n","\n","def one_line(s: str) -> str:\n","    if pd.isna(s): s = \"\"\n","    s = str(s).replace(\"\\n\", \" \")\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s\n","\n","def build_input(row: dict) -> str:\n","    # Compact 5W1H list — this is what your Colab tester expects as input\n","    parts = []\n","    if row.get(\"what\"): parts.append(f\"What: {row['what']}\")\n","    if row.get(\"when\"): parts.append(f\"When: {row['when']}\")\n","    if row.get(\"where\"): parts.append(f\"Where: {row['where']}\")\n","    if row.get(\"who\"): parts.append(f\"Who: {row['who']}\")\n","    if row.get(\"how\"): parts.append(f\"How: {row['how']}\")\n","    if row.get(\"why\"): parts.append(f\"Why: {row['why']}\")\n","    if row.get(\"contingency_actions\"): parts.append(f\"ContingencyActions: {row['contingency_actions']}\")\n","    return \"\\n\".join(parts)\n","\n","# --- Load & normalize ---\n","df = pd.read_excel(excel_path, sheet_name=sheet_name)\n","df = normalize_columns(df)\n","df_test = pd.read_excel(test_excel_path, sheet_name=sheet_name)\n","df_test = normalize_columns(df_test)\n","\n","# Keep only the columns we care about; fill missing\n","needed = [\"what\",\"when\",\"where\",\"who\",\"how\",\"why\",\"contingency_actions\",\"reference_report\"]\n","for k in needed:\n","    if k not in df.columns:\n","        df[k] = \"\"\n","df = df[needed].fillna(\"\")\n","\n","# Build input/target\n","records = []\n","too_long = 0\n","empty_targets = 0\n","for _, r in df.iterrows():\n","    row = {k: one_line(r[k]) for k in needed}\n","    inp = build_input(row)\n","    tgt = one_line(row[\"reference_report\"])\n","\n","    if len(tgt) > max_chars: too_long += 1\n","\n","    records.append({\"input\": inp, \"target\": tgt})\n","\n","# Keep only the columns we care about; fill missing\n","needed = [\"what\",\"when\",\"where\",\"who\",\"how\",\"why\",\"contingency_actions\",\"reference_report\"]\n","for k in needed:\n","    if k not in df_test.columns:\n","        df_test[k] = \"\"\n","df_test = df_test[needed].fillna(\"\")\n","\n","# Build input/target\n","test_recs = []\n","too_long = 0\n","empty_targets = 0\n","for _, r in df_test.iterrows():\n","    row = {k: one_line(r[k]) for k in needed}\n","    inp = build_input(row)\n","    tgt = one_line(row[\"reference_report\"])\n","\n","    if len(tgt) > max_chars: too_long += 1\n","\n","    test_recs.append({\"input\": inp, \"target\": tgt})\n","\n","\n","# Split train/eval\n","train_recs, eval_recs = train_test_split(records, test_size=1-train_frac, random_state=random_state)\n","print(f\"Rows train prepared: {len(train_recs)}\")\n","print(f\"Rows eval prepared: {len(eval_recs)}\")\n","print(f\"Rows test prepared: {len(test_recs)}\")\n","# Save JSONL\n","\n","def to_jsonl(path, data):\n","    with open(path, \"w\", encoding=\"utf-8\") as f:\n","        for d in data:\n","            f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n","\n","to_jsonl(os.path.join(out_dir, \"train.jsonl\"), train_recs)\n","to_jsonl(os.path.join(out_dir, \"eval.jsonl\"),  eval_recs)\n","to_jsonl(os.path.join(out_dir, \"test.jsonl\"), test_recs)\n","\n","print(\"Wrote:\",\n","      os.path.join(out_dir, \"train.jsonl\"),\n","      os.path.join(out_dir, \"eval.jsonl\"),\n","      os.path.join(out_dir, \"test.jsonl\"),\n","      sep=\"\\n - \")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o3s5kibRSy-F","executionInfo":{"status":"ok","timestamp":1756409248782,"user_tz":-120,"elapsed":944,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"e5931407-fab6-46b0-f681-39cae01f753b"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["Rows train prepared: 629\n","Rows eval prepared: 70\n","Rows test prepared: 101\n","Wrote:\n"," - app/datasets/training/train.jsonl\n"," - app/datasets/training/eval.jsonl\n"," - app/datasets/training/test.jsonl\n"]}]},{"cell_type":"markdown","source":["### Push dataset to HF"],"metadata":{"id":"vg1nsJUSB4Mu"}},{"cell_type":"code","source":["# -------------------- BUILD & PUSH TO HF HUB --------------------\n","from datasets import Dataset, DatasetDict\n","from huggingface_hub import HfApi, login, create_repo\n","\n","\n","# 1) Login (uses env var if set; else interactive prompt)\n","from google.colab import userdata\n","HF_TOKEN = userdata.get('hf_token')\n","try:\n","    login(token=HF_TOKEN) if HF_TOKEN else login()\n","except Exception as e:\n","    print(\"⚠️ Hugging Face login skipped/failed:\", e)\n","\n","# 2) Create DatasetDict from your records\n","train_ds = Dataset.from_list(train_recs)   # expects dicts with at least {'input': ..., 'target': ...}\n","eval_ds  = Dataset.from_list(eval_recs)\n","test_ds = Dataset.from_list(test_recs)\n","ds = DatasetDict({\"train\": train_ds, \"eval\": eval_ds, \"test\": test_ds})\n","\n","# 3) Choose repo id (owner/repo). If you set DATASET_REPO manually, it will be used.\n","api = HfApi()\n","user = (api.whoami(token=HF_TOKEN) or {}).get(\"name\", None)\n","\n","repo_name = f\"accidents-reports-{DATASET_OD}\"\n","DATASET_REPO = globals().get(\"DATASET_REPO\", f\"{user}/{repo_name}\" if user else repo_name)\n","\n","# 4) Create (or reuse) the dataset repo\n","create_repo(DATASET_REPO, repo_type=\"dataset\", private=True, exist_ok=True)\n","\n","# 5) Optional: write a lightweight Dataset Card\n","card_path = os.path.join(out_dir, \"README.md\")\n","card = f\"\"\"---\n","pretty_name: Accident Reports (One Paragraph)\n","tasks:\n","- text2text-generation\n","language:\n","- en\n","size_categories:\n","- n<{len(train_recs)+len(eval_recs)+len(test_recs)}\n","---\n","\n","# Accident Reports (One Paragraph)\n","\n","Supervised pairs for one-paragraph accident reporting.\n","Each example has:\n","- `input`: \"What, When, Where, Who, How, Why, ContingencyActions\" facts in one string.\n","- `target`: a single-paragraph neutral report.\n","\n","**Splits**\n","- train: {len(train_recs)}\n","- eval:  {len(eval_recs)}\n","- test:  {len(test_recs)}\n","\"\"\"\n","with open(card_path, \"w\", encoding=\"utf-8\") as f:\n","    f.write(card)\n","\n","# 6) Push DatasetDict (structured HF dataset)\n","print(f\"Pushing DatasetDict to hub: {DATASET_REPO}\")\n","ds.push_to_hub(DATASET_REPO, private=True, commit_message=\"Initial dataset upload\")\n","\n","# 7) Also upload raw files (JSONL + README) into the dataset repo under /data\n","print(\"Uploading raw files & README...\")\n","api.upload_folder(\n","    repo_id=DATASET_REPO,\n","    repo_type=\"dataset\",\n","    folder_path=out_dir,\n","    path_in_repo=\"data\",\n","    commit_message=\"Add raw JSONL and dataset card\"\n",")\n","\n","print(f\"✅ Done. Your dataset is at: https://huggingface.co/datasets/{DATASET_REPO}\")"],"metadata":{"id":"_QBIg8OmAO9-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Grid Search on trained model"],"metadata":{"id":"JF2rrRz-hGGD"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","from google.colab import userdata\n","github_token = userdata.get('zbotta_token')\n","\n","token = github_token\n","username = \"zbotta\"\n","repo = 'reportingAgent'\n","%cd /content/drive/MyDrive/GitHub/{repo}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NLG_KdJXo3tW","executionInfo":{"status":"ok","timestamp":1756208209239,"user_tz":-120,"elapsed":2758,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"ab804cb1-26eb-4158-aadb-077bae5de0c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/GitHub/reportingAgent\n"]}]},{"cell_type":"code","source":["!git pull"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-p1p8UNKnwC","executionInfo":{"status":"ok","timestamp":1756208210350,"user_tz":-120,"elapsed":1076,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"d50e9e57-71d5-455d-e6db-ab6bbf4212f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}]},{"cell_type":"code","source":["!pip install -r requirements_colab.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N3n-gjEoh8ur","executionInfo":{"status":"ok","timestamp":1756208230000,"user_tz":-120,"elapsed":13955,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"04d6dc86-cb41-46e6-aa58-c9a462f9bef7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: absl-py~=1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 1)) (1.4.0)\n","Requirement already satisfied: aiohappyeyeballs~=2.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 2)) (2.6.1)\n","Requirement already satisfied: aiohttp~=3.12.15 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 3)) (3.12.15)\n","Requirement already satisfied: aiosignal~=1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 4)) (1.4.0)\n","Requirement already satisfied: annotated-types~=0.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 5)) (0.7.0)\n","Requirement already satisfied: anyio~=4.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 6)) (4.10.0)\n","Requirement already satisfied: attrs~=25.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 7)) (25.3.0)\n","Requirement already satisfied: audioread~=3.0.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 8)) (3.0.1)\n","Requirement already satisfied: bert-score~=0.3.13 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 9)) (0.3.13)\n","Requirement already satisfied: certifi~=2025.8.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 10)) (2025.8.3)\n","Requirement already satisfied: charset-normalizer~=3.4.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 11)) (3.4.3)\n","Requirement already satisfied: click~=8.2.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 12)) (8.2.1)\n","Requirement already satisfied: cloudpickle~=3.1.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 13)) (3.1.1)\n","Requirement already satisfied: colorama~=0.4.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 14)) (0.4.6)\n","Requirement already satisfied: contourpy~=1.3.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 15)) (1.3.3)\n","Requirement already satisfied: cycler~=0.12.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 16)) (0.12.1)\n","Requirement already satisfied: datasets~=4.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 17)) (4.0.0)\n","Requirement already satisfied: dill~=0.3.8 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 18)) (0.3.8)\n","Requirement already satisfied: distributed~=2025.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 19)) (2025.5.0)\n","Requirement already satisfied: diskcache~=5.6.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 20)) (5.6.3)\n","Requirement already satisfied: distro~=1.9.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 21)) (1.9.0)\n","Requirement already satisfied: docstring_parser~=0.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 22)) (0.17.0)\n","Requirement already satisfied: docutils~=0.21.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 23)) (0.21.2)\n","Requirement already satisfied: dotenv~=0.9.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 24)) (0.9.9)\n","Requirement already satisfied: et_xmlfile~=2.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 25)) (2.0.0)\n","Requirement already satisfied: evaluate~=0.4.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 26)) (0.4.5)\n","Requirement already satisfied: Farama-Notifications~=0.0.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 27)) (0.0.4)\n","Requirement already satisfied: fastapi~=0.116.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 28)) (0.116.1)\n","Requirement already satisfied: filelock~=3.18.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 29)) (3.18.0)\n","Requirement already satisfied: fonttools~=4.59.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 30)) (4.59.1)\n","Requirement already satisfied: frozenlist~=1.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 31)) (1.7.0)\n","Requirement already satisfied: fsspec~=2025.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 32)) (2025.3.0)\n","Requirement already satisfied: genson~=1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 33)) (1.3.0)\n","Requirement already satisfied: groq~=0.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 34)) (0.26.0)\n","Requirement already satisfied: h11~=0.16.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 35)) (0.16.0)\n","Requirement already satisfied: httpcore~=1.0.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 36)) (1.0.9)\n","Requirement already satisfied: httpx~=0.28.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 37)) (0.28.1)\n","Requirement already satisfied: huggingface-hub~=0.34.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 38)) (0.34.4)\n","Requirement already satisfied: idna~=3.10 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 39)) (3.10)\n","Requirement already satisfied: iniconfig~=2.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 40)) (2.1.0)\n","Requirement already satisfied: instructor~=1.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 41)) (1.10.0)\n","Requirement already satisfied: interegular~=0.3.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 42)) (0.3.3)\n","Requirement already satisfied: iso3166~=2.1.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 43)) (2.1.1)\n","Requirement already satisfied: Jinja2~=3.1.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 44)) (3.1.6)\n","Requirement already satisfied: jiter~=0.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 45)) (0.10.0)\n","Requirement already satisfied: joblib~=1.5.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 46)) (1.5.1)\n","Requirement already satisfied: jsonpath-ng~=1.7.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 47)) (1.7.0)\n","Requirement already satisfied: jsonpickle~=4.1.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 48)) (4.1.1)\n","Requirement already satisfied: jsonpointer~=3.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 49)) (3.0.0)\n","Requirement already satisfied: jsonschema~=4.25.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 50)) (4.25.1)\n","Requirement already satisfied: jsonschema-specifications~=2025.4.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 51)) (2025.4.1)\n","Requirement already satisfied: kiwisolver~=1.4.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 52)) (1.4.9)\n","Requirement already satisfied: lark~=1.2.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 53)) (1.2.2)\n","Requirement already satisfied: markdown-it-py~=4.0.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 54)) (4.0.0)\n","Requirement already satisfied: MarkupSafe~=3.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 55)) (3.0.2)\n","Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 56)) (3.10.0)\n","Requirement already satisfied: mdurl~=0.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 57)) (0.1.2)\n","Requirement already satisfied: mpmath~=1.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 58)) (1.3.0)\n","Requirement already satisfied: multidict~=6.6.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 59)) (6.6.4)\n","Requirement already satisfied: multiprocess~=0.70.16 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 60)) (0.70.16)\n","Requirement already satisfied: nest-asyncio~=1.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 61)) (1.6.0)\n","Requirement already satisfied: networkx~=3.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 62)) (3.5)\n","Requirement already satisfied: nltk~=3.9.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 63)) (3.9.1)\n","Requirement already satisfied: numpy~=2.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 64)) (2.0.2)\n","Requirement already satisfied: openai~=1.99.9 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 65)) (1.99.9)\n","Requirement already satisfied: openpyxl~=3.1.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 66)) (3.1.5)\n","Requirement already satisfied: outlines~=1.1.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 67)) (1.1.1)\n","Requirement already satisfied: outlines_core~=0.1.26 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 68)) (0.1.26)\n","Requirement already satisfied: packaging~=25.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 69)) (25.0)\n","Requirement already satisfied: pandas~=2.2.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 70)) (2.2.2)\n","Requirement already satisfied: pillow~=11.3.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 71)) (11.3.0)\n","Requirement already satisfied: pluggy~=1.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 72)) (1.6.0)\n","Requirement already satisfied: ply~=3.11 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 73)) (3.11)\n","Requirement already satisfied: propcache~=0.3.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 74)) (0.3.2)\n","Requirement already satisfied: pyarrow~=18.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 75)) (18.1.0)\n","Requirement already satisfied: pydantic~=2.11.7 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 76)) (2.11.7)\n","Requirement already satisfied: pydantic_core~=2.33.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 77)) (2.33.2)\n","Requirement already satisfied: Pygments~=2.19.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 78)) (2.19.2)\n","Requirement already satisfied: pyparsing~=3.2.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 79)) (3.2.3)\n","Requirement already satisfied: pytest~=8.4.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 80)) (8.4.1)\n","Requirement already satisfied: python-dateutil~=2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 81)) (2.9.0.post0)\n","Requirement already satisfied: python-louvain~=0.16 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 82)) (0.16)\n","Requirement already satisfied: python-dotenv~=1.1.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 83)) (1.1.1)\n","Requirement already satisfied: pytz~=2025.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 84)) (2025.2)\n","Requirement already satisfied: PyYAML~=6.0.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 85)) (6.0.2)\n","Requirement already satisfied: referencing~=0.36.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 86)) (0.36.2)\n","Requirement already satisfied: regex~=2024.11.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 87)) (2024.11.6)\n","Requirement already satisfied: reportlab>=3.6 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 88)) (4.4.3)\n","Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 89)) (2.32.4)\n","Requirement already satisfied: rich~=13.9.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 90)) (13.9.4)\n","Requirement already satisfied: rouge_score~=0.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 91)) (0.1.2)\n","Requirement already satisfied: rpds-py~=0.27.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 92)) (0.27.0)\n","Requirement already satisfied: safetensors~=0.6.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 93)) (0.6.2)\n","Requirement already satisfied: scikit-learn~=1.6.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 94)) (1.6.1)\n","Requirement already satisfied: scipy~=1.16.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 95)) (1.16.1)\n","Requirement already satisfied: sentence-transformers~=5.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 96)) (5.1.0)\n","Requirement already satisfied: shellingham~=1.5.4 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 97)) (1.5.4)\n","Requirement already satisfied: six~=1.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 98)) (1.17.0)\n","Requirement already satisfied: sniffio~=1.3.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 99)) (1.3.1)\n","Requirement already satisfied: starlette~=0.47.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 100)) (0.47.2)\n","Requirement already satisfied: streamlit>=1.35 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 101)) (1.48.1)\n","Requirement already satisfied: streamlit-extras>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 102)) (0.6.0)\n","Requirement already satisfied: sympy~=1.13.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 103)) (1.13.3)\n","Requirement already satisfied: tenacity~=9.1.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 104)) (9.1.2)\n","Requirement already satisfied: threadpoolctl~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 105)) (3.6.0)\n","Requirement already satisfied: tokenizers~=0.21.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 106)) (0.21.4)\n","Requirement already satisfied: torch~=2.8 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 107)) (2.8.0+cu126)\n","Requirement already satisfied: tqdm~=4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 108)) (4.67.1)\n","Requirement already satisfied: transformers~=4.53.3 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 109)) (4.53.3)\n","Requirement already satisfied: typer~=0.16.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 110)) (0.16.0)\n","Requirement already satisfied: typing-inspection~=0.4.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 111)) (0.4.1)\n","Requirement already satisfied: typing_extensions~=4.14.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 112)) (4.14.1)\n","Requirement already satisfied: tzdata~=2025.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 113)) (2025.2)\n","Requirement already satisfied: urllib3~=2.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 114)) (2.5.0)\n","Requirement already satisfied: xxhash~=3.5.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 115)) (3.5.0)\n","Requirement already satisfied: yarl~=1.20.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements_colab.txt (line 116)) (1.20.1)\n","Requirement already satisfied: dask==2025.5.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (2025.5.0)\n","Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (1.0.0)\n","Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (1.1.1)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (5.9.5)\n","Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (2.4.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (3.1.0)\n","Requirement already satisfied: toolz>=0.11.2 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (0.12.1)\n","Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (6.4.2)\n","Requirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (3.0.0)\n","Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask==2025.5.0->distributed~=2025.5.0->-r requirements_colab.txt (line 19)) (1.4.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub~=0.34.4->-r requirements_colab.txt (line 38)) (1.1.7)\n","Requirement already satisfied: airportsdata in /usr/local/lib/python3.12/dist-packages (from outlines~=1.1.1->-r requirements_colab.txt (line 67)) (20250811)\n","Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (5.5.2)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (5.29.5)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (0.10.2)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (3.1.45)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.35->-r requirements_colab.txt (line 101)) (0.9.1)\n","Requirement already satisfied: entrypoints>=0.4 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.4)\n","Requirement already satisfied: htbuilder>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.9.0)\n","Requirement already satisfied: markdownlit>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.0.7)\n","Requirement already satisfied: plotly>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (5.24.1)\n","Requirement already satisfied: prometheus-client>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.22.1)\n","Requirement already satisfied: st-annotated-text>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (4.0.2)\n","Requirement already satisfied: st-theme>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (1.2.3)\n","Requirement already satisfied: streamlit-avatar>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.1.3)\n","Requirement already satisfied: streamlit-camera-input-live>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.2.0)\n","Requirement already satisfied: streamlit-card>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (1.0.2)\n","Requirement already satisfied: streamlit-embedcode>=0.1.2 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.1.2)\n","Requirement already satisfied: streamlit-faker>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.0.4)\n","Requirement already satisfied: streamlit-image-coordinates<0.2.0,>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.1.9)\n","Requirement already satisfied: streamlit-keyup>=0.1.9 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.3.0)\n","Requirement already satisfied: streamlit-toggle-switch>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (1.0.2)\n","Requirement already satisfied: streamlit-vertical-slider>=2.5.5 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (2.5.5)\n","Requirement already satisfied: validators>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.35.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (75.2.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch~=2.8->-r requirements_colab.txt (line 107)) (3.4.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.35->-r requirements_colab.txt (line 101)) (2.1.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.35->-r requirements_colab.txt (line 101)) (4.0.12)\n","Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (3.8.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (5.4.0)\n","Requirement already satisfied: favicon in /usr/local/lib/python3.12/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.7.0)\n","Requirement already satisfied: pymdown-extensions in /usr/local/lib/python3.12/dist-packages (from markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (10.16.1)\n","Requirement already satisfied: altex>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from streamlit-faker>=0.0.2->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (0.2.0)\n","Requirement already satisfied: faker in /usr/local/lib/python3.12/dist-packages (from streamlit-faker>=0.0.2->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (37.5.3)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.35->-r requirements_colab.txt (line 101)) (5.0.2)\n","Requirement already satisfied: beautifulsoup4>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from favicon->markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (4.13.4)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.7.0->favicon->markdownlit>=0.0.5->streamlit-extras>=0.4.0->-r requirements_colab.txt (line 102)) (2.7)\n"]}]},{"cell_type":"markdown","source":["## on prommpts A B and C"],"metadata":{"id":"Bs8kGlQdLBMr"}},{"cell_type":"markdown","source":["Let's compare using only two lines of the Test set **traffic_accident_reports_collection.xlsx**. We are comparing:\n","-"],"metadata":{"id":"dolyKLEDh3CW"}},{"cell_type":"code","source":["!python app/reportParamGridSearch.py --model_id zBotta/smollm2-accident-reporter-360m  --non-threaded --prompt_method A B C --max_workers 4 --dataset_filename traffic_accident_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True & python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM2-360M-Instruct  --non-threaded --prompt_method A B C --max_workers 4 --dataset_filename traffic_accident_reports_collection.xlsx --start_idx 1 --end_idx 2  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BI_pzidchH_8","outputId":"2d241678-2322-4cd8-935d-f62bd1f25b4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-08-25 15:14:44.748350: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1756134884.805916   13054 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1756134884.824498   13054 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1756134884.859128   13054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1756134884.859173   13054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1756134884.859181   13054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1756134884.859187   13054 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-08-25 15:14:44.866543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2025-08-25 15:14:45.075174: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1756134885.132140   13053 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1756134885.141895   13053 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1756134885.165895   13053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1756134885.165931   13053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1756134885.165940   13053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1756134885.165948   13053 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-08-25 15:14:45.173560: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['zBotta/smollm2-accident-reporter-360m'], 'prompt_method': ['A', 'B', 'C'], 'dataset_filename': 'traffic_accident_reports_collection.xlsx', 'start_idx': [1], 'end_idx': [80], 'temperature': [0.3, 0.7, 1.0, 1.3], 'top_p': [0.3, 0.6, 0.9], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['HuggingFaceTB/SmolLM2-360M-Instruct'], 'prompt_method': ['A', 'B', 'C'], 'dataset_filename': 'traffic_accident_reports_collection.xlsx', 'start_idx': [1], 'end_idx': [2], 'temperature': [0.3, 0.7, 1.0, 1.3], 'top_p': [0.3, 0.6, 0.9], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","08/25/2025 15:15:10 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM2-360M-Instruct\n","08/25/2025 15:15:10 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM2-360M-Instruct\n","08/25/2025 15:15:10 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM2-360M-Instruct\n","Generation parameters: \n","{'temperature': [0.3, 0.7, 1.0, 1.3], 'top_p': [0.3, 0.6, 0.9], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","08/25/2025 15:15:10 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=zBotta/smollm2-accident-reporter-360m\n","08/25/2025 15:15:10 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=zBotta/smollm2-accident-reporter-360m\n","08/25/2025 15:15:10 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=zBotta/smollm2-accident-reporter-360m\n","Generation parameters: \n","{'temperature': [0.3, 0.7, 1.0, 1.3], 'top_p': [0.3, 0.6, 0.9], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","Results file is expected to have 72 rows.\n","******* Starting GRID SEARCH ***********\n","******* Starting NOT THREADED PROCESS ************\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 95.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.95it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Results file is expected to have 2880 rows.\n","******* Starting GRID SEARCH ***********\n","******* Starting NOT THREADED PROCESS ************\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 58.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.97it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.90it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 38.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.65it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 85.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.66it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 42.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.66it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 91.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.57it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.48it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 69.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.99it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.63it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 83.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.16it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:16:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1341 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...e accident is a minivan', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 32.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.79it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:16:21 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1449 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...wing actions were taken', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 58.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.19it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.13it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:16:51 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1288 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...olved in the accident. ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.45it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.31it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 74.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.42it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 83.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.60it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:17:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1252 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...ndro Ruiz – crossover', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.78it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.34it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 40.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.75it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.10it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.12it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:18:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1545 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...port. The vehicle A was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 67.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.90it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 92.10it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 66.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.15it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:18:15 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1409 [type=json_invalid, input_value='{ \"title\": \"Report on Ve... report was filed and a', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 86.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.91it/s]\n","Ref_row:1 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.54it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 37.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.37it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:18:43 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 431 [type=json_invalid, input_value='{\"title\":\"Report\",\"repor...2pm, 5:42pm, 5:42pm, 5:', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.42it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 86.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.53it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 71.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.04it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 81.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.64it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:18:55 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1314 [type=json_invalid, input_value='{ \"title\": \"Report on Ve... The incident was filed', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 43.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.79it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 79.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.67it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 70.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 101.84it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 57.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.45it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 79.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.34it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.93it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.71it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:19:42 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1443 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...nt was determined to be', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 72.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.76it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:19:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1479 [type=json_invalid, input_value='{ \"title\": \"Report of Ve... clear the intersection', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 66.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.16it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.74it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.87it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.48it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.44it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.40it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.41it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.52it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:20:38 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1547 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re... The report is provided', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.64it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 63.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.22it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.99it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.34it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.02it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.15it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.89it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.36it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 90.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.85it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.05it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.66it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.96it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.73it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.53it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:22:15 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1284 [type=json_invalid, input_value='{ \"title\": \"Vehicle coll...tersection. The vehicle', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 30.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.40it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.51it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.13it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.77it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.40it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 75.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.45it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.05it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.63it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:23:16 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 378 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...07:55, 07:55, 07:55, 07', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.35it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.01it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.99it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.86it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:23:48 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 349 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...07:56, 07:56, 07:56, 07', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 89.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.26it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.72it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.73it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:24:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1464 [type=json_invalid, input_value='{ \"title\": \"Report on th...ocal police department.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.95it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:24:42 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 372 [type=json_invalid, input_value='{\"title\":\"Three-vehicle ... 07:55, 07:56, 07:55, 0', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 68.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.18it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.28it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.96it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:25:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1219 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...e report was created in', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/25/2025 15:25:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 378 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re... 07:55, 07:56, 07:55, 0', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.46it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.03it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 81.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.69it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:25:57 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1535 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...escription of Injuries,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 78.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.14it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:26:01 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 870 [type=json_invalid, input_value='{ \"title\": \"Report on th...ctions: 33. Contingency', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 27.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.37it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:26:31 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1412 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...dden braking by Vehicle', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 39.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 97.53it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:26:34 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 349 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...07:56, 07:56, 07:56, 07', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 56.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.38it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 66.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.13it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.90it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:27:03 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1021 [type=json_invalid, input_value='{ \"title\": \"Report on Ev...e B); Mr. Tom Novak –', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 72.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.18it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:27:15 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1478 [type=json_invalid, input_value='{ \"title\": \"Report on th...e incident was reported', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 80.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.87it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.26it/s]\n","Ref_row:2 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.81it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.20it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 82.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.71it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:28:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1487 [type=json_invalid, input_value='{ \"title\": \"Report on th...e incident was reported', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 42.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.33it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.13it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.23it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.46it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:29:01 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1294 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re... concise manner, and it', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.17it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:29:16 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1458 [type=json_invalid, input_value='{ \"title\": \"Report on th...urate and coherent. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 70.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.84it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:29:33 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1504 [type=json_invalid, input_value='{ \"title\": \"Report on th...tted to the authorities', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 57.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.03it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:29:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1401 [type=json_invalid, input_value='{ \"title\": \"Report on th... minor and the severity', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 41.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.12it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.45it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.91it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 69.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.11it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.47it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:30:39 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1215 [type=json_invalid, input_value='{ \"title\": \"Report on th...ntrol, and the accident', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.21it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:30:41 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1442 [type=json_invalid, input_value='{ \"title\": \"Report on th...vided. The title of the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 86.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.17it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.69it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:31:14 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1224 [type=json_invalid, input_value='{ \"title\": \"Report on Th...d in the need for first', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 74.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.56it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.02it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.93it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.04it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.81it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:31:53 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1539 [type=json_invalid, input_value='{ \"title\": \"Report on th... accurate and coherent.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 60.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.86it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.77it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.70it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.39it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 88.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.87it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.00it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 74.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.78it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.94it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.21it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 88.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.63it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.39it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 81.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.92it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.86it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.49it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.40it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.57it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.82it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.52it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 57.52it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.65it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.99it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.32it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.88it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 92.57it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.29it/s]\n","Ref_row:3 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.00it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.73it/s]\n","***** Starting statistical analyisis for the experiment_id=HuggingFaceTB-SmolLM2-360M-Instruct-25-082025_15-15-14 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-HuggingFaceTB-SmolLM2-360M-Instruct-25-082025_15-15-14.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_A-HuggingFaceTB-SmolLM2-360M-Instruct-25-082025_15-15-14.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_B-HuggingFaceTB-SmolLM2-360M-Instruct-25-082025_15-15-14.xlsx\n","08/25/2025 15:35:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1420 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re... to the authorities and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.64it/s]\n","Ref_row:3 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_C-HuggingFaceTB-SmolLM2-360M-Instruct-25-082025_15-15-14.xlsx\n","reportParamGridSearch time --- 20.19454689025879 minutes ---\n","08/25/2025 15:35:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1382 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re... accident was Vehicle A', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 125.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.94it/s]\n","Ref_row:3 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 74.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.40it/s]\n","Ref_row:3 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:35:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1448 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...artment. The report was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 114.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.30it/s]\n","Ref_row:3 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 123.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.19it/s]\n","Ref_row:3 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 87.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.03it/s]\n","Ref_row:3 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:36:13 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1159 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...Rita Zhang, a hatchback', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 123.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.39it/s]\n","Ref_row:3 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:36:31 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1378 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...al police and emergency', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 122.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.38it/s]\n","Ref_row:3 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.95it/s]\n","Ref_row:3 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/25/2025 15:36:51 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1442 [type=json_invalid, input_value='{ \"title\": \"Report on Ve... the roundabout without', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 116.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.02it/s]\n","Ref_row:3 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n"]}]},{"cell_type":"markdown","source":["# On prompt D\n","Prompt D has a traffic accident example specified in it"],"metadata":{"id":"w00b6hpgK93o"}},{"cell_type":"code","source":["!python app/reportParamGridSearch.py --model_id zBotta/smollm2-accident-reporter-360m  --non-threaded --prompt_method D --max_workers 4 --dataset_filename traffic_accident_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True \\\n","& python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM2-360M-Instruct  --non-threaded --prompt_method A --max_workers 4 --dataset_filename traffic_accident_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True \\\n","& python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM2-360M-Instruct  --non-threaded --prompt_method B --max_workers 4 --dataset_filename traffic_accident_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True \\\n","& python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM2-360M-Instruct  --non-threaded --prompt_method C --max_workers 4 --dataset_filename traffic_accident_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True \\\n","& python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM2-360M-Instruct  --non-threaded --prompt_method D --max_workers 4 --dataset_filename traffic_accident_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eMi99IkmK72K","executionInfo":{"status":"ok","timestamp":1756250475356,"user_tz":-120,"elapsed":42061229,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"16199201-d562-447c-bf40-037a0412d23d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 53.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.34it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.11it/s]\n","Ref_row:63 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.87it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:42:32 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1335 [type=json_invalid, input_value='{ \"title\": \"Report on Sn...ent was reported to the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 25.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.93it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.17it/s]\n","Ref_row:63 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:42:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1392 [type=json_invalid, input_value='{ \"title\": \"Report of th...er of people injured in', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 33.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.42it/s]\n","Ref_row:61 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.92it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.43it/s]\n","Ref_row:63 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:43:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 659 [type=json_invalid, input_value='{ \"title\": \"Car chain-re... }  }  }  }  }  }  }  }', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 21.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.45it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.71it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.18it/s]\n","Ref_row:63 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:43:57 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1438 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re... easy to understand and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 20.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.61it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:44:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1285 [type=json_invalid, input_value='{ \"title\": \"Report on th... The vehicle that rear-', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 23.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.69it/s]\n","Ref_row:61 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.26it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.06it/s]\n","Ref_row:61 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.55it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:44:51 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1143 [type=json_invalid, input_value='{ \"title\": \"Car chain-re...ote: Use JSON format in', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 26.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.38it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.59it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.54it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.20it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.90it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.77it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.84it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.67it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 12.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.73it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.55it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:46:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1412 [type=json_invalid, input_value='{ \"title\": \"Report on th...e incident was reported', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 19.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.28it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.10it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:46:41 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1191 [type=json_invalid, input_value='{ \"title\": \"Report on th...//www.police.gov.tw/en/', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 26.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.81it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.27it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.75it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.11it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.63it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.80it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.83it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.18it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.86it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:47:54 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1343 [type=json_invalid, input_value='{ \"title\": \"Report on th... in the recovery of the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 27.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.52it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.12it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.59it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.25it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.11it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.52it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.11it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.95it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.16it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.68it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:49:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1492 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...authorities for further', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 24.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.73it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.16it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.31it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.30it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.92it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.92it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:50:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1057 [type=json_invalid, input_value='{ \"title\": \"Report on th...bruised) and Mr. Ali Ch', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 20.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.01it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.10it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.70it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.82it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.91it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:50:48 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1482 [type=json_invalid, input_value='{ \"title\": \"Report on th...sportation department. ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 30.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.35it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.66it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.50it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.05it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.68it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.46it/s]\n","Ref_row:64 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.61it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.43it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.75it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.25it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 24.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.84it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.00it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.93it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.78it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:52:42 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1626 [type=json_invalid, input_value='{ \"title\": \"Report on th...he incident was handled', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 28.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.72it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.21it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.57it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.20it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.74it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.94it/s]\n","08/26/2025 20:53:20 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1293 [type=json_invalid, input_value='{ \"title\": \"Report on th...ng lane, resulting in a', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.07it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.23it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.93it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.93it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.76it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:54:26 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1430 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...sponse team at (510) 52', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 48.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.55it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.24it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.16it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.26it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.77it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 76.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.88it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:54:59 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1150 [type=json_invalid, input_value='{ \"title\": \"Bridge side-... to be $100,000. Damage', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 49.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.57it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.53it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.48it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.69it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.21it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.10it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:56:01 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1411 [type=json_invalid, input_value='{ \"title\": \"Report on Bi...e local authorities and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 16.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.50it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:56:03 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1348 [type=json_invalid, input_value='{ \"title\": \"Report on He...rt was submitted to the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 18.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.15it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.31it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.22it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.72it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.23it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.02it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.49it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 47.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.39it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.61it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 33.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.99it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 47.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.90it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.65it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.36it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:57:28 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1366 [type=json_invalid, input_value='{\"title\":\"Report\",\"repor...on February 17, 2024 at', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 31.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.72it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.83it/s]\n","Ref_row:62 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.84it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.36it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.60it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.23it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.42it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.12it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.09it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:58:47 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 567 [type=json_invalid, input_value='{ \"title\": \"Parking lot ...\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 29.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.03it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.64it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 20:59:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1450 [type=json_invalid, input_value='{ \"title\": \"Report on Hi...ased from the hospital.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 26.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.83it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.30it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.57it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.83it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 32.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.28it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.16it/s]\n","Ref_row:65 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.53it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.70it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.95it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.20it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.45it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:00:28 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1498 [type=json_invalid, input_value='{ \"title\": \"Report on Hi...e incident was reported', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 20.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.44it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:00:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1406 [type=json_invalid, input_value='{ \"title\": \"Parking lot ...isor, Mr. David Neuman,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 30.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.96it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.16it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.48it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.05it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.61it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.27it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.86it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.25it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.38it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.09it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.16it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 23.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.78it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.15it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.05it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.67it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.41it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:02:54 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1477 [type=json_invalid, input_value='{ \"title\": \"Report on Hi... and failure to yield. ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 48.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.84it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:03:05 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1261 [type=json_invalid, input_value='{ \"title\": \"Parking Lot ... stationary Vehicle A (', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 33.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.96it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.03it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.43it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.85it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.04it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.98it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.05it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.83it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:04:12 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 645 [type=json_invalid, input_value='{ \"title\": \"Report on Hi...\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 27.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.38it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.37it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.88it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.70it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.16it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.61it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.14it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 36.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.88it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.33it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.23it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.72it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.45it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.67it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:06:16 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 967 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...red at 23:45 on October', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 17.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.45it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.73it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 28.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.47it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 31.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.80it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:06:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1085 [type=json_invalid, input_value='{ \"title\": \"Report of Hi...s taken to hospital. 22', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 47.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.58it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.69it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.12it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.22it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.81it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.56it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.75it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:07:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1426 [type=json_invalid, input_value='{ \"title\": \"Bus-barrier ...e and the engineer. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 22.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.96it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.40it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.93it/s]\n","Ref_row:66 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.01it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.34it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:08:26 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1394 [type=json_invalid, input_value='{ \"title\": \"Report on Hi...pened for traffic.  The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 33.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.34it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.32it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:08:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 556 [type=json_invalid, input_value='{ \"title\": \"Report on Ve... \\\\\"\\\\\"\\\\\"\\\\\"  \\\\\"\\\\\"\\\\', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 47.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.69it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.24it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.62it/s]\n","Ref_row:63 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.05it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.07it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.83it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.50it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.30it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.03it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.28it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.57it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:10:20 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1212 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...nded the car. The cause', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 24.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.24it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.52it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:10:42 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 376 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...:65, 14:66, 14:67, 14:6', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 37.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.63it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.65it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.42it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.34it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.38it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.04it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.19it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.22it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.38it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.98it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 13.66it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:12:11 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 376 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...:65, 14:66, 14:67, 14:6', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 26.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.97it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.31it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.83it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.05it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:13:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1164 [type=json_invalid, input_value='{ \"title\": \"Report of Ve... the lanes as necessary', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 40.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.05it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.96it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.90it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.45it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.89it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:13:39 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1294 [type=json_invalid, input_value='{ \"title\": \"Detailed Eve...for personal protection', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 29.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.06it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.44it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.64it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.90it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:14:39 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 376 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...:65, 14:66, 14:67, 14:6', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 23.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.54it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.36it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:14:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1452 [type=json_invalid, input_value='{ \"title\": \"Report on Pe...ailing to stop in time.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 11.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.82it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.27it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.98it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.36it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.44it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.04it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.35it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.80it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.16it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.21it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.13it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:16:11 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 363 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re... 14:68, 14:69, 14:70, 1', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 20.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.73it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.04it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.98it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.99it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.50it/s]\n","Ref_row:67 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.85it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.85it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.89it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.01it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.52it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.84it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.29it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:17:42 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1465 [type=json_invalid, input_value='{ \"title\": \"Report on ev...e department as a cause', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 30.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.73it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.10it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.21it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.29it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.32it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 27.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.58it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.25it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.00it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.43it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.93it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.98it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.17it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.39it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.68it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.94it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.07it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.68it/s]\n","Ref_row:64 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.41it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.53it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.95it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:20:02 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1377 [type=json_invalid, input_value='{ \"title\": \"Report on Pe... secured until the EMTs', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 25.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.69it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.74it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.20it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.44it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.56it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.34it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.50it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.36it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.08it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.59it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:21:21 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1198 [type=json_invalid, input_value='{ \"title\": \"Report on th... and the bridge patrol.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 22.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.06it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.36it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 53.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.73it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.25it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.08it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.85it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.53it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.32it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.11it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.75it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.92it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.08it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.07it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:23:12 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 570 [type=json_invalid, input_value='{ \"title\": \"Unsafe lane ...2 33.11.31.12 33.11.31.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 45.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.44it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.97it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.55it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.90it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.00it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:24:07 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1135 [type=json_invalid, input_value='{ \"title\": \"Report on th...er, and Mr. Lucas Grant', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 45.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.49it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.18it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.57it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 28.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.18it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.08it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.14it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 73.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.58it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.16it/s]\n","Ref_row:68 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.81it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.82it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.40it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.03it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.90it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.22it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.46it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.06it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 57.25it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:26:28 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1017 [type=json_invalid, input_value='{ \"title\": \"Report on th...ved a van and a convert', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/26/2025 21:26:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 640 [type=json_invalid, input_value='{ \"title\": \"Report on ve... \\\\\\\\  \\\\\\\\  \\\\\\\\  \\\\\\\\', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 26.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.32it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.18it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 20.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.82it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.35it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.81it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.60it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.85it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.75it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.68it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.35it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.50it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.95it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.69it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.48it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.91it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.82it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.39it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:28:26 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1402 [type=json_invalid, input_value='{ \"title\": \"Report on Re...authorities for further', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 24.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.05it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.12it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.29it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.28it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.72it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.10it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.45it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.95it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.18it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.16it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.12it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 78.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.47it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.99it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:30:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1264 [type=json_invalid, input_value='{\"title\":\"Report of a co...igate the incident? The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 18.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.83it/s]\n","Ref_row:65 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.24it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.17it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.42it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.34it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 49.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.55it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:31:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1259 [type=json_invalid, input_value='{ \"title\": \"Report on Re...The report is clear and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 30.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.87it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.64it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.07it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.60it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.13it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.49it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:32:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1491 [type=json_invalid, input_value='{ \"title\": \"Report on th...ent was reported to the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 49.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.63it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.71it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.86it/s]\n","Ref_row:69 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:32:33 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1342 [type=json_invalid, input_value='{ \"title\": \"Report on Re...i driver and the bridge', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 36.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.69it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.75it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.97it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.68it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.81it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.54it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 28.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.75it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.36it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches:   0% 0/1 [00:00<?, ?it/s]08/26/2025 21:33:47 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1637 [type=json_invalid, input_value='{ \"title\": \"Report on th... to the local bystander', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 25.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.05it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.08it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.65it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.04it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.24it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.33it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.99it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:34:53 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1438 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...s properly managed. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 18.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 13.79it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.11it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.74it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.17it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.80it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.42it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:36:01 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1528 [type=json_invalid, input_value='{ \"title\": \"Report on th...ncident was closed. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 29.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.76it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:36:07 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1413 [type=json_invalid, input_value='{ \"title\": \"Sideswipe co...ccident was documented.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 30.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.63it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.00it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:36:35 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1567 [type=json_invalid, input_value='{ \"title\": \"Report on Re...and easy-to-read format', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 27.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.04it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.52it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.21it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.67it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.18it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 13.95it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.20it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.52it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:37:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1055 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...n September 12, 2024 at', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 20.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.71it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.86it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.19it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.81it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.57it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.39it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.91it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.34it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.66it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.66it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.17it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:39:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1443 [type=json_invalid, input_value='{ \"title\": \"Report on Mo...e used to contribute to', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 55.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.93it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.70it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.48it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:39:21 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1344 [type=json_invalid, input_value='{ \"title\": \"Report on th...fic signal by Vehicle A', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 35.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.87it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.83it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 23.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.52it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.64it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.20it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.56it/s]\n","Ref_row:70 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.38it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:40:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1522 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...c control. The incident', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 25.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.57it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.18it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.40it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.82it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.44it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.61it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:41:23 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1495 [type=json_invalid, input_value='{ \"title\": \"Accident rep...orted to the police and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 19.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.01it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.97it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 49.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.30it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.04it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.54it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.54it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.23it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.23it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.21it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.53it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.91it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.58it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.61it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.28it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:43:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1490 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...rted to the EMS and the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 25.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.55it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.58it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.45it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.89it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 49.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.94it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.22it/s]\n","Ref_row:66 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.54it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.87it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.34it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.76it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.29it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.35it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.24it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:45:01 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1510 [type=json_invalid, input_value='{ \"title\": \"Contingency ...t a surveillance of all', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 27.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.82it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.41it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 33.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.24it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 48.79it/s]\n","Batches:   0% 0/1 [00:00<?, ?it/s]08/26/2025 21:45:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1331 [type=json_invalid, input_value='{ \"title\": \"Report on th...e that was involved was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 41.56it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.98it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.31it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.22it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.53it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.25it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.25it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.61it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 53.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.45it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.03it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 49.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.67it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.54it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.34it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.70it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.86it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 26.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.72it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 29.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.58it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.06it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:47:19 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 609 [type=json_invalid, input_value='{ \"title\": \"Sideswipe co... 2024-02-03 22:08 0 0 2', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 13.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.45it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.54it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:47:34 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1331 [type=json_invalid, input_value='{ \"title\": \"Report on th...e that was involved was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 19.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.96it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.23it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.22it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.98it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.96it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.29it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.59it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 17.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.88it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.85it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.93it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.65it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.94it/s]\n","Ref_row:71 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.44it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.01it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:49:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1193 [type=json_invalid, input_value='{ \"title\": \"Report on th...lved was SUV driver Ms.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 23.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.85it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.86it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:49:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1100 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...ide Boulevard, Hillside', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 53.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.42it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.04it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.02it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.50it/s]\n","Ref_row:67 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.44it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.73it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.32it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 25.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.07it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.43it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:51:14 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1221 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...ast-moving traffic. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 37.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.29it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:51:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1578 [type=json_invalid, input_value='{ \"title\": \"Vehicle-pede...tal. The pedestrian was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 48.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.48it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.39it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.90it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.69it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.99it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.31it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.90it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:52:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 382 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...11:41, 11:41, 11:41, 11', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 19.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.48it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.09it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.05it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.62it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.70it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.19it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.91it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.78it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.16it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.73it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.16it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.74it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.83it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.92it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.96it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.82it/s]\n","***** Starting statistical analyisis for the experiment_id=HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_572753 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_572753.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_C-HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_572753.xlsx\n","reportParamGridSearch time --- 612.5818611343701 minutes ---\n","Batches: 100% 1/1 [00:00<00:00, 50.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.78it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.82it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.13it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.73it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.98it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.26it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 72.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.88it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.07it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.32it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.85it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.35it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.35it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.11it/s]\n","Ref_row:72 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.03it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:56:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 979 [type=json_invalid, input_value='{ \"title\": \"Report on Ta...police on site. 28. Tax', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 74.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.98it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.06it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 44.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.55it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.25it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.07it/s]\n","Ref_row:68 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:56:39 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1439 [type=json_invalid, input_value='{ \"title\": \"Report on th...ation. The incident was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 40.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.00it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:56:46 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1204 [type=json_invalid, input_value='{ \"title\": \"Collision be...aveling at 60 km/h when', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 47.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.63it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.86it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 28.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.28it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.63it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:57:38 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1498 [type=json_invalid, input_value='{ \"title\": \"Vehicle-pede...tal. The pedestrian was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 26.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.68it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.75it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:57:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1290 [type=json_invalid, input_value='{ \"title\": \"Report on a ...ed were notified of the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 35.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.25it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.21it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.14it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.06it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:58:53 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1590 [type=json_invalid, input_value='{ \"title\": \"Vehicle-pede...ated and transported to', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 19.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.67it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 21:59:03 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1580 [type=json_invalid, input_value='{ \"title\": \"Report on th... reported to the county', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 32.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.33it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.82it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.27it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.99it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.17it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.31it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.43it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.64it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.76it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:00:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 370 [type=json_invalid, input_value='{\"title\":\"Car-pedestrian...00000000000000000000000', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 27.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.96it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:00:31 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1386 [type=json_invalid, input_value='{ \"title\": \"Report on th...itions, possible driver', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 29.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.35it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.54it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.63it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.31it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.00it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.74it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.90it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.27it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.78it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.25it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.85it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:02:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1531 [type=json_invalid, input_value='{ \"title\": \"Report on Hi...uthorities and the road', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 53.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.70it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:02:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1689 [type=json_invalid, input_value='{ \"title\": \"Report of Ca...ntion and police filing', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 23.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.13it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.64it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.31it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.96it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.00it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.42it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:03:34 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1588 [type=json_invalid, input_value='{ \"title\": \"Report on Hi...tance. The incident was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 60.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.26it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.35it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.15it/s]\n","Ref_row:73 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.29it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:04:13 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1474 [type=json_invalid, input_value='{ \"title\": \"Report on Ca...d to the police and the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 35.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.15it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.33it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.06it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.32it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:04:47 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1532 [type=json_invalid, input_value='{ \"title\": \"Report on Hi...ital for treatment. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 39.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.21it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.57it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.69it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.90it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.35it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.07it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.66it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:05:58 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1483 [type=json_invalid, input_value='{ \"title\": \"Report on Hi... by a medical team. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 31.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.32it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:06:13 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1376 [type=json_invalid, input_value='{ \"title\": \"Report of Ca...he pedestrian was taken', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 39.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.71it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.67it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.18it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.60it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.19it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.10it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.73it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:07:23 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1474 [type=json_invalid, input_value='{ \"title\": \"Report on a ...Car-pedestrian crossing', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 72.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.28it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.62it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.64it/s]\n","Ref_row:69 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.11it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 49.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.76it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.38it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.22it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.04it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 48.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.56it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches:   0% 0/1 [00:00<?, ?it/s]Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 23.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.58it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.82it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.89it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.20it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.54it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 40.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.39it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.00it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.66it/s]\n","Ref_row:74 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:09:08 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 743 [type=json_invalid, input_value='{\"title\":\"High-speed ped...ponse: 46  Response: 47', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 46.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.87it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.73it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.86it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.64it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.92it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:10:01 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1165 [type=json_invalid, input_value='{ \"title\": \"Report on Re...led to brake in time at', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 41.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.72it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.29it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.66it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.48it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.01it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.48it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.47it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.75it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.70it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.47it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 76.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.82it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.71it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.05it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.44it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.97it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.39it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.48it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.22it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:12:21 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1186 [type=json_invalid, input_value='{ \"title\": \"Report on th...e incident was reported', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 22.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.87it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.61it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.81it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.40it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.45it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.53it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:12:58 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1217 [type=json_invalid, input_value='{ \"title\": \"Report on th...volved Vehicle A, which', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 43.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.63it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 50.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.53it/s]\n","Ref_row:70 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.50it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.37it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.26it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.85it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.85it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.07it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.50it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.03it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:14:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1480 [type=json_invalid, input_value='{ \"title\": \"Report on Si...ce and the incident was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 20.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.08it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.48it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.75it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.96it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.22it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.53it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.38it/s]\n","Ref_row:75 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.63it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.29it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.14it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.21it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:15:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1324 [type=json_invalid, input_value='{ \"title\": \"Report on th...was reported as a chain', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 50.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 57.46it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.60it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.60it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.61it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.92it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.04it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.92it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:16:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1326 [type=json_invalid, input_value='{ \"title\": \"Report on Si... as follows:  Sideswipe', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 44.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.26it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.50it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.37it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.54it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 53.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.23it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.78it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.91it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.28it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 49.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.24it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 53.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.67it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.66it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.58it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.90it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.98it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.46it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 53.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.76it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 24.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.14it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.62it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.19it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.83it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.72it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.60it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:19:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1416 [type=json_invalid, input_value='{ \"title\": \"Report on th...e traffic was held. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 42.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.84it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:19:58 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1168 [type=json_invalid, input_value='{ \"title\": \"Car-bicycle ...s taken to the hospital', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 38.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.32it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.06it/s]\n","Ref_row:71 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.81it/s]\n","Ref_row:76 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 43.28it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.17it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.97it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:20:57 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 345 [type=json_invalid, input_value='{\"title\":\"Report\",\"repor... 18:27, 18:27, 18:27, 1', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 38.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.17it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.97it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.91it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.91it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 51.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.72it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.14it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.27it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 22.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.20it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.78it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.78it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.38it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.02it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.73it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:22:41 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1103 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...5th Avenue and Pine Bou', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 27.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.01it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.95it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.73it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.88it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:23:16 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 485 [type=json_invalid, input_value='{ \"title\": \"Report on a ... \\\\\"\\\\\"  \\\\\"\\\\\"  \\\\\"\\\\\"', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 49.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.16it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.26it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.33it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.99it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 49.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.31it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.81it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.54it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:24:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1391 [type=json_invalid, input_value='{ \"title\": \"Report on th... The report is accurate', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 42.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.27it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.12it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.41it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.67it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.42it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.51it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.28it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.87it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.57it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.00it/s]\n","Ref_row:72 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.80it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.25it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.18it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.64it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:26:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1452 [type=json_invalid, input_value='{ \"title\": \"Report on th...d in the questions. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 54.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.10it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 52.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.79it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.40it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 21.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.63it/s]\n","Ref_row:77 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 57.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.71it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.42it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.89it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:27:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1259 [type=json_invalid, input_value='{ \"title\": \"Report on th...fic signals, lanes, and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 20.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.47it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.37it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:27:35 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1540 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...d of the incident.  The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 38.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.81it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.80it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.59it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.25it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.62it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 30.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.05it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 29.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.05it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:28:48 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1310 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re... driven by Mr. Julian L', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 32.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.92it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:28:52 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1477 [type=json_invalid, input_value='{ \"title\": \"Report on th...the scene. The incident', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 35.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.33it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 49.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.79it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.89it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.19it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:29:54 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1402 [type=json_invalid, input_value='{ \"title\": \"Accident on ...rted to the authorities', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 22.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.92it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:30:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1570 [type=json_invalid, input_value='{ \"title\": \"Report on Ve... aid department and the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 21.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.82it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.60it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.63it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:30:34 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 403 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re.... 2024. 2024. 2024. 202', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 36.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.90it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.87it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.69it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 42.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.15it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:31:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1481 [type=json_invalid, input_value='{\"title\":\"Maple Drive ne...s reported to the local', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 49.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.92it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.23it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.31it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 32.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.56it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:32:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1436 [type=json_invalid, input_value='{ \"title\": \"Accident on ...ent was reported to the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 26.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.41it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.66it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:32:28 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1605 [type=json_invalid, input_value='{ \"title\": \"Report on th...e incident was reported', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 35.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.80it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:32:42 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1492 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...ed him to the hospital.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 22.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 22.27it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.53it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.44it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.58it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.59it/s]\n","Ref_row:73 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.53it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.20it/s]\n","Ref_row:78 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:33:41 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1412 [type=json_invalid, input_value='{ \"title\": \"Report on mo...cal police and EMS. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 19.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.82it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 20.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.01it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.88it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.46it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:34:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1419 [type=json_invalid, input_value='{ \"title\":\"Reuters - A p...here had been one known', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 27.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.33it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.25it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 40.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.17it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 60.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.29it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 43.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.44it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:35:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1475 [type=json_invalid, input_value='{ \"title\": \"Accident on ... investigated by police', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 28.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.80it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:35:05 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1491 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...ell-written and follows', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.12it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 25.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.10it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.79it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.59it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 23.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.82it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.36it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:36:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1399 [type=json_invalid, input_value='{ \"title\": \"Report on th...or stakeholders and law', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/26/2025 22:36:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 703 [type=json_invalid, input_value='{ \"title\": \"Accident on ...}  }  }  }  }  }  }  } ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 49.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.36it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.62it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.82it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.94it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:36:51 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1291 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...ollision was a delivery', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 25.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.66it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.42it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.12it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:37:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1221 [type=json_invalid, input_value='{ \"title\": \"Loss-of-cont...he police and EMTs. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 39.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.89it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 58.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.68it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:37:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1352 [type=json_invalid, input_value='{ \"title\": \"Report on th...t was a resident of the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 28.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.89it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.02it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.94it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:38:10 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1139 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...Mall security notified.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 36.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.76it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.64it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.19it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.64it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:39:12 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1424 [type=json_invalid, input_value='{ \"title\": \"Loss-of-cont...ffice. The accident was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 48.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.41it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.38it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:39:20 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1161 [type=json_invalid, input_value='{\"title\":\"Report\",\"repor...into the SUV (Vehicle A', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 41.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.07it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.53it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.37it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 27.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 23.41it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.36it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:40:03 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1618 [type=json_invalid, input_value='{ \"title\": \"Report on Bu...gency. The incident was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 27.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.90it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.16it/s]\n","Ref_row:74 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.35it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.04it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 19.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.93it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 35.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.69it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.32it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.25it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.35it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 48.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.27it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.48it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 26.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.86it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 28.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 26.77it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.94it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.34it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 29.99it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.32it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.64it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 18.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 20.80it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 32.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.89it/s]\n","***** Starting statistical analyisis for the experiment_id=zBotta-smollm2-accident-reporter-360m-26-082025_11-42-13_688175 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-zBotta-smollm2-accident-reporter-360m-26-082025_11-42-13_688175.xlsx\n","Batches: 100% 1/1 [00:00<00:00, 56.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.27it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_D-zBotta-smollm2-accident-reporter-360m-26-082025_11-42-13_688175.xlsx\n","reportParamGridSearch time --- 660.4647814830145 minutes ---\n","Batches: 100% 1/1 [00:00<00:00, 67.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.80it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:42:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1458 [type=json_invalid, input_value='{ \"title\": \"Report on St... department for further', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 64.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.12it/s]\n","Ref_row:79 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.34it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 49.41it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.15it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 27.04it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.72it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.66it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 31.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 37.58it/s]\n","Ref_row:75 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 40.56it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.69it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 28.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.07it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.71it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.35it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.78it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 33.05it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 49.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.25it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.67it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 39.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.01it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 36.19it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 35.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 32.88it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.65it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:45:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1316 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...s were not taken if the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 25.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.33it/s]\n","Batches:   0% 0/1 [00:00<?, ?it/s]Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.12it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.54it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.19it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 33.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.69it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:45:54 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1423 [type=json_invalid, input_value='{ \"title\": \"Report on Ta...plete and comprehensive', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 49.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.16it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:46:07 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1488 [type=json_invalid, input_value='{ \"title\": \"Report on Un...ed to the authorities. ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 42.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.48it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 52.56it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.72it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 29.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.49it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 24.74it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 75.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.62it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.59it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 36.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 39.40it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:47:08 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1490 [type=json_invalid, input_value='{ \"title\": \"Report on Ta... result of insufficient', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 36.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.77it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 85.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 25.15it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.22it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 46.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.65it/s]\n","Ref_row:80 & prompt_method=D: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.62it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 64.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.60it/s]\n","***** Starting statistical analyisis for the experiment_id=HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_605443 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_605443.xlsx\n","Batches: 100% 1/1 [00:00<00:00, 38.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.27it/s]\n","Ref_row:76 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_D-HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_605443.xlsx\n","reportParamGridSearch time --- 666.4654786785444 minutes ---\n","08/26/2025 22:48:07 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 383 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...11:41, 11:41, 11:41, 11', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 89.52it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:48:38 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1215 [type=json_invalid, input_value='{\"title\":\"Report to Agen...y / incidents], and the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 79.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.72it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 65.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.94it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:49:12 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1344 [type=json_invalid, input_value='{ \"title\": \"Report on th...Vehicle A failed to see', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 98.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.73it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:49:13 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1433 [type=json_invalid, input_value='{ \"title\": \"Report on Ta...h the title and content', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 74.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 101.23it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 86.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.30it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:49:46 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1335 [type=json_invalid, input_value='{ \"title\": \"Report on th...ent was a result of the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 91.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.39it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 38.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.88it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 41.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 44.53it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 32.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 35.85it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.10it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.83it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 54.64it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:50:38 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1168 [type=json_invalid, input_value='{ \"title\": \"Report on th...mbulance and the police', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 59.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.62it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:51:07 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1494 [type=json_invalid, input_value='{ \"title\": \"Report on th...who issued a warning to', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 70.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.49it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:51:12 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1296 [type=json_invalid, input_value='{ \"title\": \"Report on th...The title of the report', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 74.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.82it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 59.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 38.16it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 82.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.59it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 34.29it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:51:41 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1513 [type=json_invalid, input_value='{ \"title\": \"Report on Ca...red by the local media.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.82it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 73.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.65it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:52:16 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1524 [type=json_invalid, input_value='{ \"title\": \"Report on th...ported to the media and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 70.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.98it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:52:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1637 [type=json_invalid, input_value='{ \"title\": \"Report on Ca...sequent assistance. Car', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 75.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 99.05it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.47it/s]\n","Ref_row:77 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:52:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1583 [type=json_invalid, input_value='{ \"title\": \"Report on th...lice department and the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.35it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:53:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1319 [type=json_invalid, input_value='{\"title\":\"Car-bicycle co...t. It was reported that', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 72.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.50it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.26it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.64it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:53:46 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1500 [type=json_invalid, input_value='{ \"title\": \"Report on th...a report and marked the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 80.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 100.33it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:53:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1206 [type=json_invalid, input_value='{ \"title\": \"Report on th...ehicle that hit Vehicle', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 88.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.65it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 82.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.83it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 90.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.96it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 66.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.31it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 74.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.62it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.70it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.13it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 69.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.40it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:55:02 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1349 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...nt. The severity of the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 58.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 57.34it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 34.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.42it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:55:26 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1528 [type=json_invalid, input_value='{ \"title\": \"Report on th...ed and submitted to the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 53.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.57it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 55.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.24it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.17it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.94it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.20it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 45.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.04it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:56:19 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1477 [type=json_invalid, input_value='{ \"title\": \"Report on th...vestigation. The report', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 74.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.34it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 82.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.06it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:56:26 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 877 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re..., Vehicle B, Vehicle C,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 88.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.37it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 48.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.28it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 62.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.67it/s]\n","Ref_row:78 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.85it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 36.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 45.04it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.06it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 32.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 31.58it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.82it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.50it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.74it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.50it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 68.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.86it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 44.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 48.13it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.85it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.06it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.15it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:58:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1486 [type=json_invalid, input_value='{ \"title\": \"Report on Si...d to the police and the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 90.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.60it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:58:35 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1124 [type=json_invalid, input_value='{ \"title\": \"Report on Lo...d reported the incident', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 53.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.91it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:58:46 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1219 [type=json_invalid, input_value='{ \"title\": \"Report on Si...B, which was parked sed', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 75.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.88it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:59:11 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1170 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...accident was the car of', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 84.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.22it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.40it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:59:19 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1433 [type=json_invalid, input_value='{ \"title\": \"Report on Si...urate and coherent. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 93.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.79it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:59:48 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1389 [type=json_invalid, input_value='{ \"title\": \"Report on Lo...----  -----------------', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 61.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.88it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 22:59:54 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1536 [type=json_invalid, input_value='{ \"title\": \"Report on Si...herent. The contingency', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 82.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.86it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.60it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.99it/s]\n","Ref_row:79 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.67it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.48it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.49it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 75.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.80it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 82.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.41it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 70.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.51it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:00:57 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1161 [type=json_invalid, input_value='{ \"title\": \"Report on th... as follows: The report', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.56it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 47.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 50.01it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:01:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1073 [type=json_invalid, input_value='{\"title\": \"Report of Sid...No. 55, and the who was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.28it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.69it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 67.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.87it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:01:56 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1107 [type=json_invalid, input_value='{ \"title\": \"Report on th...nvolved in the accident', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.30it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 71.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.11it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 56.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 62.27it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:02:31 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1295 [type=json_invalid, input_value='{ \"title\": \"Report on th...by using a mobile phone', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 62.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.17it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:02:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1481 [type=json_invalid, input_value='{ \"title\": \"Report on th...ent was not reported to', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 53.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.30it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:02:59 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 643 [type=json_invalid, input_value='{ \"title\": \"Report on th...\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\\"\\\\', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 79.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 56.46it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 92.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.41it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 37.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 46.44it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.12it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:03:36 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1082 [type=json_invalid, input_value='{ \"title\": \"Report on th...r was assessed on site.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 65.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.66it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 63.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.76it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.54it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 73.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.40it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.59it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 68.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.76it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:04:49 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1309 [type=json_invalid, input_value='{ \"title\": \"Report on th...ngency actions taken to', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 77.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 89.42it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:04:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1413 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...he police. The accident', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.77it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 61.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.21it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 70.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.16it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:05:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1301 [type=json_invalid, input_value='{ \"title\": \"Report on Re...ion and traffic delays.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 69.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.07it/s]\n","Ref_row:80 & prompt_method=A: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 72.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.45it/s]\n","***** Starting statistical analyisis for the experiment_id=HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_475643 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_475643.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_A-HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_475643.xlsx\n","reportParamGridSearch time --- 683.9390078862508 minutes ---\n","08/26/2025 23:05:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1515 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...he incident was treated', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 113.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.59it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:06:03 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1428 [type=json_invalid, input_value='{ \"title\": \"Report on Ve... not see the pedestrian', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 98.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.26it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:06:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1619 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...o vehicles. The vehicle', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 123.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.78it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:06:41 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1538 [type=json_invalid, input_value='{ \"title\": \"Report on Ve....  The EMTs transported', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 63.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.54it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.56it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.03it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:07:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1447 [type=json_invalid, input_value='{ \"title\": \"Report on Ve...ident was resolved with', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 50.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.46it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.24it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.30it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.77it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.24it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.72it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.70it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.79it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:08:04 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 953 [type=json_invalid, input_value='{\"title\":\"Report\",\"repor...g three vehicles at 16:', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 71.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.16it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.07it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.39it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 90.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.22it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.86it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:08:47 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1290 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re... low speed at low speed', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 120.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.48it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.56it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.25it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:09:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1221 [type=json_invalid, input_value='{\"title\":\"Three-vehicle ...ate of emergency. There', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 98.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.50it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.93it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 120.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.67it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 90.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 61.32it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 128.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.38it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 127.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.49it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:09:46 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1232 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...what, when, where, why,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 124.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 99.25it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:10:05 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1284 [type=json_invalid, input_value='{ \"title\": \"Report on th...w, why, and contingency', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 91.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 103.44it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:10:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1384 [type=json_invalid, input_value='{ \"title\": \"Report on St...urrounding the accident', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 122.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.14it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 74.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.88it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.49it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 94.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.65it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.83it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.81it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.45it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.11it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:11:23 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1455 [type=json_invalid, input_value='{\"title\":\"Safe Lane Chan...t is clear and concise,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 125.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.95it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:11:43 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1533 [type=json_invalid, input_value='{ \"title\": \"Report on Un...e incident was reported', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 121.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.21it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.89it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.70it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:12:08 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1433 [type=json_invalid, input_value='{ \"title\": \"Report on Un...hat caused the accident', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 111.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.23it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.81it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.44it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.60it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 84.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.72it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 91.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.91it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.25it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:13:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1335 [type=json_invalid, input_value='{ \"title\": \"Report on th...eport is well-organized', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 79.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.16it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.11it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.39it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 69.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.84it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.01it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.83it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:14:15 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1358 [type=json_invalid, input_value='{ \"title\": \"Report on Ca...n ambulance and filed a', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 129.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.42it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.56it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.06it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.40it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.94it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.74it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:14:59 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1361 [type=json_invalid, input_value='{ \"title\": \"Report on th...o the police department', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 90.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 99.31it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:15:20 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 968 [type=json_invalid, input_value='{ \"title\": \"Report on th...k, which hit Vehicle B,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 123.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.95it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:15:39 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1432 [type=json_invalid, input_value='{ \"title\": \"Report on th...e involved in a serious', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 122.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.25it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:15:59 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1244 [type=json_invalid, input_value='{ \"title\": \"Report on th...juries sustained by the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 122.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.21it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 91.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.40it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:16:32 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1364 [type=json_invalid, input_value='{ \"title\": \"Report on th... easy for the reader to', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 93.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 101.84it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.20it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.60it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:17:10 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1307 [type=json_invalid, input_value='{ \"title\": \"Report on Th...nformation provided was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 114.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.53it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 75.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.54it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.20it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.38it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.47it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.50it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 54.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.31it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:18:12 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1468 [type=json_invalid, input_value='{ \"title\": \"Report on th...public in the aftermath', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 120.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.57it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.90it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:18:38 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1423 [type=json_invalid, input_value='{ \"title\": \"Report on Lo...scription of the event.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 120.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.32it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.98it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 91.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.56it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.57it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.04it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.00it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:19:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1350 [type=json_invalid, input_value='{ \"title\": \"Report on th...ities. The incident was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 116.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 107.32it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:19:36 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1261 [type=json_invalid, input_value='{ \"title\": \"Report on th...s accurate and coherent', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 120.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.44it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.57it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:19:59 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1302 [type=json_invalid, input_value='{ \"title\": \"Report on th...on of the incident. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 111.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.52it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 120.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.67it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.52it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 119.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.67it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/26/2025 23:20:37 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 392 [type=json_invalid, input_value='{ \"title\": \"Report on th..., 08:39, 08:40, 08:41, ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 124.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.20it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.41it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.57it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 91.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.90it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.18it/s]\n","***** Starting statistical analyisis for the experiment_id=HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_370182 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_370182.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_B-HuggingFaceTB-SmolLM2-360M-Instruct-26-082025_11-42-14_370182.xlsx\n","reportParamGridSearch time --- 699.6013542691867 minutes ---\n"]}]},{"cell_type":"code","source":["# KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","\n","from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"lfEA1XoazFKA"},"execution_count":null,"outputs":[]}]}
{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/ml_tricks/colab/colab_connect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"ubgiAABGmadO"},"source":["# This Colab notebook is ment to be executed from a GPU resources in Colab\n","The idea is to remotely execute the repo files (classes, main, etc)"]},{"cell_type":"markdown","metadata":{"id":"Kc3SGLPSX_7E"},"source":["# 0: Before starting, verifiy that: **After pulling the repo, you have copied the .env file into the Github repo !**"]},{"cell_type":"markdown","metadata":{"id":"r0sG1QyylCkW"},"source":["# 1 Mount Google Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":104534,"status":"ok","timestamp":1755367335698,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"w69hWB_qkwLI","outputId":"b3274a13-6630-4843-daff-0fc0ea303216"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Who is using this notebook? Samd\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","notebook_user = input('Who is using this notebook? ') # Samd or Matias"]},{"cell_type":"markdown","metadata":{"id":"FPK232yxlHD1"},"source":["# 2 config Git"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":213,"status":"ok","timestamp":1755367561330,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"GradLm7NkxKN"},"outputs":[],"source":["if notebook_user == \"Matias\":\n","    !git config --global user.name \"zbotta\"\n","    !git config --global user.email \"zbotta@proton.me\"\n","elif notebook_user == \"Samd\":\n","    !git config --global user.name \"SamdGuizani\"\n","    !git config --global user.email \"samd.guizani@gmail.com\"\n","else:\n","    print(\"WARNING - Notebookuser unknown.\")"]},{"cell_type":"markdown","metadata":{"id":"tQRJVtrHynSw"},"source":["## RUN THIS CELL ONLY ONCE!\n","To clone the Github repo"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":381,"status":"ok","timestamp":1755367679268,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"E1otG_64l1Qu"},"outputs":[],"source":["from google.colab import userdata\n","\n","if notebook_user == \"Matias\":\n","    github_token = userdata.get('zbotta_token')\n","elif notebook_user == \"Samd\":\n","    github_token = userdata.get('GitHub_Samd_ReportAgent_GoogleColab')\n","else:\n","    print(\"WARNING - Notebookuser unknown. No GitHub token loaded\")\n","\n","token = github_token # why is this needed? token is a variable not used later.\n","username = \"zbotta\"\n","repo = 'reportingAgent'\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":579,"status":"ok","timestamp":1755367697689,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"PQgo_qSExuXm","outputId":"b41687f4-6c70-4a46-e770-e72d2a0b8fee"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path '/content/drive/MyDrive/GitHub/reportingAgent' already exists and is not an empty directory.\n"]}],"source":["!git clone https://{username}:{github_token}@github.com/{username}/{repo}.git /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1755367699036,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"ze8qK-7nmx_O","outputId":"ae99ecaf-9b2f-4fff-e493-24541771bb33"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVgHJTXrHLIX"},"outputs":[],"source":["#!git remote set-url origin https://{username}:{github_token}@github.com/{username}/{repo}.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Rwp4A70m_2U"},"outputs":[],"source":["#!git remote get-url origin"]},{"cell_type":"markdown","metadata":{"id":"q0fNz_r7m4UG"},"source":["# Use git commands"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16679,"status":"ok","timestamp":1755367723708,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"ybZIZbg2-qhU","outputId":"76ff8ba6-39dc-47c6-ce19-11e7033d73ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["M\tPoC/reportAgent-remote.ipynb\n","Already on 'dev'\n","Your branch is up to date with 'origin/dev'.\n"]}],"source":["!git fetch\n","!git checkout dev"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":853,"status":"ok","timestamp":1755367728789,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"g9D7zP_oRsaN","outputId":"9778959a-8f71-4acc-d433-b41868523877"},"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}],"source":["!git pull"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":608,"status":"ok","timestamp":1755367731425,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"ooqu1-hlm7Af","outputId":"f1a8b1dd-d8af-44b4-fd48-4f6db93c0982"},"outputs":[{"output_type":"stream","name":"stdout","text":["On branch dev\n","Your branch is up to date with 'origin/dev'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   PoC/reportAgent-remote.ipynb\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mPoC/Experiments/\u001b[m\n","\t\u001b[31mPoC/python_env_setup.sh\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}],"source":["!git status"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":416,"status":"ok","timestamp":1755367747776,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"BsN8-zMZ8Z-n","outputId":"1c4802e4-c50f-4ada-b7a6-3232aafe16ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Everything up-to-date\n"]}],"source":["!git push origin dev\n"]},{"cell_type":"markdown","metadata":{"id":"nXz98pxKo8oH"},"source":["# Using project scripts\n","\n","Reference : [Importing python library from Drive](https://colab.research.google.com/drive/12qC2abKAIAlUM_jNAokGlooKY-idbSxi#scrollTo=prUMpfLaB-D7)"]},{"cell_type":"markdown","metadata":{"id":"mvLEUb7VzVD0"},"source":["## Install project dependencies\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1755190554203,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"S6sPDJjVSHQf","outputId":"544b1fa0-6eb3-4a72-f58f-9b887c0b0d3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"-lZSS9qFzSLk","outputId":"4a513ae4-a302-42c9-a371-f785c8f931fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: aiohappyeyeballs==2.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.1)\n","Collecting aiohttp==3.12.14 (from -r requirements.txt (line 2))\n","  Downloading aiohttp-3.12.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n","Requirement already satisfied: aiosignal==1.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.4.0)\n","Collecting airportsdata==20250706 (from -r requirements.txt (line 4))\n","  Downloading airportsdata-20250706-py3-none-any.whl.metadata (9.1 kB)\n","Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.7.0)\n","Collecting anyio==4.9.0 (from -r requirements.txt (line 6))\n","  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: attrs==25.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (25.3.0)\n","Collecting bert-score==0.3.13 (from -r requirements.txt (line 8))\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Collecting certifi==2025.7.14 (from -r requirements.txt (line 9))\n","  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n","Collecting charset-normalizer==3.4.2 (from -r requirements.txt (line 10))\n","  Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n","Requirement already satisfied: click==8.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (8.2.1)\n","Requirement already satisfied: cloudpickle==3.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (3.1.1)\n","Collecting colorama==0.4.6 (from -r requirements.txt (line 13))\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Collecting contourpy==1.3.2 (from -r requirements.txt (line 14))\n","  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n","Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.12.1)\n","Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (4.0.0)\n","Requirement already satisfied: dill==0.3.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (0.3.8)\n","Collecting diskcache==5.6.3 (from -r requirements.txt (line 18))\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (1.9.0)\n","Requirement already satisfied: docstring_parser==0.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (0.17.0)\n","Collecting dotenv==0.9.9 (from -r requirements.txt (line 21))\n","  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n","Requirement already satisfied: et_xmlfile==2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (2.0.0)\n","Collecting evaluate==0.4.5 (from -r requirements.txt (line 23))\n","  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: fastapi==0.116.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (0.116.1)\n","Requirement already satisfied: filelock==3.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (3.18.0)\n","Requirement already satisfied: fonttools==4.59.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (4.59.0)\n","Requirement already satisfied: frozenlist==1.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (1.7.0)\n","Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (2025.3.0)\n","Collecting genson==1.3.0 (from -r requirements.txt (line 29))\n","  Downloading genson-1.3.0-py3-none-any.whl.metadata (28 kB)\n","Collecting groq==0.26.0 (from -r requirements.txt (line 30))\n","  Downloading groq-0.26.0-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: h11==0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (0.16.0)\n","Requirement already satisfied: httpcore==1.0.9 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (1.0.9)\n","Requirement already satisfied: httpx==0.28.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (0.28.1)\n","Collecting huggingface-hub==0.33.4 (from -r requirements.txt (line 34))\n","  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (3.10)\n","Requirement already satisfied: iniconfig==2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 36)) (2.1.0)\n","Collecting instructor==1.10.0 (from -r requirements.txt (line 37))\n","  Downloading instructor-1.10.0-py3-none-any.whl.metadata (11 kB)\n","Collecting interegular==0.3.3 (from -r requirements.txt (line 38))\n","  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n","Collecting iso3166==2.1.1 (from -r requirements.txt (line 39))\n","  Downloading iso3166-2.1.1-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: Jinja2==3.1.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (3.1.6)\n","Requirement already satisfied: jiter==0.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (0.10.0)\n","Requirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (1.5.1)\n","Collecting jsonpath-ng==1.7.0 (from -r requirements.txt (line 43))\n","  Downloading jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: jsonschema==4.25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 44)) (4.25.0)\n","Requirement already satisfied: jsonschema-specifications==2025.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 45)) (2025.4.1)\n","Collecting kiwisolver==1.4.8 (from -r requirements.txt (line 46))\n","  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n","Collecting lark==1.2.2 (from -r requirements.txt (line 47))\n","  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n","Collecting markdown-it-py==3.0.0 (from -r requirements.txt (line 48))\n","  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 49)) (3.0.2)\n","Collecting matplotlib==3.10.3 (from -r requirements.txt (line 50))\n","  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 51)) (0.1.2)\n","Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 52)) (1.3.0)\n","Collecting multidict==6.6.3 (from -r requirements.txt (line 53))\n","  Downloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n","Requirement already satisfied: multiprocess==0.70.16 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 54)) (0.70.16)\n","Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 55)) (1.6.0)\n","Requirement already satisfied: networkx==3.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 56)) (3.5)\n","Collecting numpy==2.3.1 (from -r requirements.txt (line 57))\n","  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting openai==1.97.1 (from -r requirements.txt (line 58))\n","  Downloading openai-1.97.1-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: openpyxl==3.1.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 59)) (3.1.5)\n","Collecting outlines==1.1.1 (from -r requirements.txt (line 60))\n","  Downloading outlines-1.1.1-py3-none-any.whl.metadata (27 kB)\n","Collecting outlines_core==0.1.26 (from -r requirements.txt (line 61))\n","  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: packaging==25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 62)) (25.0)\n","Collecting pandas==2.3.1 (from -r requirements.txt (line 63))\n","  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow==11.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 64)) (11.3.0)\n","Requirement already satisfied: pluggy==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 65)) (1.6.0)\n","Requirement already satisfied: ply==3.11 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 66)) (3.11)\n","Requirement already satisfied: propcache==0.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 67)) (0.3.2)\n","Collecting pyarrow==21.0.0 (from -r requirements.txt (line 68))\n","  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Requirement already satisfied: pydantic==2.11.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 69)) (2.11.7)\n","Requirement already satisfied: pydantic_core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 70)) (2.33.2)\n","Requirement already satisfied: Pygments==2.19.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 71)) (2.19.2)\n","Requirement already satisfied: pyparsing==3.2.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 72)) (3.2.3)\n","Requirement already satisfied: pytest==8.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 73)) (8.4.1)\n","Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 74)) (2.9.0.post0)\n","Collecting python-dotenv==1.1.1 (from -r requirements.txt (line 75))\n","  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: pytz==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 76)) (2025.2)\n","Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 77)) (6.0.2)\n","Requirement already satisfied: referencing==0.36.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 78)) (0.36.2)\n","Requirement already satisfied: regex==2024.11.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 79)) (2024.11.6)\n","Collecting requests==2.32.4 (from -r requirements.txt (line 80))\n","  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n","Collecting rich==14.0.0 (from -r requirements.txt (line 81))\n","  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n","Collecting rpds-py==0.26.0 (from -r requirements.txt (line 82))\n","  Downloading rpds_py-0.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n","Collecting safetensors==0.5.3 (from -r requirements.txt (line 83))\n","  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Collecting scikit-learn==1.7.1 (from -r requirements.txt (line 84))\n","  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Collecting scipy==1.16.0 (from -r requirements.txt (line 85))\n","  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentence-transformers==5.0.0 (from -r requirements.txt (line 86))\n","  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 87)) (1.5.4)\n","Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 88)) (1.17.0)\n","Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 89)) (1.3.1)\n","Requirement already satisfied: starlette==0.47.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 90)) (0.47.2)\n","Collecting sympy==1.14.0 (from -r requirements.txt (line 91))\n","  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: tenacity==9.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 92)) (9.1.2)\n","Requirement already satisfied: threadpoolctl==3.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 93)) (3.6.0)\n","Collecting tokenizers==0.21.2 (from -r requirements.txt (line 94))\n","  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting torch==2.7.1 (from -r requirements.txt (line 95))\n","  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n","Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 96)) (4.67.1)\n","Collecting transformers==4.53.3 (from -r requirements.txt (line 97))\n","  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer==0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 98)) (0.16.0)\n","Requirement already satisfied: typing-inspection==0.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 99)) (0.4.1)\n","Requirement already satisfied: typing_extensions==4.14.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 100)) (4.14.1)\n","Requirement already satisfied: tzdata==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 101)) (2025.2)\n","Requirement already satisfied: urllib3==2.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 102)) (2.5.0)\n","Requirement already satisfied: xxhash==3.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 103)) (3.5.0)\n","Requirement already satisfied: yarl==1.20.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 104)) (1.20.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.33.4->-r requirements.txt (line 34)) (1.1.7)\n","Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.3.1 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch==2.7.1->-r requirements.txt (line 95)) (75.2.0)\n","Downloading aiohttp-3.12.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading airportsdata-20250706-py3-none-any.whl (912 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n","Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading genson-1.3.0-py3-none-any.whl (21 kB)\n","Downloading groq-0.26.0-py3-none-any.whl (129 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.6/129.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading instructor-1.10.0-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.5/119.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n","Downloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n","Downloading jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\n","Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.6/246.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.97.1-py3-none-any.whl (764 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.4/764.4 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading outlines-1.1.1-py3-none-any.whl (100 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n","Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rpds_py-0.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.9/383.9 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.53.3-py3-none-any.whl (10.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, genson, triton, sympy, safetensors, rpds-py, python-dotenv, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, multidict, markdown-it-py, lark, kiwisolver, jsonpath-ng, iso3166, interegular, diskcache, colorama, charset-normalizer, certifi, anyio, airportsdata, scipy, rich, requests, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, dotenv, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, huggingface-hub, aiohttp, torch, tokenizers, openai, groq, transformers, outlines_core, instructor, sentence-transformers, outlines, evaluate, bert-score\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n","    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.1\n","    Uninstalling sympy-1.13.1:\n","      Successfully uninstalled sympy-1.13.1\n","  Attempting uninstall: safetensors\n","    Found existing installation: safetensors 0.6.2\n","    Uninstalling safetensors-0.6.2:\n","      Successfully uninstalled safetensors-0.6.2\n","  Attempting uninstall: rpds-py\n","    Found existing installation: rpds-py 0.27.0\n","    Uninstalling rpds-py-0.27.0:\n","      Successfully uninstalled rpds-py-0.27.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 18.1.0\n","    Uninstalling pyarrow-18.1.0:\n","      Successfully uninstalled pyarrow-18.1.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.127\n","    Uninstalling nvidia-nvtx-cu12-12.4.127:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: multidict\n","    Found existing installation: multidict 6.6.4\n","    Uninstalling multidict-6.6.4:\n","      Successfully uninstalled multidict-6.6.4\n","  Attempting uninstall: markdown-it-py\n","    Found existing installation: markdown-it-py 4.0.0\n","    Uninstalling markdown-it-py-4.0.0:\n","      Successfully uninstalled markdown-it-py-4.0.0\n","  Attempting uninstall: kiwisolver\n","    Found existing installation: kiwisolver 1.4.9\n","    Uninstalling kiwisolver-1.4.9:\n","      Successfully uninstalled kiwisolver-1.4.9\n","  Attempting uninstall: charset-normalizer\n","    Found existing installation: charset-normalizer 3.4.3\n","    Uninstalling charset-normalizer-3.4.3:\n","      Successfully uninstalled charset-normalizer-3.4.3\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2025.8.3\n","    Uninstalling certifi-2025.8.3:\n","      Successfully uninstalled certifi-2025.8.3\n","  Attempting uninstall: anyio\n","    Found existing installation: anyio 4.10.0\n","    Uninstalling anyio-4.10.0:\n","      Successfully uninstalled anyio-4.10.0\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.16.1\n","    Uninstalling scipy-1.16.1:\n","      Successfully uninstalled scipy-1.16.1\n","  Attempting uninstall: rich\n","    Found existing installation: rich 13.9.4\n","    Uninstalling rich-13.9.4:\n","      Successfully uninstalled rich-13.9.4\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.32.3\n","    Uninstalling requests-2.32.3:\n","      Successfully uninstalled requests-2.32.3\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: contourpy\n","    Found existing installation: contourpy 1.3.3\n","    Uninstalling contourpy-1.3.3:\n","      Successfully uninstalled contourpy-1.3.3\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.6.1\n","    Uninstalling scikit-learn-1.6.1:\n","      Successfully uninstalled scikit-learn-1.6.1\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.10.0\n","    Uninstalling matplotlib-3.10.0:\n","      Successfully uninstalled matplotlib-3.10.0\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.34.4\n","    Uninstalling huggingface-hub-0.34.4:\n","      Successfully uninstalled huggingface-hub-0.34.4\n","  Attempting uninstall: aiohttp\n","    Found existing installation: aiohttp 3.12.15\n","    Uninstalling aiohttp-3.12.15:\n","      Successfully uninstalled aiohttp-3.12.15\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.4\n","    Uninstalling tokenizers-0.21.4:\n","      Successfully uninstalled tokenizers-0.21.4\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.99.8\n","    Uninstalling openai-1.99.8:\n","      Successfully uninstalled openai-1.99.8\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.55.0\n","    Uninstalling transformers-4.55.0:\n","      Successfully uninstalled transformers-4.55.0\n","  Attempting uninstall: sentence-transformers\n","    Found existing installation: sentence-transformers 5.1.0\n","    Uninstalling sentence-transformers-5.1.0:\n","      Successfully uninstalled sentence-transformers-5.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n","google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n","sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\n","gradio 5.42.0 requires huggingface-hub<1.0,>=0.33.5, but you have huggingface-hub 0.33.4 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.1 which is incompatible.\n","bigframes 2.14.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n","torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n","cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n","pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.1 which is incompatible.\n","dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiohttp-3.12.14 airportsdata-20250706 anyio-4.9.0 bert-score-0.3.13 certifi-2025.7.14 charset-normalizer-3.4.2 colorama-0.4.6 contourpy-1.3.2 diskcache-5.6.3 dotenv-0.9.9 evaluate-0.4.5 genson-1.3.0 groq-0.26.0 huggingface-hub-0.33.4 instructor-1.10.0 interegular-0.3.3 iso3166-2.1.1 jsonpath-ng-1.7.0 kiwisolver-1.4.8 lark-1.2.2 markdown-it-py-3.0.0 matplotlib-3.10.3 multidict-6.6.3 numpy-2.3.1 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 openai-1.97.1 outlines-1.1.1 outlines_core-0.1.26 pandas-2.3.1 pyarrow-21.0.0 python-dotenv-1.1.1 requests-2.32.4 rich-14.0.0 rpds-py-0.26.0 safetensors-0.5.3 scikit-learn-1.7.1 scipy-1.16.0 sentence-transformers-5.0.0 sympy-1.14.0 tokenizers-0.21.2 torch-2.7.1 transformers-4.53.3 triton-3.3.1\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"81689b78e9c74c078dfa93390e0c045e","pip_warning":{"packages":["certifi","kiwisolver","matplotlib","mpl_toolkits","numpy"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":160672,"status":"ok","timestamp":1755258282837,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"7jbcWB1PGsoj","outputId":"f1e6af3a-44d7-4073-aa29-b7881fd291cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.1)\n","Collecting torch\n","  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Collecting torchvision\n","  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n","  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n","  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n","Collecting nvidia-nccl-cu12==2.27.3 (from torch)\n","  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n","  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==3.4.0 (from torch)\n","  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (75.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.3.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n","    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.3.1\n","    Uninstalling triton-3.3.1:\n","      Successfully uninstalled triton-3.3.1\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.6.77\n","    Uninstalling nvidia-nvtx-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n","    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.26.2\n","    Uninstalling nvidia-nccl-cu12-2.26.2:\n","      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufile-cu12\n","    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n","    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n","      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n","    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.7.1\n","    Uninstalling torch-2.7.1:\n","      Successfully uninstalled torch-2.7.1\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.21.0+cu124\n","    Uninstalling torchvision-0.21.0+cu124:\n","      Successfully uninstalled torchvision-0.21.0+cu124\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\n","cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.8.0 torchvision-0.23.0 triton-3.4.0\n"]}],"source":["!pip install --upgrade torch torchvision"]},{"cell_type":"markdown","metadata":{"id":"-2wXabjgzszt"},"source":["## Import a python script from project"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1755258282908,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"a8WjxROAzwUT","outputId":"4d19e0ae-d832-4b05-b280-2ed6831b3e41"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":187,"status":"ok","timestamp":1755273246733,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"-oZlOWBODm4M","outputId":"0e456dbe-ed97-4e5c-86ae-871251818ef6"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: not a git repository (or any of the parent directories): .git\n"]}],"source":["!git pull"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2312,"status":"ok","timestamp":1755127732579,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"4r_Hif3sXJDN","outputId":"860f44ae-05d9-4a41-a2d6-6d8c1d3d8714"},"outputs":[{"name":"stdout","output_type":"stream","text":["08/13/2025 12:15:59 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:15:59 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:17:05 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:17:07 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:17:08 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:17:08 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:17:09 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:17:10 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:17:10 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpq5628o7m\n","08/13/2025 12:17:10 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpq5628o7m/_remote_module_non_scriptable.py\n","08/13/2025 12:17:17 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:17:17 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:17:17 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:17:20 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:17:20 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:17:21 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:17:21 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:17:21 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:18:33 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:18:35 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:18:35 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:18:35 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:18:36 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:18:37 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:18:37 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpwkv8wcnc\n","08/13/2025 12:18:37 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpwkv8wcnc/_remote_module_non_scriptable.py\n","08/13/2025 12:18:44 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:18:44 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:18:44 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:18:46 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:18:46 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:18:46 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:18:46 - huggingface_hub.file_download - INFO - Downloading 'generation_config.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/cf4ba0e4ebd236033b404876c972eac1be74b969.incomplete'\n","08/13/2025 12:18:46 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/cf4ba0e4ebd236033b404876c972eac1be74b969\n","08/13/2025 12:18:46 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:18:46 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:18:46 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:18:46 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:18:46 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:18:46 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:18:46 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:18:46 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:18:46 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:19:28 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:19:31 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:19:31 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:19:31 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:19:32 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:19:33 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:19:33 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmptn19p4br\n","08/13/2025 12:19:33 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmptn19p4br/_remote_module_non_scriptable.py\n","08/13/2025 12:19:40 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:19:40 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:19:40 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:19:41 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:19:41 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:19:41 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:19:41 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:19:41 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:19:41 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:19:41 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:19:41 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:19:41 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:19:41 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:19:41 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:19:41 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:22:11 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:22:13 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:22:13 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:22:13 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:22:14 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:22:15 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:22:15 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmp_u6jw64o\n","08/13/2025 12:22:15 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmp_u6jw64o/_remote_module_non_scriptable.py\n","08/13/2025 12:22:23 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:22:23 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:22:23 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:22:24 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:22:25 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:22:25 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:22:25 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:22:25 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:22:25 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:22:25 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:22:25 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:22:25 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:22:25 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:22:25 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/5145e0895f2fe7f1ccb3eb9da69ec74ec9c680db.incomplete'\n","08/13/2025 12:22:25 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/5145e0895f2fe7f1ccb3eb9da69ec74ec9c680db\n","08/13/2025 12:22:28 - transformers.configuration_utils - INFO - Model config PhiConfig {\n","  \"architectures\": [\n","    \"PhiForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 50256,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_size\": 2560,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 10240,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"phi\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"partial_rotary_factor\": 0.4,\n","  \"qk_layernorm\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","da48b7be36cc\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/c1148447551675ea739c440ee3e247df9f354d8f.incomplete'\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/c1148447551675ea739c440ee3e247df9f354d8f\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Downloading 'added_tokens.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7debb4784a7d53328d4d021fc46314bec4af3833.incomplete'\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7debb4784a7d53328d4d021fc46314bec4af3833\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/0204ed10c186a4c7c68f55dff8f26087a45898d6.incomplete'\n","08/13/2025 12:22:28 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/0204ed10c186a4c7c68f55dff8f26087a45898d6\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/vocab.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/merges.txt\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/added_tokens.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/special_tokens_map.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer_config.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","08/13/2025 12:22:28 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/011968cc02a5cca9fd46ecd994fc961e6906c0bd.incomplete'\n","08/13/2025 12:22:28 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/011968cc02a5cca9fd46ecd994fc961e6906c0bd\n","08/13/2025 12:22:28 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/config.json\n","08/13/2025 12:22:32 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors.index.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/fd9e856cfa23cd4bd5122f982feba515d23260a5.incomplete'\n","08/13/2025 12:22:32 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/fd9e856cfa23cd4bd5122f982feba515d23260a5\n","08/13/2025 12:22:32 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/model.safetensors.index.json\n","08/13/2025 12:22:33 - huggingface_hub.file_download - INFO - Downloading 'model-00002-of-00002.safetensors' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/17b98759e4b7953cbcf63ec49be7edbc9b863b57c207d84a52f5d2f5bcfcf6b4.incomplete'\n","08/13/2025 12:22:33 - huggingface_hub.file_download - INFO - Downloading 'model-00001-of-00002.safetensors' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7fbcdefa72edf7527bf5da40535b57d9f5bd3d16829b94a9d25d2b457df62e84.incomplete'\n","08/13/2025 12:23:22 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/17b98759e4b7953cbcf63ec49be7edbc9b863b57c207d84a52f5d2f5bcfcf6b4\n","08/13/2025 12:24:20 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7fbcdefa72edf7527bf5da40535b57d9f5bd3d16829b94a9d25d2b457df62e84\n","08/13/2025 12:24:20 - transformers.modeling_utils - INFO - Instantiating PhiForCausalLM model under default dtype torch.float32.\n","08/13/2025 12:24:20 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:24:44 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing PhiForCausalLM.\n","\n","08/13/2025 12:24:44 - transformers.modeling_utils - INFO - All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n","08/13/2025 12:24:45 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:24:45 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:24:46 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:24:46 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:24:46 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:24:47 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:24:47 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:24:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:24:55 - evaluate.utils.file_utils - INFO - https://huggingface.co/spaces/evaluate-metric/bertscore/resolve/main/bertscore.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/evaluate/downloads/tmp_hwmv5h2\n","08/13/2025 12:24:55 - evaluate.utils.file_utils - INFO - storing https://huggingface.co/spaces/evaluate-metric/bertscore/resolve/main/bertscore.py in cache at /root/.cache/huggingface/evaluate/downloads/2fb1405ec250844307c59b3bad240bb98cf25006711d08add1afcaf365fa5899.a4fc40aee04c356ddb89f55c1f36393b16fa831e9b5a9daa80e5e77cac867d1c.py\n","08/13/2025 12:24:55 - evaluate.utils.file_utils - INFO - creating metadata file for /root/.cache/huggingface/evaluate/downloads/2fb1405ec250844307c59b3bad240bb98cf25006711d08add1afcaf365fa5899.a4fc40aee04c356ddb89f55c1f36393b16fa831e9b5a9daa80e5e77cac867d1c.py\n","08/13/2025 12:24:55 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/e5c73d8a50df1f56fb5b0b8002d7cf4010afdccb.incomplete'\n","08/13/2025 12:24:55 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/e5c73d8a50df1f56fb5b0b8002d7cf4010afdccb\n","08/13/2025 12:24:56 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/150367d8744161cd17b3f6462a14f3a9648752da.incomplete'\n","08/13/2025 12:24:56 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/150367d8744161cd17b3f6462a14f3a9648752da\n","08/13/2025 12:24:56 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:24:56 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:24:56 - huggingface_hub.file_download - INFO - Downloading 'vocab.txt' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.incomplete'\n","08/13/2025 12:24:57 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938\n","08/13/2025 12:24:57 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/949a6f013d67eb8a5b4b5b46026217b888021b88.incomplete'\n","08/13/2025 12:24:57 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/949a6f013d67eb8a5b4b5b46026217b888021b88\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:24:57 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:24:57 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:24:58 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:24:58 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:24:59 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/5e3f1108e3cb34ee048634875d8482665b65ac713291a7e32396fb18f6ff0063.incomplete'\n","08/13/2025 12:25:06 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/5e3f1108e3cb34ee048634875d8482665b65ac713291a7e32396fb18f6ff0063\n","08/13/2025 12:25:06 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:06 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:06 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:06 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:06 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './modules.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/952a9b81c0bfd99800fabf352f69c7ccd46c5e43.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/952a9b81c0bfd99800fabf352f69c7ccd46c5e43\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './config_sentence_transformers.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fd1b291129c607e5d49799f87cb219b27f98acdf.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fd1b291129c607e5d49799f87cb219b27f98acdf\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './README.md' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/58d4a9a45664eb9e12de9549c548c09b6134c17f.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/58d4a9a45664eb9e12de9549c548c09b6134c17f\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './sentence_bert_config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/59d594003bf59880a884c574bf88ef7555bb0202.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/59d594003bf59880a884c574bf88ef7555bb0202\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/72b987fd805cfa2b58c4c8c952b274a11bfd5a00.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/72b987fd805cfa2b58c4c8c952b274a11bfd5a00\n","08/13/2025 12:25:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/53aa51172d142c89d9012cce15ae4d6cc0ca6895895114379cacb4fab128d9db.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/53aa51172d142c89d9012cce15ae4d6cc0ca6895895114379cacb4fab128d9db\n","08/13/2025 12:25:09 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:09 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:09 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/c79f2b6a0cea6f4b564fed1938984bace9d30ff0.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/c79f2b6a0cea6f4b564fed1938984bace9d30ff0\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'vocab.txt' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/cb202bfe2e3c98645018a6d12f182a434c9d3e02.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/cb202bfe2e3c98645018a6d12f182a434c9d3e02\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Downloading '1_Pooling/config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/d1514c3162bbe87b343f565fadc62e6c06f04f03.incomplete'\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/d1514c3162bbe87b343f565fadc62e6c06f04f03\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/88bc4f74b33a2073abc9a66cb532b889448ac3ed.incomplete'\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/88bc4f74b33a2073abc9a66cb532b889448ac3ed\n","08/13/2025 12:25:10 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:10 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/821d1aa69520101d6e0737f78a042ae25b19e5cb9160701909d10434f4aeb0ae.incomplete'\n","08/13/2025 12:25:12 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/821d1aa69520101d6e0737f78a042ae25b19e5cb9160701909d10434f4aeb0ae\n","08/13/2025 12:25:12 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:12 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:12 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/a2435fedfac32b9ad70f052d4f84007730cd3109.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/a2435fedfac32b9ad70f052d4f84007730cd3109\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'vocab.txt' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/688882a79f44442ddc1f60d70334a7ff5df0fb47.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/688882a79f44442ddc1f60d70334a7ff5df0fb47\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/7520992f25914d962f0e2fd0e0566fc33d19ec59.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/7520992f25914d962f0e2fd0e0566fc33d19ec59\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading './README.md' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/4783b64ce66b94c7c387672de541d10678980574.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/4783b64ce66b94c7c387672de541d10678980574\n","08/13/2025 12:25:13 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:14 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:15 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:15 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:22 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:22 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:23 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:24 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:24 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:24 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:24 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:26 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:26 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:26 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:26 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:34 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:34 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:34 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:34 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:34 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:35 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:35 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:35 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:35 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:35 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:36 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:36 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:36 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:36 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:36 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:36 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:36 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:36 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:45 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:45 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:45 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:45 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:45 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:45 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:45 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:47 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:25:47 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:55 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:55 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:55 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:55 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:55 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:56 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:56 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:56 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:56 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:56 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:57 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:57 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:57 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:57 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:57 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:57 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:57 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:58 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:58 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:06 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:06 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:06 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:26:06 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:26:06 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:26:06 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:26:06 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:08 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:26:08 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:26:08 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:26:08 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:26:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:15 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:26:16 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:26:16 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:26:16 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:26:16 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:18 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:26:18 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:26:18 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:26:18 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:26:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:26:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:25 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:26 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:26:26 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:26:26 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:26:26 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:26:26 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:26:28 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:26:28 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:28 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:26:28 - mods.dataHandler - WARNING - Folder does not exist, creating new folder in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench\n","08/13/2025 12:26:28 - mods.dataHandler - INFO - Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench/tb-experiment-13-082025 12-26-28.xlsx\n","08/13/2025 12:26:28 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:26:28 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:39:31 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:39:33 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:39:34 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:39:34 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:39:34 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:39:35 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:39:35 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmp59ngi2rd\n","08/13/2025 12:39:35 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmp59ngi2rd/_remote_module_non_scriptable.py\n","08/13/2025 12:39:43 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:39:43 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:39:43 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:39:44 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:39:44 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:39:44 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:39:45 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:39:45 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:39:45 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:39:45 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:39:45 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:39:45 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:39:45 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/vocab.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/merges.txt\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/added_tokens.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/special_tokens_map.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer_config.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","08/13/2025 12:39:45 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/config.json\n","08/13/2025 12:39:45 - transformers.configuration_utils - INFO - Model config PhiConfig {\n","  \"architectures\": [\n","    \"PhiForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 50256,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_size\": 2560,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 10240,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"phi\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"partial_rotary_factor\": 0.4,\n","  \"qk_layernorm\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","08/13/2025 12:39:47 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/model.safetensors.index.json\n","08/13/2025 12:39:47 - transformers.modeling_utils - INFO - Instantiating PhiForCausalLM model under default dtype torch.float32.\n","08/13/2025 12:39:47 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:11 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing PhiForCausalLM.\n","\n","08/13/2025 12:40:11 - transformers.modeling_utils - INFO - All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:12 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:21 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:21 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:23 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:23 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:23 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:23 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:23 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:24 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:24 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:24 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:24 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:24 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:25 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:25 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:25 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:25 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:25 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:25 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:25 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:25 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:34 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:34 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:34 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:34 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:34 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:35 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:35 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:35 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:35 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:35 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:36 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:36 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:36 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:36 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:36 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:36 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:36 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:36 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:44 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:45 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:45 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:45 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:45 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:47 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:47 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:47 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:47 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:47 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:47 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:55 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:55 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:55 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:55 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:55 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:56 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:56 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:56 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:56 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:56 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:57 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:57 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:57 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:57 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:57 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:57 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:58 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:05 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:05 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:05 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:05 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:05 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:06 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:06 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:06 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:06 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:06 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:07 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:07 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:07 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:07 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:07 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:41:08 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:41:08 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:41:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:15 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:16 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:16 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:16 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:16 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:18 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:18 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:41:18 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:41:18 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:41:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:26 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:26 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:26 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:26 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:26 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:27 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:27 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:27 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:27 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:27 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:28 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:28 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:28 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:28 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:28 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:28 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:28 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:41:28 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:41:28 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:41:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:36 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:36 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:36 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:36 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:36 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:37 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:37 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:37 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:37 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:37 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:38 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:38 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:38 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:38 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:38 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:38 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:39 - mods.dataHandler - INFO - Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench/tb-experiment-13-082025 12-41-39.xlsx\n","08/13/2025 12:41:39 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:41:39 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:44:13 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:44:15 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:44:16 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:44:16 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:44:16 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:44:18 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:44:18 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmp92hg4lkh\n","08/13/2025 12:44:18 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmp92hg4lkh/_remote_module_non_scriptable.py\n","08/13/2025 12:44:26 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:44:26 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:44:26 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:44:27 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:44:27 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 13:26:51 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 13:26:53 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 13:26:53 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:26:53 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:26:54 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 13:26:55 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 13:26:55 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpehk321xo\n","08/13/2025 13:26:55 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpehk321xo/_remote_module_non_scriptable.py\n","08/13/2025 13:27:02 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 13:27:02 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:27:02 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:27:03 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 13:27:03 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 13:28:55 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 13:28:57 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 13:28:57 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:28:57 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:28:58 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 13:29:00 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 13:29:00 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpc6veeyyt\n","08/13/2025 13:29:00 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpc6veeyyt/_remote_module_non_scriptable.py\n","08/13/2025 13:29:06 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 13:29:06 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:29:06 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:29:07 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 13:29:07 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 13:29:08 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 13:29:08 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 13:29:08 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:08 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 13:29:08 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 13:29:08 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 13:29:08 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 13:29:08 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 13:29:08 - transformers.configuration_utils - INFO - Model config PhiConfig {\n","  \"architectures\": [\n","    \"PhiForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 50256,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_size\": 2560,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 10240,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"phi\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"partial_rotary_factor\": 0.4,\n","  \"qk_layernorm\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","t /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/added_tokens.json\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/special_tokens_map.json\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer_config.json\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","08/13/2025 13:29:08 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/config.json\n","08/13/2025 13:29:10 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/model.safetensors.index.json\n","08/13/2025 13:29:10 - transformers.modeling_utils - INFO - Instantiating PhiForCausalLM model under default dtype torch.float32.\n","08/13/2025 13:29:10 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:40 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing PhiForCausalLM.\n","\n","08/13/2025 13:29:40 - transformers.modeling_utils - INFO - All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:40 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 0.5, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/13/2025 13:29:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 13:29:40 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 13:29:40 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n"]}],"source":["!cat app/logs/logfile.log"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i79nllcxdMwy"},"outputs":[],"source":["!python projectSetup.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205,"referenced_widgets":["0b15381868984f48a958540b50f785c6","6f06d02f23654471b0c1377b8d7f3203","73ddf9e17e6a4cfe85cb47a2625303e8","b5157bdd857148fb9d5c6488691a81b8","c260a953e8564a31bd2d64fbb3445f84","9e33742605ca4f12863776e771c9dc77","e54018a2d58a4c1e95410f5c9bc419e1","6c7b05ef240b4737948d1a48b423624e","3a0a7902dddf42028ff6c54a3c2af8d2","b91e110c59a2464284d4d00d3204eca4","aa7496c29bef49fc8a29d3102800ce4f"]},"executionInfo":{"elapsed":3162,"status":"ok","timestamp":1755087390807,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"Srd-wh7zg8Qt","outputId":"f4af760b-37a5-4868-e580-65d4c763e782"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b15381868984f48a958540b50f785c6","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=gpt2\n"]}],"source":["import sys, os\n","import torch\n","from pathlib import Path\n","sys.path.append(os.getcwd())\n","sys.path.append(os.getcwd() + '/app')\n","\n","from app.mods.promptGenerator import PromptGenerator\n","from app.mods.modelLoader import ModelLoader\n","from app.mods.reportGenerator import ReportGenerator\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","torch_dtype = torch.float32 if torch.cuda.is_available() else torch.float32\n","\n","ml = ModelLoader(model_id=\"gpt2\", device=device, torch_dtype=torch_dtype)"]},{"cell_type":"markdown","metadata":{"id":"rfGNcNvTUhdj"},"source":["## microsoft/phi-2\n","It allocates 12 GB in RAM, the extra depends on the number of workers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"P57w5lE7nDyX","outputId":"fe8ae7f1-62f0-4e37-f24d-49a6cfad33c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","08/15/2025 12:28:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=12\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:28:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:28:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=12\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:28:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:28:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","\n","08/15/2025 12:28:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:28:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","08/15/2025 12:28:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:28:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:28:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 313.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 633.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 628.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 619.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 609.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 610.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 608.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.93 GiB is allocated by PyTorch, and 604.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 607.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 588.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 279.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 574.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 573.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 571.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 296.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:29:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","\n","08/15/2025 12:29:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 581.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 581.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 581.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 306.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 577.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 564.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 562.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 310.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","08/15/2025 12:29:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","08/15/2025 12:29:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 257.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 257.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 257.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 257.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 453.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 507.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.50 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:30:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:30:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:30:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:30:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:30:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","08/15/2025 12:30:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:30:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:30:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:31:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","\n","08/15/2025 12:31:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:31:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:31:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:31:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","\n","08/15/2025 12:31:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:31:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","\n","08/15/2025 12:31:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:31:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:31:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:31:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:31:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:31:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:31:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:31:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:07 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:32:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 251.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:32:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 290.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 259.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 482.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 478.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 483.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 482.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 481.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 479.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 481.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 256.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 499.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 494.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 498.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 500.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 479.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 477.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 472.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 478.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 478.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 473.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 273.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:32:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:32:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:49 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:32:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:32:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","\n","08/15/2025 12:32:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:33:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:33:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","\n","08/15/2025 12:33:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:33:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","\n","08/15/2025 12:33:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:33:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:33:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:33:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:33:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:33:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:33:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:34:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","\n","08/15/2025 12:34:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:34:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:34:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:34:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:34:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:34:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:34:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:34:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:34:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:34:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:35:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:35:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 510.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 508.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 505.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 510.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 500.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 459.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 450.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 430.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 430.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 269.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 438.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:35:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","\n","08/15/2025 12:35:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 246.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 429.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 435.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 428.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:35:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","\n","08/15/2025 12:35:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 423.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 245.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 245.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 417.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 414.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 411.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 411.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.33 GiB is allocated by PyTorch, and 191.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:35:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","\n","08/15/2025 12:35:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 202.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:36:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:07 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:36:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","\n","08/15/2025 12:36:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1465 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...absent from the scanned', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:36:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1465 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...absent from the scanned', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:36:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","\n","08/15/2025 12:36:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:08 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1488 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...absent from the scanned', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:37:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:37:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:37:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1465 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...absent from the scanned', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:37:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:37:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:37:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:37:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:37:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","\n","08/15/2025 12:37:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:38:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","\n","08/15/2025 12:38:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:38:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","\n","08/15/2025 12:38:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:38:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 486.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 498.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","08/15/2025 12:38:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 443.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 370.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 447.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","\n","08/15/2025 12:38:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 443.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","08/15/2025 12:38:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 353.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 442.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 441.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 441.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 439.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 439.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 441.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 325.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 438.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 440.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 439.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 438.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 435.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 328.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 430.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 299.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","\n","08/15/2025 12:38:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:38:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 279.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:38:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 275.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:38:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 468.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:38:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 455.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:38:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 253.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 256.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:38:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:39:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:39:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:39:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:39:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:39:43 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 285.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:43 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","08/15/2025 12:39:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:39:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","\n","08/15/2025 12:39:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:39:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:40:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:40:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:40:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:41:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","\n","08/15/2025 12:41:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 536.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 300.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 418.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 417.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 727.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:41:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","\n","08/15/2025 12:41:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 639.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 470.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 470.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:36 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:36 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 293.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 321.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 299.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 407.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 407.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 401.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 405.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 282.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 403.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 399.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 397.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 401.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 335.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:41:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","\n","08/15/2025 12:41:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 612.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:41 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.30 GiB is allocated by PyTorch, and 219.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:41:41 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","\n","08/15/2025 12:41:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:41:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:41:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:42:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:42:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:42:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:42:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:42:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:43:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:43:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:43:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:43:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","\n","08/15/2025 12:43:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:44:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:44:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:44:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","\n","08/15/2025 12:44:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","\n","08/15/2025 12:44:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 269.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 499.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 504.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 503.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 502.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 498.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 272.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 493.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 255.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:28 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","\n","08/15/2025 12:44:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 504.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:29 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 509.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:29 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 527.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:29 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 695.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 269.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 333.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.72 GiB is allocated by PyTorch, and 803.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.68 GiB is allocated by PyTorch, and 840.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.69 GiB is allocated by PyTorch, and 836.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.68 GiB is allocated by PyTorch, and 842.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.75 GiB is allocated by PyTorch, and 778.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.75 GiB is allocated by PyTorch, and 778.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 278.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","\n","08/15/2025 12:44:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:44:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:45:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:45:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:45:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:45:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","\n","08/15/2025 12:45:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:45:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:45:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:45:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:45:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","\n","08/15/2025 12:45:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:45:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:45:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:45:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:46:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:46:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:46:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:46:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:46:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","\n","08/15/2025 12:46:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:46:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:46:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:46:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:46:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:46:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:46:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","\n","08/15/2025 12:46:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:47:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:47:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:07 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 282.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.93 GiB is allocated by PyTorch, and 608.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 484.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","\n","08/15/2025 12:47:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 467.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 477.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 365.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 551.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 584.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 463.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 463.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 568.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 736.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 542.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 347.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 551.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 551.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 346.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:28 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","\n","08/15/2025 12:47:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:29 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 363.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:29 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 548.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 380.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 498.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 577.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 577.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 513.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 316.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 426.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 357.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 346.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 486.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 521.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 315.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 394.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:34 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 322.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:34 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 335.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 351.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 384.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 334.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 383.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:36 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:36 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 546.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:36 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 373.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 398.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 357.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 351.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 395.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 536.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 533.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 538.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 509.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 537.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 518.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 524.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 511.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 533.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 499.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 527.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 482.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 525.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 480.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 512.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 496.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 529.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 522.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 526.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 506.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 528.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 429.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 428.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 458.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 327.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 300.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 448.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:41 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 273.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:41 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 300.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:41 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:41 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:42 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 275.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:42 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:42 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 270.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:42 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:43 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 414.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:43 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:43 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 291.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:43 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 297.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:43 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:43 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:44 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 270.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:44 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 289.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 357.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 349.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 360.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 354.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 346.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 338.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 260.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 333.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 332.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 260.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 333.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 290.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 332.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 337.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 336.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 258.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 325.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 294.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 299.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 295.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 297.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 299.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 297.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 292.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 296.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 296.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 297.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 290.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 295.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 297.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 295.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 283.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 290.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 291.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 291.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 290.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 290.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 283.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 288.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 289.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 286.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 290.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 268.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 271.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 307.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 258.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 308.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 317.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 318.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 307.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 316.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 257.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 316.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 312.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 295.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 306.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 317.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 316.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 305.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 305.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 263.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 303.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 303.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 314.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 305.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 325.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 328.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 328.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 323.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 328.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 322.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 325.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 303.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 304.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 303.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 301.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 301.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 301.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 300.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 280.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 279.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 278.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 278.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 273.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 252.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 251.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 255.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 278.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 277.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 278.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 268.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 293.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 284.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 293.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 248.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 233.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 289.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 300.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 283.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 281.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 286.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 286.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 281.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 285.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 283.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 284.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 570.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 315.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 346.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:47:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 595.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 332.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 309.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 417.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:02 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 283.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:02 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 418.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 432.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 432.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 427.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 424.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 403.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 260.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 355.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 355.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 448.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 560.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 550.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 550.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 515.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 447.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 301.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 267.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 253.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 253.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:07 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 266.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:07 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:08 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 235.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:08 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:08 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 248.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:08 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 262.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 480.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 590.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 628.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.75 GiB is allocated by PyTorch, and 792.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.77 GiB is allocated by PyTorch, and 766.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.73 GiB is allocated by PyTorch, and 807.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:11 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 357.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:11 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:12 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 337.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:12 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 359.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 359.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:48:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:49:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:49:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:49:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/15/2025 12:49:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:49:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:49:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:49:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=23\n","08/15/2025 12:49:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:49:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:49:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:49:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","\n","08/15/2025 12:49:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:50:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:50:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:50:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:50:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:50:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:50:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","\n","08/15/2025 12:50:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:50:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:50:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:50:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:50:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","\n","08/15/2025 12:50:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:51:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 247.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 494.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.31 GiB is allocated by PyTorch, and 216.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 254.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 509.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 507.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 500.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 504.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:51:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 504.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 233.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 486.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 489.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 404.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 404.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 358.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 515.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.30 GiB is allocated by PyTorch, and 229.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 374.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 636.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 627.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.86 GiB is allocated by PyTorch, and 663.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 620.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 250.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:51:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","\n","08/15/2025 12:51:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 477.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:51:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 250.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:51:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 482.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:51:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 259.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:34 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 314.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:34 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 264.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 272.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:51:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 290.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:51:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:51:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","\n","08/15/2025 12:51:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 310.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 309.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:51:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","\n","08/15/2025 12:51:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:51:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:52:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 292.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:52:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:52:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:52:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:52:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:52:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:52:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:52:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:52:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:52:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:52:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","08/15/2025 12:53:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","08/15/2025 12:53:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","08/15/2025 12:53:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 256.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:53:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 240.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 567.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 539.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 542.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 542.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 542.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 202.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:53:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.33 GiB is allocated by PyTorch, and 192.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:53:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 526.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:53:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 236.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:53:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 522.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:53:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","\n","08/15/2025 12:53:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 251.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:54:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 514.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 641.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:54:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 510.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:02 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:54:02 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","\n","08/15/2025 12:54:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:02 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 281.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:02 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","08/15/2025 12:54:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 432.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 573.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 228.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 344.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 344.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:07 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 271.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:07 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:54:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 267.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:54:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 445.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 445.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:54:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:54:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:54:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","\n","\n","08/15/2025 12:54:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:54:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","\n","08/15/2025 12:54:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:55:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","\n","08/15/2025 12:55:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:55:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:55:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:55:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:55:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:55:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:55:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:55:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:55:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:55:49 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","\n","08/15/2025 12:55:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:55:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:55:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:56:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","\n","08/15/2025 12:56:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:56:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","\n","08/15/2025 12:56:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","\n","08/15/2025 12:56:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:56:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 308.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","\n","08/15/2025 12:56:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 238.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:56:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 508.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:56:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 507.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","\n","08/15/2025 12:56:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 508.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:56:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 508.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","\n","08/15/2025 12:56:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 509.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:56:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 236.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 224.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 498.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 496.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 497.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 238.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 242.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 494.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 495.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 495.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 489.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 496.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 499.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 265.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 506.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 287.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:57:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 330.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 330.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:57:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","\n","08/15/2025 12:57:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 285.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:57:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","\n","08/15/2025 12:57:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 306.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 566.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:57:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:58:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:58:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:58:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:58:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:58:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:58:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:58:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:58:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:58:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:58:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:58:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","\n","08/15/2025 12:58:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 462.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.73 GiB is allocated by PyTorch, and 812.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 371.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 262.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 265.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 370.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 369.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 372.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 374.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 374.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 370.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 369.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 241.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 671.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 674.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 372.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 267.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 257.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 435.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 517.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 277.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 360.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 321.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 337.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:00:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 367.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:00:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 367.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:00:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:00:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:00:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:00:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:00:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 13:00:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:00:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:01:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","\n","08/15/2025 13:01:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:01:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:01:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:01:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:01:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","\n","08/15/2025 13:01:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:01:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:01:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:01:49 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 13:02:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:02:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:02:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:02:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","\n","08/15/2025 13:02:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:02:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:02:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:03:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 287.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 462.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 491.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.93 GiB is allocated by PyTorch, and 590.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 419.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:03:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","\n","08/15/2025 13:03:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 239.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 221.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 226.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 396.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 397.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 397.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 364.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 324.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 374.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.88 GiB is allocated by PyTorch, and 649.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 613.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:03:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","\n","08/15/2025 13:03:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.88 GiB is allocated by PyTorch, and 640.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 612.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.88 GiB is allocated by PyTorch, and 648.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 499.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 253.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 248.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 301.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 220.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 308.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:03:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 509.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:03:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 509.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 507.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:03:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 283.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 298.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:03:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:43 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 245.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:43 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:03:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:04:08 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 264.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:04:08 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:04:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:04:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:04:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:04:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:04:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:04:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:04:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:04:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:04:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","\n","08/15/2025 13:04:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:04:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:05:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:05:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:05:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:05:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:05:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:05:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:05:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:05:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:05:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","\n","08/15/2025 13:05:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:05:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:05:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:06:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.30 GiB is allocated by PyTorch, and 220.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 262.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 252.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 227.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 518.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 518.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 514.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 224.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 258.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 520.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 515.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:06:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 250.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 227.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 449.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 441.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 450.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 240.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 229.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 443.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 445.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 443.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 441.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:28 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 243.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:28 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 434.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:28 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 427.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:06:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 198.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:06:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:07:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:07:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:07:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","\n","08/15/2025 13:07:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:07:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","\n","08/15/2025 13:07:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:07:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:07:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:07:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","\n","08/15/2025 13:07:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:07:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","\n","08/15/2025 13:07:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:08:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:08:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:08:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:08:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:08:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","\n","08/15/2025 13:08:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:08:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:08:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:09:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:09:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:09:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:09:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:09:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:09:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:09:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:09:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 266.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 537.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 536.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 535.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 529.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 534.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 534.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 533.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 532.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 262.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 255.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 245.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 519.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 514.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 263.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.31 GiB is allocated by PyTorch, and 220.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 510.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 506.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 506.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:10:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","\n","08/15/2025 13:10:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 505.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 228.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 488.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 496.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 233.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:10:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","\n","08/15/2025 13:10:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 492.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:11:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:11:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:11:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:11:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1233 [type=json_invalid, input_value='{ \"title\": \"Dissolution ...D3204 of Painex 200 mg.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/15/2025 13:11:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","reportParamGridSearch time --- 86.03608230352401 minutes ---\n"]}],"source":["!python app/reportParamGridSearch.py --model_id microsoft/phi-2 --prompt_method B C --max_workers 12 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 30  --temperature 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 30 50 70 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","metadata":{"id":"vVTm54F1UaHq"},"source":["## HuggingFaceTB/SmolLM3-3B\n","It allocates 14 GB in RAM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2165754,"status":"ok","timestamp":1755207837726,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"DdF2u_DS5WaO","outputId":"ede959e3-d064-4eea-e89e-8025ea70472e"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-08-14 21:07:57.091907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755205677.113963   69791 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755205677.121261   69791 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755205677.139998   69791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755205677.140022   69791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755205677.140024   69791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755205677.140027   69791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-08-14 21:07:57.144843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Parameters passed to main script: \n","{'max_workers': [12], 'model_id': ['HuggingFaceTB/SmolLM3-3B'], 'prompt_method': ['B', 'C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [21], 'end_idx': [22], 'temperature': [0.7, 0.9, 1.1, 1.3], 'top_p': [0.4, 0.5, 0.7, 0.8], 'top_k': [30, 40, 50, 60], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","08/14/2025 21:08:12 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","08/14/2025 21:08:12 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","08/14/2025 21:08:12 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","Generation parameters: \n","{'temperature': [0.7, 0.9, 1.1, 1.3], 'top_p': [0.4, 0.5, 0.7, 0.8], 'top_k': [30, 40, 50, 60], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.15s/it]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.53it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.22it/s]\n","Batches: 100% 1/1 [00:00<00:00,  5.41it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.18it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.33it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.51it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.76it/s]\n","Batches: 100% 1/1 [00:00<00:00,  5.36it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.35it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.66it/s]\n","Batches: 100% 1/1 [00:00<00:00,  4.19it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.17it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.47it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.03it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  2.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.16it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.24it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.31it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.63it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.88it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.43it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.62it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.38it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.20it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.13it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.93it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.30it/s]\n","Batches: 100% 1/1 [00:00<00:00,  3.10it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.85it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.07it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.41it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.88it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.74it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.86it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.34it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.10it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.49it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.28it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.75it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.02it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.34it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.98it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.75it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.19it/s]\n","Batches: 100% 1/1 [00:00<00:00,  6.72it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  4.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.71it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.70it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.70it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.26it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.04it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  4.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.53it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.56it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.32it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.03it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.55it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.24it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.77it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.25it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.92it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.09it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.58it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.68it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.27it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.21it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 12.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.58it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.76it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.05it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.21it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.77it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.84it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.10it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.59it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.56it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.99it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.89it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.74it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.86it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.64it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.73it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.70it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.01it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.92it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.52it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.53it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.55it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.43it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.49it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.41it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.55it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.12it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.07it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.89it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.15it/s]\n","Batches: 100% 1/1 [00:00<00:00,  5.28it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.02it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.85it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.74it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.24it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.74it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.17it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.29it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.04it/s]\n","Batches: 100% 1/1 [00:00<00:00,  2.70it/s]\n","Batches:   0% 0/1 [00:00<?, ?it/s]Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.16it/s]\n","Batches: 100% 1/1 [00:00<00:00,  4.14it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.34it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.97it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.96it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  4.91it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.52it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.15it/s]\n","Batches: 100% 1/1 [00:00<00:00,  4.10it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.03it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.35it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.00it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.91it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.30it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.42it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.41it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.84it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.31it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.76it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.57it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.00it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.14it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.19it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.64it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.93it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.37it/s]\n","Batches: 100% 1/1 [00:00<00:00,  6.88it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.58it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.20it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.91it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.13it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.73it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.33it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.79it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.68it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.60it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.51it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.53it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.22it/s]\n","Batches: 100% 1/1 [00:00<00:00,  3.85it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.57it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.18it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.57it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.73it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00,  3.87it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.11it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.74it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.83it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.74it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.74it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.24it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.28it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.75it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 12.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.52it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.65it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.18it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.96it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.03it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.52it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.96it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.16it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 13.17it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.57it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.19it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.89it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.39it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.54it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.61it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.75it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.13it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.51it/s]\n","Batches: 100% 1/1 [00:00<00:00,  6.82it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.76it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.78it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.28it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.78it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.64it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.16it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.93it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.07it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.58it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.97it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.10it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.58it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.81it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.60it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.32it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.65it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.28it/s]\n","Batches: 100% 1/1 [00:00<00:00,  6.87it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.03it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.67it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.33it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.50it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.85it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.22it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.77it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.25it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.78it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.42it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.97it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.47it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.01it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.64it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.04it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.01it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.24it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.63it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.25it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.84it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.80it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.74it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.00it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.55it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.73it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.46it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.09it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.25it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.84it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.01it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.70it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.64it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.55it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.86it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.41it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 13.13it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.32it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.49it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.95it/s]\n","Batches: 100% 1/1 [00:00<00:00,  3.26it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.95it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.01it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.86it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.11it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.38it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.34it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.62it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.43it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.43it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.26it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.50it/s]\n","Batches: 100% 1/1 [00:00<00:00,  5.97it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  4.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.13it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.32it/s]\n","Batches: 100% 1/1 [00:00<00:00,  3.11it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.42it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.78it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 12.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.56it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.00it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.76it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.56it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 16.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.14it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.62it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.80it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.18it/s]\n","Batches: 100% 1/1 [00:00<00:00,  6.02it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.15it/s]\n","Batches: 100% 1/1 [00:00<00:00,  4.30it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.22it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.54it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.43it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.91it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.39it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.58it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  2.31it/s]\n","Batches: 100% 1/1 [00:00<00:00,  4.57it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.18it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.19it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.43it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.11it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.39it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.61it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.64it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.07it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.57it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.04it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.80it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.72it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.01it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.82it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.12it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.54it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.53it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 10.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.87it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.75it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  2.42it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.36it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.44it/s]\n","Batches: 100% 1/1 [00:00<00:00,  2.03it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.67it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.50it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.22it/s]\n","Batches:   0% 0/1 [00:00<?, ?it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.57it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","\n","Batches: 100% 1/1 [00:00<00:00,  3.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.34it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.06it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.22it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.64it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.06it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.75it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.25it/s]\n","Batches: 100% 1/1 [00:00<00:00,  3.00it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 10.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.39it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.26it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.10it/s]\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 14.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 13.17it/s]\n","Batches:   0% 0/1 [00:00<?, ?it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 181.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 172.61it/s]\n","reportParamGridSearch time --- 35.87140321334203 minutes ---\n"]}],"source":["!python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM3-3B  --prompt_method B C --max_workers 12 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 21 --end_idx 22  --temperature 0.7 0.9 1.1 1.3 --top_p 0.4 0.5 0.7 0.8 --top_k 30 40 50 60 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","metadata":{"id":"GBWVQWjZUUrm"},"source":["## GPT2-XL\n","It allocates 7.2 GB in RAM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1725507,"status":"ok","timestamp":1755203612946,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"XmhpRs4v6SBP","outputId":"e1850293-6903-48b5-9120-f34eb5a75ab0"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-08-14 20:04:52.560226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755201892.581674   54186 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755201892.588676   54186 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755201892.606844   54186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755201892.606870   54186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755201892.606873   54186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755201892.606876   54186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-08-14 20:04:52.611403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Parameters passed to main script: \n","{'max_workers': [12], 'model_id': ['openai-community/gpt2-xl'], 'prompt_method': ['B', 'C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [21], 'end_idx': [22], 'temperature': [0.7, 0.9, 1.1, 1.3], 'top_p': [0.4, 0.5, 0.7, 0.8], 'top_k': [30, 40, 50, 60], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","08/14/2025 20:05:07 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","08/14/2025 20:05:07 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","08/14/2025 20:05:07 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","Generation parameters: \n","{'temperature': [0.7, 0.9, 1.1, 1.3], 'top_p': [0.4, 0.5, 0.7, 0.8], 'top_k': [30, 40, 50, 60], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 13.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.83it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches:   0% 0/1 [00:00<?, ?it/s]08/14/2025 20:05:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 14.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.08it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:05:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 12.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.49it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:05:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 14.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.53it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:05:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches:   0% 0/1 [00:00<?, ?it/s]08/14/2025 20:05:59 - mods.testBench - ERROR - FAILED report export: write to closed file on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:00 - mods.testBench - ERROR - FAILED report export: Invalid IPC message: negative metadata length on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.82it/s]\n","Batches:   0% 0/1 [00:00<?, ?it/s]08/14/2025 20:06:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00,  1.81it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 13.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.81it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:06:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","Batches:   0% 0/1 [00:00<?, ?it/s]08/14/2025 20:06:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 13.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.71it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:06:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:06:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:06:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:06:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:06:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:06:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:06:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:06:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:06:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:06:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:06:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:07:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:07:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:08:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:08:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:08:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:08:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:08:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:08:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:08:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:08:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:08:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:08:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:08:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:09:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:09:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:09:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:09:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:09:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:09:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:09:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:10:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:10:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:10:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:10:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:10:37 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1054 [type=json_invalid, input_value='{ \"title\": \"Batch B2001 ...tch B2001 OOS result in', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:10:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:10:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:10:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:10:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:10:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:10:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:11:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:11:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:11:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:11:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:11:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:11:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 949 [type=json_invalid, input_value='{ \"title\": \"OOS result i...ons-observations-observ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:11:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:11:53 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 357 [type=json_invalid, input_value='{ \"title\": \"OOS result i...-0-0-0-0-0-0-0-0-0-0-0-', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:11:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:12:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1392 [type=json_invalid, input_value='{ \"title\":\"Report Title\"...t Results: Test Results', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:12:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:12:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:12:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:12:07 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:12:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:12:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:12:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:12:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:12:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:12:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:12:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:12:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:13:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:13:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:13:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:13:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:14:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:14:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:14:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:14:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:14:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:14:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:14:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:14:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:14:39 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1299 [type=json_invalid, input_value='{ \"title\": \"Correct tabl... in the Microbiology QC', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:14:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:14:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:14:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:14:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:14:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:14:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:14:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:14:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:15:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:15:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:15:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:15:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:15:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:15:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:15:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:15:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:15:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:15:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:16:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:16:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:16:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:16:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:16:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:16:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:16:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:16:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:17:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:08 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1448 [type=json_invalid, input_value='{ \"title\": \"OOS result i...iately retested Batch B', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:17:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1196 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...sting of Batch B2001 of', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:17:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:17:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:17:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:17:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:17:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:17:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:18:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:18:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:18:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:18:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1231 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...ted. The batch was held', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:18:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:18:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:18:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:18:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:18:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:18:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:18:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:19:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:19:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:19:49 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:19:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:19:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:19:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:55 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1310 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet... in the Microbiology QC', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:20:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:20:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:20:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:21:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:21:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:21:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:21:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:21:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:21:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:21:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:21:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:21:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:21:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:21:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:21:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:21:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:21:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:21:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:21:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:21:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:22:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:22:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:22:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:22:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:23:02 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1364 [type=json_invalid, input_value='{ \"title\": \"Humidity exc....\\\\\"} {\\\\\"title\\\\\": \\\\\"', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:23:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:23:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:23:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:23:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:23:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:23:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:23:33 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 510 [type=json_invalid, input_value='{ \"title\": \"Humidity exc...0-2, G-20-2, G-20-2, G-', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:23:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:23:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:24:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:24:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:24:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:24:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:24:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:24:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:24:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:25:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:25:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:25:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1126 [type=json_invalid, input_value='{ \"title\": \"Humidity exc...t. Report is in [title,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:25:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:25:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:25:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:25:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:25:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 731 [type=json_invalid, input_value='{ \"title\": \"HVAC Filter\"...s://www.google.com/url?', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:25:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:25:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:25:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:25:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:26:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:26:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:26:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:26:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:26:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:26:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:26:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:26:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:27:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:27:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:27:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:27:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:27:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:27:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:27:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1534 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...logs were reviewed. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:27:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:27:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:27:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:27:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:28:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1338 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...rantined and the filter', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:28:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:28:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:28:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:28:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:28:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:28:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:28:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:28:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:28:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:28:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:28:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:28:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:29:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:29:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:29:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:29:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:29:38 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 528 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...                       ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:29:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:29:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:29:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:29:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:29:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:29:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:10 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 644 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...                       ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:30:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:30 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 670 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...                       ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:30:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:30:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:30:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:30:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:30:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1275 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...urned to 68% RH for the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:30:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:31:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:31:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:31:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:31:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:31:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:31:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:32:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:32:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:32:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:32:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:32:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:32:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:32:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:32:30 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1288 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...ity decreased to 33% RH', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:32:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:32:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:32:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1262 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...om humidity rose to 68%', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:33:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:30 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1519 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...onmental logs reviewed.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:33:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","reportParamGridSearch time --- 28.532266501585642 minutes ---\n"]}],"source":["!python app/reportParamGridSearch.py --model_id openai-community/gpt2-xl --prompt_method B C --max_workers 12 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 21 --end_idx 22  --temperature 0.7 0.9 1.1 1.3 --top_p 0.4 0.5 0.7 0.8 --top_k 30 40 50 60 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","source":["## meta-llama/Llama-3.2-1B\n","It allocates 7.2 GB in RAM"],"metadata":{"id":"FOyyMTHXHNsm"}},{"cell_type":"code","source":["from huggingface_hub import login\n","login(token=userdata.get('HF_TOKEN'))  # insert your Hugging Face token here\n","\n","!python app/reportParamGridSearch.py --model_id meta-llama/Llama-3.2-1B --non-threaded --prompt_method B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 21 --end_idx 22  --temperature 0.5 0.7 1.0 --top_p 0.8 1 --top_k 50 --max_new_tokens 300 --do_sample True"],"metadata":{"id":"Y4e6CcfAHN_z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JozIHMbjZGtI"},"source":["# Several models run in parallel\n","In linux command with the & we can run several programs in parallel\n","**Careful it fills GPU RAM quickly if models are > 2 B parameters**\n","\n","**NOTA:** Better use the Colab L4 GPU for charging two models in parallel. The L4 GPU size is 25 GB, so it could admit up to 5B parameters more or less, i.e.:\n","\n","- two models of at most 2.5 B parameters.\n","- Three models of 1 B parameters each\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCGd0oBQZbTL"},"outputs":[],"source":["!python app/reportParamGridSearch.py --model_id microsoft/phi-2 --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True & python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM3-3B --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0P1LHHOXp0d"},"outputs":[],"source":["# KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b15381868984f48a958540b50f785c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f06d02f23654471b0c1377b8d7f3203","IPY_MODEL_73ddf9e17e6a4cfe85cb47a2625303e8","IPY_MODEL_b5157bdd857148fb9d5c6488691a81b8"],"layout":"IPY_MODEL_c260a953e8564a31bd2d64fbb3445f84"}},"3a0a7902dddf42028ff6c54a3c2af8d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c7b05ef240b4737948d1a48b423624e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f06d02f23654471b0c1377b8d7f3203":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e33742605ca4f12863776e771c9dc77","placeholder":"​","style":"IPY_MODEL_e54018a2d58a4c1e95410f5c9bc419e1","value":"generation_config.json: 100%"}},"73ddf9e17e6a4cfe85cb47a2625303e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c7b05ef240b4737948d1a48b423624e","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a0a7902dddf42028ff6c54a3c2af8d2","value":124}},"9e33742605ca4f12863776e771c9dc77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa7496c29bef49fc8a29d3102800ce4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5157bdd857148fb9d5c6488691a81b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b91e110c59a2464284d4d00d3204eca4","placeholder":"​","style":"IPY_MODEL_aa7496c29bef49fc8a29d3102800ce4f","value":" 124/124 [00:00&lt;00:00, 12.1kB/s]"}},"b91e110c59a2464284d4d00d3204eca4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c260a953e8564a31bd2d64fbb3445f84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e54018a2d58a4c1e95410f5c9bc419e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/ml_tricks/colab/colab_connect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"ubgiAABGmadO"},"source":["# This Colab notebook is ment to be executed for using the GPU units in Colab\n","The idea is to remotely execute the repo files (classes, main, etc)"]},{"cell_type":"markdown","metadata":{"id":"Kc3SGLPSX_7E"},"source":["# 0: Before starting, verifiy that: **After pulling the repo, you have copied the .env file into the Github repo !**"]},{"cell_type":"markdown","metadata":{"id":"r0sG1QyylCkW"},"source":["# 1 Mount Google Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21919,"status":"ok","timestamp":1755429124456,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"w69hWB_qkwLI","outputId":"23641420-074e-4127-f303-84d45b62d893"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"FPK232yxlHD1"},"source":["# 2 config Git"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":243,"status":"ok","timestamp":1755429124706,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"GradLm7NkxKN"},"outputs":[],"source":["!git config --global user.name \"zbotta\"\n","!git config --global user.email \"zbotta@proton.me\""]},{"cell_type":"markdown","metadata":{"id":"tQRJVtrHynSw"},"source":["## RUN THIS CELL ONLY ONCE!\n","To clone the Github repo"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1016,"status":"ok","timestamp":1755429125748,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"E1otG_64l1Qu"},"outputs":[],"source":["from google.colab import userdata\n","github_token = userdata.get('zbotta_token')\n","\n","token = github_token\n","username = \"zbotta\"\n","repo = 'reportingAgent'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1614,"status":"ok","timestamp":1755168347891,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"PQgo_qSExuXm","outputId":"3e7d64bc-ce9a-4f1b-fa22-fb0dc7b6e70f"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path '/content/drive/MyDrive/GitHub/reportingAgent' already exists and is not an empty directory.\n"]}],"source":["!git clone https://{username}:{github_token}@github.com/{username}/{repo}.git /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2277,"status":"ok","timestamp":1755429127991,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"ze8qK-7nmx_O","outputId":"ecba69e1-fda1-499d-f190-bf1df4882770"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVgHJTXrHLIX"},"outputs":[],"source":["#!git remote set-url origin https://{username}:{github_token}@github.com/{username}/{repo}.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Rwp4A70m_2U"},"outputs":[],"source":["#!git remote get-url origin"]},{"cell_type":"markdown","metadata":{"id":"q0fNz_r7m4UG"},"source":["# Use git commands"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1046,"status":"ok","timestamp":1755258113943,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"ybZIZbg2-qhU","outputId":"7e620e07-490f-4492-c0fa-43a3a502dcaa"},"outputs":[{"name":"stdout","output_type":"stream","text":["M\tPoC/reportAgent-remote.ipynb\n","Already on 'dev'\n","Your branch is up to date with 'origin/dev'.\n"]}],"source":["!git fetch\n","!git checkout dev"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2457,"status":"ok","timestamp":1755429342204,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"g9D7zP_oRsaN","outputId":"e1242faf-7504-4b26-eeec-8f878d42d36a"},"outputs":[{"output_type":"stream","name":"stdout","text":["remote: Enumerating objects: 5, done.\u001b[K\n","remote: Counting objects:  20% (1/5)\u001b[K\rremote: Counting objects:  40% (2/5)\u001b[K\rremote: Counting objects:  60% (3/5)\u001b[K\rremote: Counting objects:  80% (4/5)\u001b[K\rremote: Counting objects: 100% (5/5)\u001b[K\rremote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n","remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\n","Unpacking objects: 100% (3/3), 330 bytes | 0 bytes/s, done.\n","From https://github.com/zbotta/reportingAgent\n","   0a82014..2b89d7f  dev        -> origin/dev\n","Updating 0a82014..2b89d7f\n","Fast-forward\n"," requirements.txt | 3 \u001b[32m+++\u001b[m\n"," 1 file changed, 3 insertions(+)\n"]}],"source":["!git pull"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359,"status":"ok","timestamp":1755429279249,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"ooqu1-hlm7Af","outputId":"3bd5f418-0e7b-4ebb-88c4-56a428b362e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["On branch dev\n","Your branch is up to date with 'origin/dev'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   PoC/PoC_03_ROUGE and BLEU scores.ipynb\u001b[m\n","\t\u001b[31mmodified:   PoC/reportAgent-remote.ipynb\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}],"source":["!git status"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1754480985734,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"BsN8-zMZ8Z-n","outputId":"3e5c50cb-1172-4f17-da0f-f1e10830014c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Everything up-to-date\n"]}],"source":["!git push origin dev\n"]},{"cell_type":"markdown","metadata":{"id":"nXz98pxKo8oH"},"source":["# Using project scripts\n","\n","Reference : [Importing python library from Drive](https://colab.research.google.com/drive/12qC2abKAIAlUM_jNAokGlooKY-idbSxi#scrollTo=prUMpfLaB-D7)"]},{"cell_type":"markdown","metadata":{"id":"mvLEUb7VzVD0"},"source":["## Install project dependencies\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52,"status":"ok","timestamp":1755429348812,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"S6sPDJjVSHQf","outputId":"4ac2bd9f-2a53-4ed0-81db-137fa844665e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-lZSS9qFzSLk","outputId":"28c561a8-cb49-48af-87ee-5e7f34438296"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting absl-py==2.3.1 (from -r requirements.txt (line 1))\n","  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: aiohappyeyeballs==2.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.6.1)\n","Collecting aiohttp==3.12.14 (from -r requirements.txt (line 3))\n","  Downloading aiohttp-3.12.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n","Requirement already satisfied: aiosignal==1.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.4.0)\n","Collecting airportsdata==20250706 (from -r requirements.txt (line 5))\n","  Downloading airportsdata-20250706-py3-none-any.whl.metadata (9.1 kB)\n","Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.7.0)\n","Collecting anyio==4.9.0 (from -r requirements.txt (line 7))\n","  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: attrs==25.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (25.3.0)\n","Collecting bert-score==0.3.13 (from -r requirements.txt (line 9))\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Collecting certifi==2025.7.14 (from -r requirements.txt (line 10))\n","  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n","Collecting charset-normalizer==3.4.2 (from -r requirements.txt (line 11))\n","  Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n","Requirement already satisfied: click==8.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (8.2.1)\n","Requirement already satisfied: cloudpickle==3.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (3.1.1)\n","Collecting colorama==0.4.6 (from -r requirements.txt (line 14))\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Collecting contourpy==1.3.2 (from -r requirements.txt (line 15))\n","  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n","Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (0.12.1)\n","Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (4.0.0)\n","Requirement already satisfied: dill==0.3.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (0.3.8)\n","Collecting diskcache==5.6.3 (from -r requirements.txt (line 19))\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (1.9.0)\n","Requirement already satisfied: docstring_parser==0.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (0.17.0)\n","Collecting dotenv==0.9.9 (from -r requirements.txt (line 22))\n","  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n","Requirement already satisfied: et_xmlfile==2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (2.0.0)\n","Collecting evaluate==0.4.5 (from -r requirements.txt (line 24))\n","  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: fastapi==0.116.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (0.116.1)\n","Requirement already satisfied: filelock==3.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (3.18.0)\n","Requirement already satisfied: fonttools==4.59.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (4.59.0)\n","Requirement already satisfied: frozenlist==1.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (1.7.0)\n","Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 29)) (2025.3.0)\n","Collecting genson==1.3.0 (from -r requirements.txt (line 30))\n","  Downloading genson-1.3.0-py3-none-any.whl.metadata (28 kB)\n","Collecting groq==0.26.0 (from -r requirements.txt (line 31))\n","  Downloading groq-0.26.0-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: h11==0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (0.16.0)\n","Requirement already satisfied: httpcore==1.0.9 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (1.0.9)\n","Requirement already satisfied: httpx==0.28.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (0.28.1)\n","Collecting huggingface-hub==0.33.4 (from -r requirements.txt (line 35))\n","  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 36)) (3.10)\n","Requirement already satisfied: iniconfig==2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (2.1.0)\n","Collecting instructor==1.10.0 (from -r requirements.txt (line 38))\n","  Downloading instructor-1.10.0-py3-none-any.whl.metadata (11 kB)\n","Collecting interegular==0.3.3 (from -r requirements.txt (line 39))\n","  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n","Collecting iso3166==2.1.1 (from -r requirements.txt (line 40))\n","  Downloading iso3166-2.1.1-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: Jinja2==3.1.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (3.1.6)\n","Requirement already satisfied: jiter==0.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (0.10.0)\n","Requirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 43)) (1.5.1)\n","Collecting jsonpath-ng==1.7.0 (from -r requirements.txt (line 44))\n","  Downloading jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: jsonschema==4.25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 45)) (4.25.0)\n","Requirement already satisfied: jsonschema-specifications==2025.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 46)) (2025.4.1)\n","Collecting kiwisolver==1.4.8 (from -r requirements.txt (line 47))\n","  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n","Collecting lark==1.2.2 (from -r requirements.txt (line 48))\n","  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n","Collecting markdown-it-py==3.0.0 (from -r requirements.txt (line 49))\n","  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 50)) (3.0.2)\n","Collecting matplotlib==3.10.3 (from -r requirements.txt (line 51))\n","  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 52)) (0.1.2)\n","Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 53)) (1.3.0)\n","Collecting multidict==6.6.3 (from -r requirements.txt (line 54))\n","  Downloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n","Requirement already satisfied: multiprocess==0.70.16 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 55)) (0.70.16)\n","Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 56)) (1.6.0)\n","Requirement already satisfied: networkx==3.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 57)) (3.5)\n","Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 58)) (3.9.1)\n","Collecting numpy==2.3.1 (from -r requirements.txt (line 59))\n","  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting openai==1.97.1 (from -r requirements.txt (line 60))\n","  Downloading openai-1.97.1-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: openpyxl==3.1.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 61)) (3.1.5)\n","Collecting outlines==1.1.1 (from -r requirements.txt (line 62))\n","  Downloading outlines-1.1.1-py3-none-any.whl.metadata (27 kB)\n","Collecting outlines_core==0.1.26 (from -r requirements.txt (line 63))\n","  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: packaging==25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 64)) (25.0)\n","Collecting pandas==2.3.1 (from -r requirements.txt (line 65))\n","  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow==11.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 66)) (11.3.0)\n","Requirement already satisfied: pluggy==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 67)) (1.6.0)\n","Requirement already satisfied: ply==3.11 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 68)) (3.11)\n","Requirement already satisfied: propcache==0.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 69)) (0.3.2)\n","Collecting pyarrow==21.0.0 (from -r requirements.txt (line 70))\n","  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Requirement already satisfied: pydantic==2.11.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 71)) (2.11.7)\n","Requirement already satisfied: pydantic_core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 72)) (2.33.2)\n","Requirement already satisfied: Pygments==2.19.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 73)) (2.19.2)\n","Requirement already satisfied: pyparsing==3.2.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 74)) (3.2.3)\n","Requirement already satisfied: pytest==8.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 75)) (8.4.1)\n","Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 76)) (2.9.0.post0)\n","Collecting python-dotenv==1.1.1 (from -r requirements.txt (line 77))\n","  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: pytz==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 78)) (2025.2)\n","Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 79)) (6.0.2)\n","Requirement already satisfied: referencing==0.36.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 80)) (0.36.2)\n","Requirement already satisfied: regex==2024.11.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 81)) (2024.11.6)\n","Collecting requests==2.32.4 (from -r requirements.txt (line 82))\n","  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n","Collecting rich==14.0.0 (from -r requirements.txt (line 83))\n","  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n","Collecting rouge_score==0.1.2 (from -r requirements.txt (line 84))\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting rpds-py==0.26.0 (from -r requirements.txt (line 85))\n","  Downloading rpds_py-0.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n","Collecting safetensors==0.5.3 (from -r requirements.txt (line 86))\n","  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Collecting scikit-learn==1.7.1 (from -r requirements.txt (line 87))\n","  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Collecting scipy==1.16.0 (from -r requirements.txt (line 88))\n","  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentence-transformers==5.0.0 (from -r requirements.txt (line 89))\n","  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 90)) (1.5.4)\n","Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 91)) (1.17.0)\n","Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 92)) (1.3.1)\n","Requirement already satisfied: starlette==0.47.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 93)) (0.47.2)\n","Collecting sympy==1.14.0 (from -r requirements.txt (line 94))\n","  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: tenacity==9.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 95)) (9.1.2)\n","Requirement already satisfied: threadpoolctl==3.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 96)) (3.6.0)\n","Collecting tokenizers==0.21.2 (from -r requirements.txt (line 97))\n","  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting torch==2.7.1 (from -r requirements.txt (line 98))\n","  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n","Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 99)) (4.67.1)\n","Collecting transformers==4.53.3 (from -r requirements.txt (line 100))\n","  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer==0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 101)) (0.16.0)\n","Requirement already satisfied: typing-inspection==0.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 102)) (0.4.1)\n","Requirement already satisfied: typing_extensions==4.14.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 103)) (4.14.1)\n","Requirement already satisfied: tzdata==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 104)) (2025.2)\n","Requirement already satisfied: urllib3==2.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 105)) (2.5.0)\n","Requirement already satisfied: xxhash==3.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 106)) (3.5.0)\n","Requirement already satisfied: yarl==1.20.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 107)) (1.20.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.33.4->-r requirements.txt (line 35)) (1.1.7)\n","Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.3.1 (from torch==2.7.1->-r requirements.txt (line 98))\n","  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch==2.7.1->-r requirements.txt (line 98)) (75.2.0)\n","Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiohttp-3.12.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading airportsdata-20250706-py3-none-any.whl (912 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n","Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading genson-1.3.0-py3-none-any.whl (21 kB)\n","Downloading groq-0.26.0-py3-none-any.whl (129 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.6/129.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading instructor-1.10.0-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.5/119.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n","Downloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n","Downloading jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\n","Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.6/246.6 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.97.1-py3-none-any.whl (764 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.4/764.4 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading outlines-1.1.1-py3-none-any.whl (100 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n","Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rpds_py-0.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.9/383.9 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.53.3-py3-none-any.whl (10.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=2376902ea6c04fba3366f8f52dc95227f2bcb24a78a4a607e4d872847123c79f\n","  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n","Successfully built rouge_score\n","Installing collected packages: nvidia-cusparselt-cu12, genson, triton, sympy, safetensors, rpds-py, python-dotenv, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, multidict, markdown-it-py, lark, kiwisolver, jsonpath-ng, iso3166, interegular, diskcache, colorama, charset-normalizer, certifi, anyio, airportsdata, absl-py, scipy, rouge_score, rich, requests, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, dotenv, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, huggingface-hub, aiohttp, torch, tokenizers, openai, groq, transformers, outlines_core, instructor, sentence-transformers, outlines, evaluate, bert-score\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n","    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":134692,"status":"ok","timestamp":1755379938772,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"7jbcWB1PGsoj","outputId":"1dfb012e-e394-4f36-eb47-3b88027869eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.1)\n","Collecting torch\n","  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Collecting torchvision\n","  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n","  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n","  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n","Collecting nvidia-nccl-cu12==2.27.3 (from torch)\n","  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n","  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==3.4.0 (from torch)\n","  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (75.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.3.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m143.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n","    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.3.1\n","    Uninstalling triton-3.3.1:\n","      Successfully uninstalled triton-3.3.1\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.6.77\n","    Uninstalling nvidia-nvtx-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n","    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.26.2\n","    Uninstalling nvidia-nccl-cu12-2.26.2:\n","      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufile-cu12\n","    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n","    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n","      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n","    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.7.1\n","    Uninstalling torch-2.7.1:\n","      Successfully uninstalled torch-2.7.1\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.21.0+cu124\n","    Uninstalling torchvision-0.21.0+cu124:\n","      Successfully uninstalled torchvision-0.21.0+cu124\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\n","cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.8.0 torchvision-0.23.0 triton-3.4.0\n"]}],"source":["!pip install --upgrade torch torchvision"]},{"cell_type":"markdown","metadata":{"id":"-2wXabjgzszt"},"source":["## Import a python script from project"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82,"status":"ok","timestamp":1755379938862,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"a8WjxROAzwUT","outputId":"3114e3f1-80bc-487b-acf8-e717bbffd13b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1451,"status":"ok","timestamp":1755383259261,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"-oZlOWBODm4M","outputId":"58aa21e3-0d21-4e09-dd65-75084bed829f"},"outputs":[{"output_type":"stream","name":"stdout","text":["remote: Enumerating objects: 9, done.\u001b[K\n","remote: Counting objects:  11% (1/9)\u001b[K\rremote: Counting objects:  22% (2/9)\u001b[K\rremote: Counting objects:  33% (3/9)\u001b[K\rremote: Counting objects:  44% (4/9)\u001b[K\rremote: Counting objects:  55% (5/9)\u001b[K\rremote: Counting objects:  66% (6/9)\u001b[K\rremote: Counting objects:  77% (7/9)\u001b[K\rremote: Counting objects:  88% (8/9)\u001b[K\rremote: Counting objects: 100% (9/9)\u001b[K\rremote: Counting objects: 100% (9/9), done.\u001b[K\n","remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n","remote: Total 5 (delta 4), reused 5 (delta 4), pack-reused 0 (from 0)\u001b[K\n","Unpacking objects:  20% (1/5)\rUnpacking objects:  40% (2/5)\rUnpacking objects:  60% (3/5)\rUnpacking objects:  80% (4/5)\rUnpacking objects: 100% (5/5)\rUnpacking objects: 100% (5/5), 468 bytes | 13.00 KiB/s, done.\n","From https://github.com/zbotta/reportingAgent\n","   c55ab3b..a4c0d00  dev        -> origin/dev\n","Updating c55ab3b..a4c0d00\n","Fast-forward\n"," app/mods/dataHandler.py | 3 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n"," 1 file changed, 2 insertions(+), 1 deletion(-)\n"]}],"source":["!git pull"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":2312,"status":"ok","timestamp":1755127732579,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"4r_Hif3sXJDN","outputId":"860f44ae-05d9-4a41-a2d6-6d8c1d3d8714"},"outputs":[{"name":"stdout","output_type":"stream","text":["08/13/2025 12:15:59 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:15:59 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:17:05 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:17:07 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:17:08 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:17:08 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:17:09 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:17:10 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:17:10 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpq5628o7m\n","08/13/2025 12:17:10 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpq5628o7m/_remote_module_non_scriptable.py\n","08/13/2025 12:17:17 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:17:17 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:17:17 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:17:20 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:17:20 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:17:21 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:17:21 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:17:21 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:18:33 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:18:35 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:18:35 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:18:35 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:18:36 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:18:37 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:18:37 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpwkv8wcnc\n","08/13/2025 12:18:37 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpwkv8wcnc/_remote_module_non_scriptable.py\n","08/13/2025 12:18:44 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:18:44 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:18:44 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:18:46 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:18:46 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:18:46 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:18:46 - huggingface_hub.file_download - INFO - Downloading 'generation_config.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/cf4ba0e4ebd236033b404876c972eac1be74b969.incomplete'\n","08/13/2025 12:18:46 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/cf4ba0e4ebd236033b404876c972eac1be74b969\n","08/13/2025 12:18:46 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:18:46 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:18:46 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:18:46 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:18:46 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:18:46 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:18:46 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:18:46 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:18:46 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:19:28 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:19:31 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:19:31 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:19:31 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:19:32 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:19:33 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:19:33 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmptn19p4br\n","08/13/2025 12:19:33 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmptn19p4br/_remote_module_non_scriptable.py\n","08/13/2025 12:19:40 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:19:40 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:19:40 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:19:41 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:19:41 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:19:41 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:19:41 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:19:41 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:19:41 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:19:41 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:19:41 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:19:41 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:19:41 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:19:41 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:19:41 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:22:11 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:22:13 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:22:13 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:22:13 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:22:14 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:22:15 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:22:15 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmp_u6jw64o\n","08/13/2025 12:22:15 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmp_u6jw64o/_remote_module_non_scriptable.py\n","08/13/2025 12:22:23 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:22:23 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:22:23 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:22:24 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:22:25 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:22:25 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:22:25 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:22:25 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:22:25 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:22:25 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:22:25 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:22:25 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:22:25 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:22:25 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/5145e0895f2fe7f1ccb3eb9da69ec74ec9c680db.incomplete'\n","08/13/2025 12:22:25 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/5145e0895f2fe7f1ccb3eb9da69ec74ec9c680db\n","08/13/2025 12:22:28 - transformers.configuration_utils - INFO - Model config PhiConfig {\n","  \"architectures\": [\n","    \"PhiForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 50256,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_size\": 2560,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 10240,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"phi\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"partial_rotary_factor\": 0.4,\n","  \"qk_layernorm\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","da48b7be36cc\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/c1148447551675ea739c440ee3e247df9f354d8f.incomplete'\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/c1148447551675ea739c440ee3e247df9f354d8f\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Downloading 'added_tokens.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7debb4784a7d53328d4d021fc46314bec4af3833.incomplete'\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7debb4784a7d53328d4d021fc46314bec4af3833\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/0204ed10c186a4c7c68f55dff8f26087a45898d6.incomplete'\n","08/13/2025 12:22:28 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/0204ed10c186a4c7c68f55dff8f26087a45898d6\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/vocab.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/merges.txt\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/added_tokens.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/special_tokens_map.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer_config.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","08/13/2025 12:22:28 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/011968cc02a5cca9fd46ecd994fc961e6906c0bd.incomplete'\n","08/13/2025 12:22:28 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/011968cc02a5cca9fd46ecd994fc961e6906c0bd\n","08/13/2025 12:22:28 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/config.json\n","08/13/2025 12:22:32 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors.index.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/fd9e856cfa23cd4bd5122f982feba515d23260a5.incomplete'\n","08/13/2025 12:22:32 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/fd9e856cfa23cd4bd5122f982feba515d23260a5\n","08/13/2025 12:22:32 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/model.safetensors.index.json\n","08/13/2025 12:22:33 - huggingface_hub.file_download - INFO - Downloading 'model-00002-of-00002.safetensors' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/17b98759e4b7953cbcf63ec49be7edbc9b863b57c207d84a52f5d2f5bcfcf6b4.incomplete'\n","08/13/2025 12:22:33 - huggingface_hub.file_download - INFO - Downloading 'model-00001-of-00002.safetensors' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7fbcdefa72edf7527bf5da40535b57d9f5bd3d16829b94a9d25d2b457df62e84.incomplete'\n","08/13/2025 12:23:22 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/17b98759e4b7953cbcf63ec49be7edbc9b863b57c207d84a52f5d2f5bcfcf6b4\n","08/13/2025 12:24:20 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7fbcdefa72edf7527bf5da40535b57d9f5bd3d16829b94a9d25d2b457df62e84\n","08/13/2025 12:24:20 - transformers.modeling_utils - INFO - Instantiating PhiForCausalLM model under default dtype torch.float32.\n","08/13/2025 12:24:20 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:24:44 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing PhiForCausalLM.\n","\n","08/13/2025 12:24:44 - transformers.modeling_utils - INFO - All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n","08/13/2025 12:24:45 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:24:45 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:24:46 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:24:46 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:24:46 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:24:47 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:24:47 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:24:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:24:55 - evaluate.utils.file_utils - INFO - https://huggingface.co/spaces/evaluate-metric/bertscore/resolve/main/bertscore.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/evaluate/downloads/tmp_hwmv5h2\n","08/13/2025 12:24:55 - evaluate.utils.file_utils - INFO - storing https://huggingface.co/spaces/evaluate-metric/bertscore/resolve/main/bertscore.py in cache at /root/.cache/huggingface/evaluate/downloads/2fb1405ec250844307c59b3bad240bb98cf25006711d08add1afcaf365fa5899.a4fc40aee04c356ddb89f55c1f36393b16fa831e9b5a9daa80e5e77cac867d1c.py\n","08/13/2025 12:24:55 - evaluate.utils.file_utils - INFO - creating metadata file for /root/.cache/huggingface/evaluate/downloads/2fb1405ec250844307c59b3bad240bb98cf25006711d08add1afcaf365fa5899.a4fc40aee04c356ddb89f55c1f36393b16fa831e9b5a9daa80e5e77cac867d1c.py\n","08/13/2025 12:24:55 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/e5c73d8a50df1f56fb5b0b8002d7cf4010afdccb.incomplete'\n","08/13/2025 12:24:55 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/e5c73d8a50df1f56fb5b0b8002d7cf4010afdccb\n","08/13/2025 12:24:56 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/150367d8744161cd17b3f6462a14f3a9648752da.incomplete'\n","08/13/2025 12:24:56 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/150367d8744161cd17b3f6462a14f3a9648752da\n","08/13/2025 12:24:56 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:24:56 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:24:56 - huggingface_hub.file_download - INFO - Downloading 'vocab.txt' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.incomplete'\n","08/13/2025 12:24:57 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938\n","08/13/2025 12:24:57 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/949a6f013d67eb8a5b4b5b46026217b888021b88.incomplete'\n","08/13/2025 12:24:57 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/949a6f013d67eb8a5b4b5b46026217b888021b88\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:24:57 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:24:57 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:24:58 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:24:58 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:24:59 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/5e3f1108e3cb34ee048634875d8482665b65ac713291a7e32396fb18f6ff0063.incomplete'\n","08/13/2025 12:25:06 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/5e3f1108e3cb34ee048634875d8482665b65ac713291a7e32396fb18f6ff0063\n","08/13/2025 12:25:06 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:06 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:06 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:06 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:06 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './modules.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/952a9b81c0bfd99800fabf352f69c7ccd46c5e43.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/952a9b81c0bfd99800fabf352f69c7ccd46c5e43\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './config_sentence_transformers.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fd1b291129c607e5d49799f87cb219b27f98acdf.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fd1b291129c607e5d49799f87cb219b27f98acdf\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './README.md' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/58d4a9a45664eb9e12de9549c548c09b6134c17f.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/58d4a9a45664eb9e12de9549c548c09b6134c17f\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './sentence_bert_config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/59d594003bf59880a884c574bf88ef7555bb0202.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/59d594003bf59880a884c574bf88ef7555bb0202\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/72b987fd805cfa2b58c4c8c952b274a11bfd5a00.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/72b987fd805cfa2b58c4c8c952b274a11bfd5a00\n","08/13/2025 12:25:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/53aa51172d142c89d9012cce15ae4d6cc0ca6895895114379cacb4fab128d9db.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/53aa51172d142c89d9012cce15ae4d6cc0ca6895895114379cacb4fab128d9db\n","08/13/2025 12:25:09 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:09 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:09 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/c79f2b6a0cea6f4b564fed1938984bace9d30ff0.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/c79f2b6a0cea6f4b564fed1938984bace9d30ff0\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'vocab.txt' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/cb202bfe2e3c98645018a6d12f182a434c9d3e02.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/cb202bfe2e3c98645018a6d12f182a434c9d3e02\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Downloading '1_Pooling/config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/d1514c3162bbe87b343f565fadc62e6c06f04f03.incomplete'\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/d1514c3162bbe87b343f565fadc62e6c06f04f03\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/88bc4f74b33a2073abc9a66cb532b889448ac3ed.incomplete'\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/88bc4f74b33a2073abc9a66cb532b889448ac3ed\n","08/13/2025 12:25:10 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:10 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/821d1aa69520101d6e0737f78a042ae25b19e5cb9160701909d10434f4aeb0ae.incomplete'\n","08/13/2025 12:25:12 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/821d1aa69520101d6e0737f78a042ae25b19e5cb9160701909d10434f4aeb0ae\n","08/13/2025 12:25:12 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:12 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:12 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/a2435fedfac32b9ad70f052d4f84007730cd3109.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/a2435fedfac32b9ad70f052d4f84007730cd3109\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'vocab.txt' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/688882a79f44442ddc1f60d70334a7ff5df0fb47.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/688882a79f44442ddc1f60d70334a7ff5df0fb47\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/7520992f25914d962f0e2fd0e0566fc33d19ec59.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/7520992f25914d962f0e2fd0e0566fc33d19ec59\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading './README.md' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/4783b64ce66b94c7c387672de541d10678980574.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/4783b64ce66b94c7c387672de541d10678980574\n","08/13/2025 12:25:13 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:14 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:15 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:15 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:22 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:22 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:23 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:24 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:24 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:24 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:24 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:26 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:26 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:26 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:26 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:34 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:34 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:34 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:34 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:34 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:35 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:35 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:35 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:35 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:35 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:36 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:36 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:36 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:36 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:36 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:36 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:36 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:36 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:45 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:45 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:45 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:45 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:45 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:45 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:45 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:47 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:25:47 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:55 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:55 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:55 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:55 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:55 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:56 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:56 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:56 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:56 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:56 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:57 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:57 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:57 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:57 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:57 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:57 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:57 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:58 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:58 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:06 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:06 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:06 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:26:06 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:26:06 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:26:06 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:26:06 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:08 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:26:08 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:26:08 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:26:08 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:26:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:15 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:26:16 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:26:16 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:26:16 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:26:16 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:18 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:26:18 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:26:18 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:26:18 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:26:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:26:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:25 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:26 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:26:26 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:26:26 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:26:26 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:26:26 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:26:28 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:26:28 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:28 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:26:28 - mods.dataHandler - WARNING - Folder does not exist, creating new folder in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench\n","08/13/2025 12:26:28 - mods.dataHandler - INFO - Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench/tb-experiment-13-082025 12-26-28.xlsx\n","08/13/2025 12:26:28 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:26:28 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:39:31 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:39:33 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:39:34 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:39:34 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:39:34 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:39:35 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:39:35 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmp59ngi2rd\n","08/13/2025 12:39:35 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmp59ngi2rd/_remote_module_non_scriptable.py\n","08/13/2025 12:39:43 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:39:43 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:39:43 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:39:44 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:39:44 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:39:44 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:39:45 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:39:45 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:39:45 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:39:45 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:39:45 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:39:45 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:39:45 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/vocab.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/merges.txt\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/added_tokens.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/special_tokens_map.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer_config.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","08/13/2025 12:39:45 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/config.json\n","08/13/2025 12:39:45 - transformers.configuration_utils - INFO - Model config PhiConfig {\n","  \"architectures\": [\n","    \"PhiForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 50256,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_size\": 2560,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 10240,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"phi\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"partial_rotary_factor\": 0.4,\n","  \"qk_layernorm\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","08/13/2025 12:39:47 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/model.safetensors.index.json\n","08/13/2025 12:39:47 - transformers.modeling_utils - INFO - Instantiating PhiForCausalLM model under default dtype torch.float32.\n","08/13/2025 12:39:47 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:11 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing PhiForCausalLM.\n","\n","08/13/2025 12:40:11 - transformers.modeling_utils - INFO - All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:12 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:21 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:21 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:23 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:23 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:23 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:23 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:23 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:24 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:24 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:24 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:24 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:24 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:25 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:25 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:25 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:25 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:25 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:25 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:25 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:25 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:34 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:34 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:34 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:34 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:34 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:35 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:35 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:35 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:35 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:35 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:36 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:36 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:36 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:36 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:36 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:36 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:36 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:36 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:44 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:45 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:45 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:45 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:45 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:47 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:47 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:47 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:47 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:47 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:47 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:55 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:55 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:55 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:55 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:55 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:56 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:56 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:56 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:56 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:56 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:57 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:57 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:57 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:57 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:57 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:57 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:58 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:05 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:05 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:05 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:05 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:05 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:06 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:06 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:06 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:06 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:06 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:07 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:07 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:07 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:07 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:07 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:41:08 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:41:08 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:41:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:15 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:16 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:16 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:16 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:16 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:18 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:18 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:41:18 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:41:18 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:41:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:26 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:26 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:26 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:26 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:26 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:27 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:27 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:27 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:27 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:27 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:28 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:28 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:28 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:28 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:28 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:28 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:28 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:41:28 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:41:28 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:41:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:36 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:36 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:36 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:36 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:36 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:37 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:37 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:37 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:37 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:37 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:38 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:38 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:38 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:38 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:38 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:38 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:39 - mods.dataHandler - INFO - Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench/tb-experiment-13-082025 12-41-39.xlsx\n","08/13/2025 12:41:39 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:41:39 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:44:13 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:44:15 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:44:16 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:44:16 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:44:16 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:44:18 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:44:18 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmp92hg4lkh\n","08/13/2025 12:44:18 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmp92hg4lkh/_remote_module_non_scriptable.py\n","08/13/2025 12:44:26 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:44:26 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:44:26 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:44:27 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:44:27 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 13:26:51 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 13:26:53 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 13:26:53 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:26:53 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:26:54 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 13:26:55 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 13:26:55 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpehk321xo\n","08/13/2025 13:26:55 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpehk321xo/_remote_module_non_scriptable.py\n","08/13/2025 13:27:02 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 13:27:02 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:27:02 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:27:03 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 13:27:03 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 13:28:55 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 13:28:57 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 13:28:57 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:28:57 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:28:58 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 13:29:00 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 13:29:00 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpc6veeyyt\n","08/13/2025 13:29:00 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpc6veeyyt/_remote_module_non_scriptable.py\n","08/13/2025 13:29:06 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 13:29:06 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:29:06 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:29:07 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 13:29:07 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 13:29:08 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 13:29:08 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 13:29:08 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:08 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 13:29:08 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 13:29:08 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 13:29:08 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 13:29:08 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 13:29:08 - transformers.configuration_utils - INFO - Model config PhiConfig {\n","  \"architectures\": [\n","    \"PhiForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 50256,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_size\": 2560,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 10240,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"phi\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"partial_rotary_factor\": 0.4,\n","  \"qk_layernorm\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","t /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/added_tokens.json\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/special_tokens_map.json\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer_config.json\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","08/13/2025 13:29:08 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/config.json\n","08/13/2025 13:29:10 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/model.safetensors.index.json\n","08/13/2025 13:29:10 - transformers.modeling_utils - INFO - Instantiating PhiForCausalLM model under default dtype torch.float32.\n","08/13/2025 13:29:10 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:40 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing PhiForCausalLM.\n","\n","08/13/2025 13:29:40 - transformers.modeling_utils - INFO - All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:40 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 0.5, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/13/2025 13:29:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 13:29:40 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 13:29:40 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n"]}],"source":["!cat app/logs/logfile.log"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i79nllcxdMwy"},"outputs":[],"source":["!python projectSetup.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205,"referenced_widgets":["0b15381868984f48a958540b50f785c6","6f06d02f23654471b0c1377b8d7f3203","73ddf9e17e6a4cfe85cb47a2625303e8","b5157bdd857148fb9d5c6488691a81b8","c260a953e8564a31bd2d64fbb3445f84","9e33742605ca4f12863776e771c9dc77","e54018a2d58a4c1e95410f5c9bc419e1","6c7b05ef240b4737948d1a48b423624e","3a0a7902dddf42028ff6c54a3c2af8d2","b91e110c59a2464284d4d00d3204eca4","aa7496c29bef49fc8a29d3102800ce4f"]},"executionInfo":{"elapsed":3162,"status":"ok","timestamp":1755087390807,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"Srd-wh7zg8Qt","outputId":"f4af760b-37a5-4868-e580-65d4c763e782"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b15381868984f48a958540b50f785c6","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=gpt2\n"]}],"source":["import sys, os\n","import torch\n","from pathlib import Path\n","sys.path.append(os.getcwd())\n","sys.path.append(os.getcwd() + '/app')\n","\n","from app.mods.promptGenerator import PromptGenerator\n","from app.mods.modelLoader import ModelLoader\n","from app.mods.reportGenerator import ReportGenerator\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","torch_dtype = torch.float32 if torch.cuda.is_available() else torch.float32\n","\n","ml = ModelLoader(model_id=\"gpt2\", device=device, torch_dtype=torch_dtype)"]},{"cell_type":"markdown","metadata":{"id":"rfGNcNvTUhdj"},"source":["## microsoft/phi-2\n","It allocates 12 GB in RAM, the extra depends on the number of workers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P57w5lE7nDyX","executionInfo":{"status":"ok","timestamp":1755383418787,"user_tz":-120,"elapsed":51076,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"2b6cab26-2100-462b-8704-216dcecd61fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-08-16 22:29:33.886532: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755383373.908027   20029 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755383373.915124   20029 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755383373.933465   20029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755383373.933490   20029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755383373.933493   20029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755383373.933495   20029 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-08-16 22:29:33.938202: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['microsoft/phi-2'], 'prompt_method': ['B', 'C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [1], 'end_idx': [2], 'temperature': [1.0], 'top_p': [0.9, 1.0], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","08/16/2025 22:29:48 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/16/2025 22:29:48 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/16/2025 22:29:48 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","Generation parameters: \n","{'temperature': [1.0], 'top_p': [0.9, 1.0], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.09it/s]\n","Results file is expected to have 8 rows.\n","******* Starting GRID SEARCH ***********\n","******* Starting NOT THREADED PROCESS ************\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/16/2025 22:29:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 78.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.68it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/16/2025 22:30:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.41it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/16/2025 22:30:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.75it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/16/2025 22:30:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n","    return func(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 2625, in generate\n","    result = self._sample(\n","             ^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\", line 3609, in _sample\n","    outputs = model_forward(**model_inputs, return_dict=True)\n","              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 943, in wrapper\n","    output = func(self, *args, **kwargs)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/phi/modeling_phi.py\", line 527, in forward\n","    outputs: BaseModelOutputWithPast = self.model(\n","                                       ^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 943, in wrapper\n","    output = func(self, *args, **kwargs)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/phi/modeling_phi.py\", line 415, in forward\n","    layer_outputs = decoder_layer(\n","                    ^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\", line 83, in __call__\n","    return super().__call__(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/phi/modeling_phi.py\", line 234, in forward\n","    attn_outputs, self_attn_weights = self.self_attn(\n","                                      ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/phi/modeling_phi.py\", line 163, in forward\n","    query_rot, key_rot = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n","                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/phi/modeling_phi.py\", line 64, in apply_rotary_pos_emb\n","    k_embed = (k * cos) + (rotate_half(k) * sin)\n","                           ^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/phi/modeling_phi.py\", line 38, in rotate_half\n","    return torch.cat((-x2, x1), dim=-1)\n","                      ^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_device.py\", line 103, in __torch_function__\n","    return func(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/content/drive/MyDrive/GitHub/reportingAgent/app/reportParamGridSearch.py\", line 132, in <module>\n","    main(**vars(args))\n","  File \"/content/drive/MyDrive/GitHub/reportingAgent/app/reportParamGridSearch.py\", line 85, in main\n","    tb.eval_gs_param(report_data=df_reports_filtered,\n","  File \"/content/drive/MyDrive/GitHub/reportingAgent/app/mods/testBench.py\", line 145, in eval_gs_param\n","    self.param_grid_search_on_row(row=row,\n","  File \"/content/drive/MyDrive/GitHub/reportingAgent/app/mods/testBench.py\", line 182, in param_grid_search_on_row\n","    self.generate_one_param_set(res=res,\n","  File \"/content/drive/MyDrive/GitHub/reportingAgent/app/mods/testBench.py\", line 223, in generate_one_param_set\n","    output, gen_param = report_generator.generate_report(prompt=gen_prompt, **gen_param)\n","                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/content/drive/MyDrive/GitHub/reportingAgent/app/mods/reportGenerator.py\", line 76, in generate_report\n","    output = self.model(prompt, output_type=self.output_type, **generation_args)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/outlines/models/base.py\", line 117, in __call__\n","    return Generator(self, output_type)(model_input, **inference_kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/outlines/generator.py\", line 306, in __call__\n","    return self.model.generate(\n","           ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/outlines/models/transformers.py\", line 306, in generate\n","    generated_ids = self._generate_output_seq(\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/outlines/models/transformers.py\", line 355, in _generate_output_seq\n","    output_ids = self.model.generate(\n","                 ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 119, in decorate_context\n","    with ctx_factory():\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/grad_mode.py\", line 84, in __exit__\n","    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:\n","\n","KeyboardInterrupt\n","^C\n"]}],"source":["!python app/reportParamGridSearch.py --model_id microsoft/phi-2 --non-threaded --prompt_method B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 2 --temperature 1.0 --top_p 0.9 1 --top_k 50 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","metadata":{"id":"vVTm54F1UaHq"},"source":["## HuggingFaceTB/SmolLM3-3B\n","It allocates 14 GB in RAM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DdF2u_DS5WaO","outputId":"96a0feb9-716e-4524-8829-4783eff307cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mSe han truncado las últimas 5000 líneas del flujo de salida.\u001b[0m\n","Batches: 100% 1/1 [00:00<00:00, 124.14it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.69it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.68it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.03it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.98it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.93it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.51it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.29it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.58it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.50it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.23it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 126.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.44it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.97it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 105.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.84it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.56it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.02it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.12it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.64it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.00it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.46it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.45it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.61it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.31it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.62it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.62it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.87it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.55it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.01it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.84it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.33it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.67it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.92it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.74it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.60it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.33it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.76it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.98it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.67it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.33it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.71it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.29it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.82it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.66it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.63it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.67it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.15it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.80it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.49it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.94it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.82it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.74it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.29it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.66it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.24it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.95it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.01it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.12it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.09it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.01it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.74it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.14it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.27it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.32it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 123.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.11it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.81it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.28it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.32it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.92it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.46it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.24it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.59it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.14it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.82it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 120.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.87it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 111.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.93it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.79it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.92it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.56it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.03it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.35it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.91it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.73it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.48it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.00it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.19it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.28it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.97it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.59it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.83it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.01it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.05it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.42it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.23it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.82it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.76it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.20it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.52it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.52it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.24it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.48it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.00it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.28it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.86it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.04it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.14it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.82it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.98it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.73it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.65it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.46it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.18it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.01it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.51it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.33it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.94it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.91it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.80it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.81it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.48it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.76it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.50it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.09it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 79.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.19it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.18it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.14it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.69it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.16it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.52it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 105.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.21it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.46it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.28it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.29it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.38it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.06it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.78it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.27it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.76it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.65it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.27it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.14it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.88it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.97it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.74it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.65it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.32it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 107.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.03it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.30it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.51it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.87it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.74it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.67it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.17it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.03it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.23it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.87it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.47it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.05it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.09it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.15it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.63it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.09it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.09it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.57it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.48it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.14it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.69it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.49it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.40it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.62it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.98it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.98it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.65it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 125.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.66it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.01it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.44it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.98it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.56it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.91it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.41it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.15it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.88it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.05it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.39it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.52it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.88it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.02it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.47it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.69it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.32it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.52it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.79it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.09it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 94.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.26it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.65it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.53it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.58it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.36it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.61it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.11it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.00it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.58it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.57it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.63it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.02it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.50it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.24it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.33it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 128.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.79it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.65it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.95it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.42it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.45it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.89it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 143.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.19it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 143.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.28it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.30it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.74it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.28it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.29it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.66it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 104.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 107.46it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.58it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.16it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 85.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.92it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.09it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.10it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.65it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.45it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.48it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.92it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.72it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.28it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.62it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.21it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.31it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.19it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.86it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.16it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.46it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.03it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.14it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.84it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.61it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.95it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.30it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.42it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.22it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.94it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.83it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.74it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.74it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.96it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.60it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.79it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.51it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.56it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.40it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.75it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.74it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.70it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.99it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.10it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.33it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.26it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.90it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.92it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.85it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.30it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.95it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.48it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.39it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 100.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.39it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.91it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.20it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.46it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.33it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.22it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.26it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.96it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.73it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 106.86it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 133.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.10it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.11it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 109.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.92it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.98it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.47it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.88it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.30it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.22it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 134.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.80it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.53it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.10it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.74it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.04it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.97it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.11it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.31it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.67it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.88it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.41it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.52it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.95it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.14it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.44it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.22it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.30it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.96it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.95it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.72it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 109.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.04it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.24it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.27it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.64it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.03it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.45it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.01it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.31it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.17it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.31it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.74it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.09it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.27it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 110.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.88it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.51it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.00it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.62it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.82it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.12it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.41it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.38it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.59it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.45it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.18it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.22it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.94it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.28it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.52it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.17it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.26it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.75it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 107.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.21it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.25it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.90it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.10it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.97it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.96it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.26it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.00it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.59it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.51it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.05it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.24it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.22it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 101.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.57it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 107.37it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.32it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.04it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.72it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.72it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.81it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.45it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.56it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.37it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.72it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.43it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.83it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.22it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.63it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 108.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.83it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.88it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.80it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.66it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.28it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.11it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.32it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.96it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.40it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.08it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.62it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.65it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.21it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.02it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.37it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.50it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.79it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.94it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.83it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.62it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.28it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 108.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.62it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.83it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.31it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.72it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.19it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.24it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.93it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.22it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.09it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.57it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.52it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.68it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.22it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.99it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.11it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.37it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.93it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.76it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 142.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.78it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.19it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.37it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.24it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.71it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.25it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.31it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.75it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.05it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.22it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.54it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.31it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.70it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.61it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.35it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.95it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.19it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.61it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.00it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.95it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.46it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.16it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 143.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.37it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.87it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.29it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.53it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.40it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.45it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.24it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.66it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.26it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.82it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.17it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.65it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.61it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 138.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.64it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.08it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.98it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.43it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.55it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.31it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.49it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.26it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.33it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 127.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.31it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.60it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.34it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.40it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.43it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.34it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.74it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.16it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.65it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.07it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.66it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.02it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.76it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.37it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.14it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.31it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.51it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.23it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.44it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.19it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.18it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.70it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.97it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.17it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.19it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.11it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.00it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.61it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.97it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.68it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.11it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.28it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.93it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.05it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.11it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.51it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.92it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.17it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.33it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.13it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.13it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.62it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.65it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.94it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.88it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.26it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 141.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.01it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.26it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.50it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 107.75it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.89it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.67it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.01it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.16it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.90it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 108.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.25it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 109.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.95it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.86it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.55it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.13it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.00it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.34it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 107.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.73it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.86it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.97it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 103.31it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.11it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.31it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.15it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.69it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.21it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.70it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.80it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.52it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.46it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.63it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.29it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.93it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.48it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.52it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.80it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.97it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.77it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.92it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.10it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.57it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.11it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.91it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.23it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.42it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.99it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.96it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 107.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.77it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.63it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.17it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.72it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.92it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.36it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.00it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.81it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 137.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.47it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.47it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.10it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.16it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.52it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.64it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.20it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.16it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.77it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.03it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.41it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 108.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.38it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.89it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.41it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.82it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.25it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.01it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.47it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.60it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.84it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.78it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.10it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 109.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.44it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.21it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.06it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.02it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.65it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.03it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.29it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.87it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.22it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.37it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.02it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.31it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.14it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.97it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.51it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.38it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.82it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.83it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.60it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.87it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.90it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 143.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.28it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.04it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.03it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.38it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.37it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.47it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.64it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.71it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.92it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.00it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.71it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 103.83it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.88it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.98it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.14it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.90it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.47it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.91it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.40it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.77it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.98it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.89it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.36it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.13it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.71it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.33it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.17it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.10it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.22it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.97it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.35it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.80it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.50it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.99it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.00it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 98.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.72it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.80it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.77it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 105.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.68it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.71it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.21it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.84it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.39it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.06it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.53it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.69it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.00it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.41it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 142.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.04it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 104.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.35it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.74it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.48it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.39it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.46it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.52it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.05it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.45it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.28it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.23it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 146.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.06it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 146.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.13it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 146.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.81it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.90it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.80it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 92.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.96it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.48it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.20it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 142.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.77it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.39it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.25it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.28it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.64it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.81it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.97it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 135.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.16it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.49it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.79it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.84it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.98it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.03it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.78it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.89it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.88it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.31it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.84it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.33it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.61it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.50it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.02it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.55it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.39it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.27it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.54it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.61it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.84it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.31it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.60it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.79it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.49it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.05it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.76it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.91it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.69it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 147.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 150.40it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 146.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.62it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.43it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.96it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.79it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.06it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.61it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.28it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 144.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.76it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 154.74it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.31it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.06it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.23it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.18it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.55it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 129.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.43it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.16it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 148.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.10it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.48it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 144.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 150.61it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.33it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.51it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.18it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.92it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.72it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.00it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.66it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.45it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.23it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.95it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.45it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.73it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.54it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.05it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.88it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.68it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.00it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.17it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.40it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.18it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.54it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.95it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.44it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.00it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.77it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.84it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.41it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.07it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.81it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.79it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.03it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.18it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.35it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.00it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.87it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.89it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.05it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.14it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.10it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.50it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.62it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.68it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.13it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.78it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.24it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.79it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.81it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.60it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.29it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.20it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.17it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.53it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.87it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.53it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.97it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.57it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.40it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.36it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.99it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.34it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.42it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.46it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.76it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 103.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.29it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.16it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.82it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.19it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.43it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.85it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.05it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.02it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.29it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.17it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.57it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.25it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.88it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.12it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.92it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.71it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.17it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.28it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.37it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.00it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.22it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.80it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.80it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.25it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.41it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.53it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.35it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.30it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.02it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.91it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.73it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.98it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 144.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.22it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.88it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 143.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.72it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.78it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.41it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.05it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 142.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.07it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.67it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 144.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.61it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.05it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.83it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.69it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.07it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.46it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.06it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.19it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.69it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.47it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.61it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.74it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.89it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.23it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.56it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.77it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.83it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.13it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.20it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.86it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.56it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.64it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.49it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.73it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.45it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.61it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 143.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.17it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.64it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 101.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.45it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.14it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.39it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.99it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.34it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.56it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.13it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 126.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.43it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.23it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.61it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.04it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.08it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.17it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.71it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.56it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.37it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.85it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.00it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 109.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.96it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.19it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.02it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.97it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.47it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.15it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.27it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.63it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.93it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.03it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.49it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.72it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.78it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.44it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.47it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.55it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.08it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.26it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.04it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 142.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.42it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.69it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.56it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.36it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.27it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.98it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.85it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.68it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.98it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.10it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.09it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.84it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.00it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.47it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.75it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.58it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.85it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.76it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.86it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.72it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.52it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.98it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.97it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.74it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.04it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.94it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.22it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.13it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.61it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.72it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.67it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 101.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 96.78it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.72it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.58it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.89it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.95it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.31it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.98it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.19it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.69it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.78it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.74it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.27it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.15it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.34it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.84it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.39it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.95it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.38it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 142.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.38it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.24it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 144.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.49it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.65it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.47it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.29it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.99it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.92it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.43it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 143.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.84it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.85it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.81it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.64it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.57it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.12it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.20it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.77it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.66it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.58it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.35it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.21it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 146.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.99it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.17it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.88it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.30it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.41it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.02it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.66it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.61it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.09it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.63it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.99it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 150.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.24it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.70it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.61it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 144.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.51it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 146.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.88it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 146.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.80it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.77it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 129.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.96it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 150.63it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 145.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.73it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 143.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.13it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.64it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 148.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.89it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 147.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.72it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.51it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 145.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.30it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 145.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.83it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 98.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.43it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 146.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.21it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.14it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.24it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.01it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.59it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.09it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.03it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.41it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.52it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.01it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.42it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.87it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.07it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.28it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.82it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.83it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.24it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.91it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.98it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.71it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.59it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.44it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.38it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.82it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.84it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.82it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.62it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 96.03it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 142.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.64it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.86it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.59it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.09it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.81it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.15it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.78it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 143.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.16it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.59it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 142.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.06it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.46it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.75it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.83it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.89it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.86it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.34it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.45it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.57it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.90it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.02it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.65it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.94it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.73it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.62it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.15it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.66it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.92it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.45it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.57it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.01it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.56it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.47it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.77it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.48it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 143.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.47it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.43it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.93it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.46it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.82it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.18it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.51it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.44it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.33it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.42it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.37it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.22it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.69it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.20it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.28it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.34it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.41it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.61it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.14it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.62it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.62it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.44it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.09it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 111.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.39it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.08it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.80it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.01it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.02it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.14it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.07it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.23it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.14it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.72it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.18it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.10it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.28it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.32it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.66it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.75it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 149.34it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.67it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.61it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.62it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.12it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 126.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.17it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.87it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 145.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 149.07it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.06it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.87it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.22it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.01it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.94it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.88it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.31it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 128.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.71it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.61it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.96it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.07it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.24it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.51it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.56it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.66it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.58it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.41it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.16it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.04it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.58it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.50it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.83it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.15it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.82it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.96it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.85it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.57it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.36it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.08it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.47it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.38it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 113.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.49it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.80it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.91it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.18it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.85it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.93it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.29it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.91it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.01it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.12it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.87it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.31it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.18it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.98it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.52it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.84it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.01it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 102.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.51it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.65it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.58it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.81it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.37it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.40it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.14it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.70it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 144.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.68it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.45it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.99it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.24it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 107.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.86it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 138.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.31it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.03it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.85it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.52it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.14it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 110.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.93it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.61it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.69it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.21it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.34it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.27it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.94it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.99it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.52it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.48it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.18it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.71it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.11it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.86it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.85it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.34it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.53it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.91it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.25it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.25it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.05it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.22it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.56it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.17it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.08it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.69it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.70it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.36it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.58it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 106.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.12it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 126.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.50it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.65it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.84it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.97it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.16it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.44it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.28it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 142.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.25it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.81it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.81it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 126.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.80it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.49it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.87it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.29it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.75it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.42it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 140.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.73it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.13it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 133.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.69it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.33it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.83it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.99it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.43it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.51it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.49it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.68it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.93it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.53it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.55it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.88it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.56it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.89it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.92it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.35it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.38it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.30it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.49it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.53it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.33it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.87it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.87it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.97it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.06it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.52it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.91it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.87it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.53it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.72it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.83it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 110.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.33it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.13it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.61it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.51it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.77it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.46it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.29it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n"]}],"source":["!python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM3-3B  --non-threaded --prompt_method A B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","metadata":{"id":"GBWVQWjZUUrm"},"source":["## GPT2-XL\n","It allocates 7.2 GB in RAM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":660640,"status":"ok","timestamp":1755277089493,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"XmhpRs4v6SBP","outputId":"ba284de6-254e-4dd1-d95f-b7bfba2f7e2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-08-15 16:47:14.579840: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755276434.600434   15000 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755276434.607361   15000 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755276434.625213   15000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755276434.625239   15000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755276434.625241   15000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755276434.625243   15000 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-08-15 16:47:14.629806: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['openai-community/gpt2-xl'], 'prompt_method': ['B', 'C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [1], 'end_idx': [2], 'temperature': [0.7, 1.0, 1.3], 'top_p': [0.3, 0.6, 0.9], 'top_k': [30, 50, 70], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","generation_config.json: 100% 124/124 [00:00<00:00, 1.04MB/s]\n","08/15/2025 16:47:34 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","08/15/2025 16:47:34 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","08/15/2025 16:47:34 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","Generation parameters: \n","{'temperature': [0.7, 1.0, 1.3], 'top_p': [0.3, 0.6, 0.9], 'top_k': [30, 50, 70], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 165kB/s]\n","vocab.json: 1.04MB [00:00, 3.65MB/s]\n","merges.txt: 456kB [00:00, 13.1MB/s]\n","tokenizer.json: 1.36MB [00:00, 14.4MB/s]\n","config.json: 100% 689/689 [00:00<00:00, 7.39MB/s]\n","model.safetensors: 100% 6.43G/6.43G [00:46<00:00, 138MB/s] \n","Results file is expected to have 108 rows.\n","******* Starting NOT THREADED PROCESS ************\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:48:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 106.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.57it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:48:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 149.99it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:48:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.23it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:48:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.25it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:48:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.94it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:48:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 146.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.75it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:48:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.49it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:48:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.64it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:49:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.45it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:49:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.52it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:49:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 16:49:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1384 [type=json_invalid, input_value='{ \"title\": \"Contaminated...loves observed during a', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 132.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.32it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:49:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.36it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:49:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 16:49:46 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1073 [type=json_invalid, input_value='{ \"title\": \"Contaminated...5, 2:40 PM. Title: Cont', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 141.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.14it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:49:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.07it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:49:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 111.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.03it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:49:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.90it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.07it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.67it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.95it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.20it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 145.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.61it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 142.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.89it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.04it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.49it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 110.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.32it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.23it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.43it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:50:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.37it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:51:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 150.11it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:51:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 115.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.81it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:51:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.73it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:51:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 141.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.26it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:51:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.75it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:51:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.67it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:51:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 155.11it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:51:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.20it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:51:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.49it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:52:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.25it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:52:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.45it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:52:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.81it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:52:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.42it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:52:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.90it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:52:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.48it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:52:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.63it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:52:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.14it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:52:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.40it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:52:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.80it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.04it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.24it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.55it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.15it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.73it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.06it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 141.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.51it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.54it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 152.59it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 142.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.77it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.59it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.25it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 143.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.92it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.09it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:53:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.23it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 143.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.54it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 141.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.42it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.38it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.62it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.89it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.45it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 150.53it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 142.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.81it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 142.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.55it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 145.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.24it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.26it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.34it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.38it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 149.02it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.02it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.16it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.45it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.30it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 151.20it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:54:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.11it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:55:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.79it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:55:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 149.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.44it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:55:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.74it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:55:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 146.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.84it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:55:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 143.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.08it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:55:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.03it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:55:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 110.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.52it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:55:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.56it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:55:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.88it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:56:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 148.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.77it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:56:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.12it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:56:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.62it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:56:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.76it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:56:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.06it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:56:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.62it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:56:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 156.42it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:56:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 152.13it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:56:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.49it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:57:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.93it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:57:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.27it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:57:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.50it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:57:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.22it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:57:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.25it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:57:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 16:57:57 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1381 [type=json_invalid, input_value='{\"title\":\"Wrong tablet c...cident as well, as this', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 132.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.83it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:57:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.34it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 16:58:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.34it/s]\n","reportParamGridSearch time --- 10.778777237733205 minutes ---\n"]}],"source":["!python app/reportParamGridSearch.py --model_id openai-community/gpt2-xl --non-threaded --prompt_method B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 2  --temperature 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 30 50 70 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","metadata":{"id":"JozIHMbjZGtI"},"source":["# Several models run in parallel\n","In linux command with the & we can run several programs in parallel\n","**Careful it fills GPU RAM quickly if models are > 2 B parameters**\n","\n","**NOTA:** Better use the Colab L4 GPU for charging two models in parallel. The L4 GPU size is 25 GB, so it could admit up to 5B parameters more or less, i.e.:\n","\n","- two models of at most 2.5 B parameters.\n","- Three models of 1 B parameters each\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCGd0oBQZbTL"},"outputs":[],"source":["!python app/reportParamGridSearch.py --model_id microsoft/phi-2 --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True & python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM3-3B --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0P1LHHOXp0d"},"outputs":[],"source":["# KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b15381868984f48a958540b50f785c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f06d02f23654471b0c1377b8d7f3203","IPY_MODEL_73ddf9e17e6a4cfe85cb47a2625303e8","IPY_MODEL_b5157bdd857148fb9d5c6488691a81b8"],"layout":"IPY_MODEL_c260a953e8564a31bd2d64fbb3445f84"}},"3a0a7902dddf42028ff6c54a3c2af8d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c7b05ef240b4737948d1a48b423624e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f06d02f23654471b0c1377b8d7f3203":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e33742605ca4f12863776e771c9dc77","placeholder":"​","style":"IPY_MODEL_e54018a2d58a4c1e95410f5c9bc419e1","value":"generation_config.json: 100%"}},"73ddf9e17e6a4cfe85cb47a2625303e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c7b05ef240b4737948d1a48b423624e","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a0a7902dddf42028ff6c54a3c2af8d2","value":124}},"9e33742605ca4f12863776e771c9dc77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa7496c29bef49fc8a29d3102800ce4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5157bdd857148fb9d5c6488691a81b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b91e110c59a2464284d4d00d3204eca4","placeholder":"​","style":"IPY_MODEL_aa7496c29bef49fc8a29d3102800ce4f","value":" 124/124 [00:00&lt;00:00, 12.1kB/s]"}},"b91e110c59a2464284d4d00d3204eca4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c260a953e8564a31bd2d64fbb3445f84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e54018a2d58a4c1e95410f5c9bc419e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/ml_tricks/colab/colab_connect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"ubgiAABGmadO"},"source":["# This Colab notebook is ment to be executed from a GPU resources in Colab\n","The idea is to remotely execute the repo files (classes, main, etc)"]},{"cell_type":"markdown","metadata":{"id":"Kc3SGLPSX_7E"},"source":["# 0: Before starting, verifiy that: **After pulling the repo, you have copied the .env file into the Github repo !**"]},{"cell_type":"markdown","metadata":{"id":"r0sG1QyylCkW"},"source":["# 1 Mount Google Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5977,"status":"ok","timestamp":1755409799266,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"w69hWB_qkwLI","outputId":"b5239487-2d77-4e3e-be6e-3c8c8c1ef16a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Who is using this notebook? Samd\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","notebook_user = input('Who is using this notebook? ') # Samd or Matias"]},{"cell_type":"markdown","metadata":{"id":"FPK232yxlHD1"},"source":["# 2 config Git"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":205,"status":"ok","timestamp":1755409799474,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"GradLm7NkxKN"},"outputs":[],"source":["if notebook_user == \"Matias\":\n","    !git config --global user.name \"zbotta\"\n","    !git config --global user.email \"zbotta@proton.me\"\n","elif notebook_user == \"Samd\":\n","    !git config --global user.name \"SamdGuizani\"\n","    !git config --global user.email \"samd.guizani@gmail.com\"\n","else:\n","    print(\"WARNING - Notebookuser unknown.\")"]},{"cell_type":"markdown","metadata":{"id":"tQRJVtrHynSw"},"source":["## RUN THIS CELL ONLY ONCE!\n","To clone the Github repo"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":586,"status":"ok","timestamp":1755409800063,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"E1otG_64l1Qu"},"outputs":[],"source":["from google.colab import userdata\n","\n","if notebook_user == \"Matias\":\n","    github_token = userdata.get('zbotta_token')\n","elif notebook_user == \"Samd\":\n","    github_token = userdata.get('GitHub_Samd_ReportAgent_GoogleColab')\n","else:\n","    print(\"WARNING - Notebookuser unknown. No GitHub token loaded\")\n","\n","token = github_token # why is this needed? token is a variable not used later.\n","username = \"zbotta\"\n","repo = 'reportingAgent'\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109,"status":"ok","timestamp":1755409800172,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"PQgo_qSExuXm","outputId":"8a5e1720-70d0-4a36-f149-ef6421184231"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path '/content/drive/MyDrive/GitHub/reportingAgent' already exists and is not an empty directory.\n"]}],"source":["!git clone https://{username}:{github_token}@github.com/{username}/{repo}.git /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1755409800180,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"ze8qK-7nmx_O","outputId":"dec1aed1-e484-4855-f75a-9ce8c8a3a425"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"kVgHJTXrHLIX","executionInfo":{"status":"ok","timestamp":1755409800202,"user_tz":-120,"elapsed":21,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"outputs":[],"source":["#!git remote set-url origin https://{username}:{github_token}@github.com/{username}/{repo}.git"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"0Rwp4A70m_2U","executionInfo":{"status":"ok","timestamp":1755409800223,"user_tz":-120,"elapsed":7,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"outputs":[],"source":["#!git remote get-url origin"]},{"cell_type":"markdown","metadata":{"id":"q0fNz_r7m4UG"},"source":["# Use git commands"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":805,"status":"ok","timestamp":1755409801023,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"ybZIZbg2-qhU","outputId":"e12bb913-0134-4cec-b67f-6d991f37315e"},"outputs":[{"output_type":"stream","name":"stdout","text":["M\tPoC/PoC_00_Setup and GitHub management.ipynb\n","M\tPoC/PoC_01_Prompt and report gen with different models.ipynb\n","M\tPoC/reportAgent-remote.ipynb\n","M\tapp/datasets/pharma_dev_reports_collection.xlsx\n","Already on 'dev'\n","Your branch is up to date with 'origin/dev'.\n"]}],"source":["!git fetch\n","!git checkout dev"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18343,"status":"ok","timestamp":1755431268852,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"g9D7zP_oRsaN","outputId":"0587179d-4a60-4538-ff10-c65c2cc3f22c"},"outputs":[{"output_type":"stream","name":"stdout","text":["remote: Enumerating objects: 29, done.\u001b[K\n","remote: Counting objects:   3% (1/29)\u001b[K\rremote: Counting objects:   6% (2/29)\u001b[K\rremote: Counting objects:  10% (3/29)\u001b[K\rremote: Counting objects:  13% (4/29)\u001b[K\rremote: Counting objects:  17% (5/29)\u001b[K\rremote: Counting objects:  20% (6/29)\u001b[K\rremote: Counting objects:  24% (7/29)\u001b[K\rremote: Counting objects:  27% (8/29)\u001b[K\rremote: Counting objects:  31% (9/29)\u001b[K\rremote: Counting objects:  34% (10/29)\u001b[K\rremote: Counting objects:  37% (11/29)\u001b[K\rremote: Counting objects:  41% (12/29)\u001b[K\rremote: Counting objects:  44% (13/29)\u001b[K\rremote: Counting objects:  48% (14/29)\u001b[K\rremote: Counting objects:  51% (15/29)\u001b[K\rremote: Counting objects:  55% (16/29)\u001b[K\rremote: Counting objects:  58% (17/29)\u001b[K\rremote: Counting objects:  62% (18/29)\u001b[K\rremote: Counting objects:  65% (19/29)\u001b[K\rremote: Counting objects:  68% (20/29)\u001b[K\rremote: Counting objects:  72% (21/29)\u001b[K\rremote: Counting objects:  75% (22/29)\u001b[K\rremote: Counting objects:  79% (23/29)\u001b[K\rremote: Counting objects:  82% (24/29)\u001b[K\rremote: Counting objects:  86% (25/29)\u001b[K\rremote: Counting objects:  89% (26/29)\u001b[K\rremote: Counting objects:  93% (27/29)\u001b[K\rremote: Counting objects:  96% (28/29)\u001b[K\rremote: Counting objects: 100% (29/29)\u001b[K\rremote: Counting objects: 100% (29/29), done.\u001b[K\n","remote: Compressing objects:  25% (1/4)\u001b[K\rremote: Compressing objects:  50% (2/4)\u001b[K\rremote: Compressing objects:  75% (3/4)\u001b[K\rremote: Compressing objects: 100% (4/4)\u001b[K\rremote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 17 (delta 14), reused 16 (delta 13), pack-reused 0 (from 0)\u001b[K\n","Unpacking objects: 100% (17/17), 25.44 KiB | 5.00 KiB/s, done.\n","From https://github.com/zBotta/reportingAgent\n","   a4c0d00..efcfd3c  dev        -> origin/dev\n","Updating a4c0d00..efcfd3c\n","error: Your local changes to the following files would be overwritten by merge:\n","\tPoC/reportAgent-remote.ipynb\n","Please commit your changes or stash them before you merge.\n","Aborting\n"]}],"source":["!git pull"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":94,"status":"ok","timestamp":1755409801819,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"ooqu1-hlm7Af","outputId":"275c2af8-05fd-4646-b1f4-aad0b9d1f2ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["On branch dev\n","Your branch is up to date with 'origin/dev'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   PoC/PoC_00_Setup and GitHub management.ipynb\u001b[m\n","\t\u001b[31mmodified:   PoC/PoC_01_Prompt and report gen with different models.ipynb\u001b[m\n","\t\u001b[31mmodified:   PoC/reportAgent-remote.ipynb\u001b[m\n","\t\u001b[31mmodified:   app/datasets/pharma_dev_reports_collection.xlsx\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mPoC/Experiments/\u001b[m\n","\t\u001b[31mPoC/python_env_setup.sh\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}],"source":["!git status"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1755409801822,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"BsN8-zMZ8Z-n"},"outputs":[],"source":["# !git push origin dev\n"]},{"cell_type":"markdown","metadata":{"id":"nXz98pxKo8oH"},"source":["# Using project scripts\n","\n","Reference : [Importing python library from Drive](https://colab.research.google.com/drive/12qC2abKAIAlUM_jNAokGlooKY-idbSxi#scrollTo=prUMpfLaB-D7)"]},{"cell_type":"markdown","metadata":{"id":"mvLEUb7VzVD0"},"source":["## Install project dependencies\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1755409801859,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"S6sPDJjVSHQf","outputId":"cd6c2e17-39ed-4831-9523-02f84a39e89f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-lZSS9qFzSLk","executionInfo":{"status":"ok","timestamp":1755409904537,"user_tz":-120,"elapsed":102677,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}},"outputId":"0ea580a3-54e3-4828-ddbb-f19afd4b1f79"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: aiohappyeyeballs==2.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.1)\n","Requirement already satisfied: aiohttp==3.12.14 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.12.14)\n","Requirement already satisfied: aiosignal==1.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.4.0)\n","Requirement already satisfied: airportsdata==20250706 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (20250706)\n","Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.7.0)\n","Requirement already satisfied: anyio==4.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.9.0)\n","Requirement already satisfied: attrs==25.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (25.3.0)\n","Requirement already satisfied: bert-score==0.3.13 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.3.13)\n","Requirement already satisfied: certifi==2025.7.14 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2025.7.14)\n","Requirement already satisfied: charset-normalizer==3.4.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (3.4.2)\n","Requirement already satisfied: click==8.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (8.2.1)\n","Requirement already satisfied: cloudpickle==3.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (3.1.1)\n","Requirement already satisfied: colorama==0.4.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.4.6)\n","Requirement already satisfied: contourpy==1.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (1.3.2)\n","Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.12.1)\n","Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (4.0.0)\n","Requirement already satisfied: dill==0.3.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (0.3.8)\n","Requirement already satisfied: diskcache==5.6.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (5.6.3)\n","Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (1.9.0)\n","Requirement already satisfied: docstring_parser==0.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (0.17.0)\n","Requirement already satisfied: dotenv==0.9.9 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (0.9.9)\n","Requirement already satisfied: et_xmlfile==2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (2.0.0)\n","Requirement already satisfied: evaluate==0.4.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (0.4.5)\n","Requirement already satisfied: fastapi==0.116.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (0.116.1)\n","Requirement already satisfied: filelock==3.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (3.18.0)\n","Requirement already satisfied: fonttools==4.59.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (4.59.0)\n","Requirement already satisfied: frozenlist==1.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (1.7.0)\n","Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (2025.3.0)\n","Requirement already satisfied: genson==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 29)) (1.3.0)\n","Requirement already satisfied: groq==0.26.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (0.26.0)\n","Requirement already satisfied: h11==0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (0.16.0)\n","Requirement already satisfied: httpcore==1.0.9 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (1.0.9)\n","Requirement already satisfied: httpx==0.28.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (0.28.1)\n","Requirement already satisfied: huggingface-hub==0.33.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 34)) (0.33.4)\n","Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (3.10)\n","Requirement already satisfied: iniconfig==2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 36)) (2.1.0)\n","Requirement already satisfied: instructor==1.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 37)) (1.10.0)\n","Requirement already satisfied: interegular==0.3.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 38)) (0.3.3)\n","Requirement already satisfied: iso3166==2.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 39)) (2.1.1)\n","Requirement already satisfied: Jinja2==3.1.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (3.1.6)\n","Requirement already satisfied: jiter==0.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (0.10.0)\n","Requirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (1.5.1)\n","Requirement already satisfied: jsonpath-ng==1.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 43)) (1.7.0)\n","Requirement already satisfied: jsonschema==4.25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 44)) (4.25.0)\n","Requirement already satisfied: jsonschema-specifications==2025.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 45)) (2025.4.1)\n","Requirement already satisfied: kiwisolver==1.4.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 46)) (1.4.8)\n","Requirement already satisfied: lark==1.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 47)) (1.2.2)\n","Requirement already satisfied: markdown-it-py==3.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 48)) (3.0.0)\n","Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 49)) (3.0.2)\n","Requirement already satisfied: matplotlib==3.10.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 50)) (3.10.3)\n","Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 51)) (0.1.2)\n","Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 52)) (1.3.0)\n","Requirement already satisfied: multidict==6.6.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 53)) (6.6.3)\n","Requirement already satisfied: multiprocess==0.70.16 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 54)) (0.70.16)\n","Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 55)) (1.6.0)\n","Requirement already satisfied: networkx==3.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 56)) (3.5)\n","Requirement already satisfied: numpy==2.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 57)) (2.3.1)\n","Requirement already satisfied: openai==1.97.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 58)) (1.97.1)\n","Requirement already satisfied: openpyxl==3.1.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 59)) (3.1.5)\n","Requirement already satisfied: outlines==1.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 60)) (1.1.1)\n","Requirement already satisfied: outlines_core==0.1.26 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 61)) (0.1.26)\n","Requirement already satisfied: packaging==25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 62)) (25.0)\n","Requirement already satisfied: pandas==2.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 63)) (2.3.1)\n","Requirement already satisfied: pillow==11.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 64)) (11.3.0)\n","Requirement already satisfied: pluggy==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 65)) (1.6.0)\n","Requirement already satisfied: ply==3.11 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 66)) (3.11)\n","Requirement already satisfied: propcache==0.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 67)) (0.3.2)\n","Requirement already satisfied: pyarrow==21.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 68)) (21.0.0)\n","Requirement already satisfied: pydantic==2.11.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 69)) (2.11.7)\n","Requirement already satisfied: pydantic_core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 70)) (2.33.2)\n","Requirement already satisfied: Pygments==2.19.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 71)) (2.19.2)\n","Requirement already satisfied: pyparsing==3.2.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 72)) (3.2.3)\n","Requirement already satisfied: pytest==8.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 73)) (8.4.1)\n","Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 74)) (2.9.0.post0)\n","Requirement already satisfied: python-dotenv==1.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 75)) (1.1.1)\n","Requirement already satisfied: pytz==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 76)) (2025.2)\n","Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 77)) (6.0.2)\n","Requirement already satisfied: referencing==0.36.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 78)) (0.36.2)\n","Requirement already satisfied: regex==2024.11.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 79)) (2024.11.6)\n","Requirement already satisfied: requests==2.32.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 80)) (2.32.4)\n","Requirement already satisfied: rich==14.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 81)) (14.0.0)\n","Requirement already satisfied: rpds-py==0.26.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 82)) (0.26.0)\n","Requirement already satisfied: safetensors==0.5.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 83)) (0.5.3)\n","Requirement already satisfied: scikit-learn==1.7.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 84)) (1.7.1)\n","Requirement already satisfied: scipy==1.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 85)) (1.16.0)\n","Requirement already satisfied: sentence-transformers==5.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 86)) (5.0.0)\n","Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 87)) (1.5.4)\n","Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 88)) (1.17.0)\n","Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 89)) (1.3.1)\n","Requirement already satisfied: starlette==0.47.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 90)) (0.47.2)\n","Requirement already satisfied: sympy==1.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 91)) (1.14.0)\n","Requirement already satisfied: tenacity==9.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 92)) (9.1.2)\n","Requirement already satisfied: threadpoolctl==3.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 93)) (3.6.0)\n","Requirement already satisfied: tokenizers==0.21.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 94)) (0.21.2)\n","Collecting torch==2.7.1 (from -r requirements.txt (line 95))\n","  Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n","Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 96)) (4.67.1)\n","Requirement already satisfied: transformers==4.53.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 97)) (4.53.3)\n","Requirement already satisfied: typer==0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 98)) (0.16.0)\n","Requirement already satisfied: typing-inspection==0.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 99)) (0.4.1)\n","Requirement already satisfied: typing_extensions==4.14.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 100)) (4.14.1)\n","Requirement already satisfied: tzdata==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 101)) (2025.2)\n","Requirement already satisfied: urllib3==2.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 102)) (2.5.0)\n","Requirement already satisfied: xxhash==3.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 103)) (3.5.0)\n","Requirement already satisfied: yarl==1.20.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 104)) (1.20.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.33.4->-r requirements.txt (line 34)) (1.1.7)\n","Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.3.1 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch==2.7.1->-r requirements.txt (line 95)) (75.2.0)\n","Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n","Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n","Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n","Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n","Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n","Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n","Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n","Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n","Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n","Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n","Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n","Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n","Installing collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n","    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.4.0\n","    Uninstalling triton-3.4.0:\n","      Successfully uninstalled triton-3.4.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.8.90\n","    Uninstalling nvidia-nvtx-cu12-12.8.90:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n","    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.27.3\n","    Uninstalling nvidia-nccl-cu12-2.27.3:\n","      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.9.90\n","    Uninstalling nvidia-curand-cu12-10.3.9.90:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n","  Attempting uninstall: nvidia-cufile-cu12\n","    Found existing installation: nvidia-cufile-cu12 1.13.1.3\n","    Uninstalling nvidia-cufile-cu12-1.13.1.3:\n","      Successfully uninstalled nvidia-cufile-cu12-1.13.1.3\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n","    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n","    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n","    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n","    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n","    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n","    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n","    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.8.0\n","    Uninstalling torch-2.8.0:\n","      Successfully uninstalled torch-2.8.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.23.0 requires torch==2.8.0, but you have torch 2.7.1 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 torch-2.7.1 triton-3.3.1\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":143535,"status":"ok","timestamp":1755410048074,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"7jbcWB1PGsoj","outputId":"9f0882d9-2b29-4c16-f40c-c2ff6b92cb77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.1)\n","Collecting torch\n","  Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n","  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n","  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n","  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n","  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n","  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n","  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n","  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n","  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n","  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n","Collecting nvidia-nccl-cu12==2.27.3 (from torch)\n","  Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n","  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n","  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n","  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==3.4.0 (from torch)\n","  Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (75.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.3.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Using cached torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n","Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n","Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n","Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n","Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n","Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n","Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n","Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n","Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n","Using cached nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n","Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n","Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","Using cached triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n","Installing collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n","    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.3.1\n","    Uninstalling triton-3.3.1:\n","      Successfully uninstalled triton-3.3.1\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.6.77\n","    Uninstalling nvidia-nvtx-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n","    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.26.2\n","    Uninstalling nvidia-nccl-cu12-2.26.2:\n","      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufile-cu12\n","    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n","    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n","      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n","    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.7.1\n","    Uninstalling torch-2.7.1:\n","      Successfully uninstalled torch-2.7.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\n","cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.8.0 triton-3.4.0\n"]}],"source":["!pip install --upgrade torch torchvision"]},{"cell_type":"markdown","metadata":{"id":"-2wXabjgzszt"},"source":["## Import a python script from project"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1755410048080,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"a8WjxROAzwUT","outputId":"9435edb2-f5a7-44bc-e0c0-25f013dd9418"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":779,"status":"ok","timestamp":1755410048883,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"-oZlOWBODm4M","outputId":"9ccdab7c-1055-4b83-ac2e-083659445272"},"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}],"source":["!git pull"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":309,"status":"ok","timestamp":1755410049194,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"4r_Hif3sXJDN","outputId":"ac2ea46e-041e-4d30-86d5-ce80020a235b"},"outputs":[{"output_type":"stream","name":"stdout","text":["08/16/2025 19:48:48 - __main__ - INFO - Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['meta-llama/Llama-3.2-1B-Instruct'], 'prompt_method': ['B', 'C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [1], 'end_idx': [10], 'temperature': [0.5, 0.7, 1.0], 'top_p': [0.8, 1.0], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","08/16/2025 19:48:48 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/16/2025 19:48:48 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/16/2025 19:48:50 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/16/2025 19:48:50 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/16/2025 19:48:52 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/16/2025 19:48:52 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/16/2025 19:48:54 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/16/2025 19:48:54 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/16/2025 19:48:54 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/16/2025 19:48:55 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/16/2025 19:48:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/16/2025 19:48:55 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/16/2025 19:48:55 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/16/2025 19:48:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/16/2025 19:48:55 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/16/2025 19:48:56 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/16/2025 19:48:56 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/16/2025 19:48:56 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/16/2025 19:48:56 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/16/2025 19:48:56 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/16/2025 19:48:56 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/16/2025 19:48:56 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/16/2025 19:48:56 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/16/2025 19:48:56 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/16/2025 19:48:56 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/16/2025 19:48:56 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/16/2025 19:48:57 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/16/2025 19:48:57 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/pharma_dev_reports_collection.xlsx\n","08/16/2025 19:48:58 - huggingface_hub.file_download - INFO - Downloading 'generation_config.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/75ae08310d6d23df373ee2644b497192b3cce6d8.incomplete'\n","08/16/2025 19:48:58 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/75ae08310d6d23df373ee2644b497192b3cce6d8\n","08/16/2025 19:48:58 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:48:58 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:48:58 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=meta-llama/Llama-3.2-1B-Instruct\n","08/16/2025 19:48:58 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=meta-llama/Llama-3.2-1B-Instruct\n","08/16/2025 19:48:58 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=meta-llama/Llama-3.2-1B-Instruct\n","08/16/2025 19:48:58 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 0.6, 'top_k': 50, 'top_p': 0.9, 'repetition_penalty': 1.0, 'do_sample': True}\n","08/16/2025 19:48:58 - mods.testBench - INFO - Test Bench loaded\n","08/16/2025 19:48:58 - huggingface_hub._login - INFO - The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","08/16/2025 19:48:59 - huggingface_hub._login - INFO - Token is valid (permission: read).\n","08/16/2025 19:48:59 - huggingface_hub.utils._auth - INFO - The token `Colab Notebook Access` has been saved to /root/.cache/huggingface/stored_tokens\n","08/16/2025 19:48:59 - huggingface_hub._login - INFO - Your token has been saved to /root/.cache/huggingface/token\n","08/16/2025 19:48:59 - huggingface_hub._login - INFO - Login successful.\n","08/16/2025 19:48:59 - huggingface_hub._login - INFO - The current active token is: `Colab Notebook Access`\n","08/16/2025 19:48:59 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/4ff488a165e900e5129cda7c20ab32d568d2a475.incomplete'\n","08/16/2025 19:48:59 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/4ff488a165e900e5129cda7c20ab32d568d2a475\n","08/16/2025 19:49:00 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/5cc5f00a5b203e90a27a3bd60d1ec393b07971e8.incomplete'\n","08/16/2025 19:49:00 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/5cc5f00a5b203e90a27a3bd60d1ec393b07971e8\n","08/16/2025 19:49:01 - huggingface_hub.file_download - INFO - Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/02ee80b6196926a5ad790a004d9efd6ab1ba6542.incomplete'\n","08/16/2025 19:49:01 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/02ee80b6196926a5ad790a004d9efd6ab1ba6542\n","08/16/2025 19:49:02 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer.json\n","08/16/2025 19:49:02 - transformers.tokenization_utils_base - INFO - loading file tokenizer.model from cache at None\n","08/16/2025 19:49:02 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/16/2025 19:49:02 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/special_tokens_map.json\n","08/16/2025 19:49:02 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/tokenizer_config.json\n","08/16/2025 19:49:02 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/16/2025 19:49:02 - transformers.tokenization_utils_base - INFO - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","08/16/2025 19:49:02 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/3e3aaf51a035cb5092d9f6827a0dc074657ba88c.incomplete'\n","08/16/2025 19:49:02 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/3e3aaf51a035cb5092d9f6827a0dc074657ba88c\n","08/16/2025 19:49:03 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/config.json\n","08/16/2025 19:49:03 - transformers.configuration_utils - INFO - Model config LlamaConfig {\n","  \"architectures\": [\n","    \"LlamaForCausalLM\"\n","  ],\n","  \"attention_bias\": false,\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"head_dim\": 64,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 2048,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8192,\n","  \"max_position_embeddings\": 131072,\n","  \"mlp_bias\": false,\n","  \"model_type\": \"llama\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 16,\n","  \"num_key_value_heads\": 8,\n","  \"pretraining_tp\": 1,\n","  \"rms_norm_eps\": 1e-05,\n","  \"rope_scaling\": {\n","    \"factor\": 32.0,\n","    \"high_freq_factor\": 4.0,\n","    \"low_freq_factor\": 1.0,\n","    \"original_max_position_embeddings\": 8192,\n","    \"rope_type\": \"llama3\"\n","  },\n","  \"rope_theta\": 500000.0,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 128256\n","}\n","\n","08/16/2025 19:49:03 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/1ff795ff6a07e6a68085d206fb84417da2f083f68391c2843cd2b8ac6df8538f.incomplete'\n","08/16/2025 19:51:21 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/blobs/1ff795ff6a07e6a68085d206fb84417da2f083f68391c2843cd2b8ac6df8538f\n","08/16/2025 19:51:21 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/model.safetensors\n","08/16/2025 19:51:21 - transformers.modeling_utils - INFO - Instantiating LlamaForCausalLM model under default dtype torch.float32.\n","08/16/2025 19:51:21 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ]\n","}\n","\n","08/16/2025 19:51:28 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing LlamaForCausalLM.\n","\n","08/16/2025 19:51:28 - transformers.modeling_utils - INFO - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-3.2-1B-Instruct.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n","08/16/2025 19:51:29 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:51:29 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:51:30 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:51:30 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:51:30 - mods.testBench - INFO - Results file is expected to have 120 rows.\n","08/16/2025 19:51:30 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:51:30 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:51:30 - mods.testBench - INFO - Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:51:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:51:41 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/16/2025 19:51:41 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/16/2025 19:51:41 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/16/2025 19:51:41 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/16/2025 19:51:41 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/16/2025 19:51:41 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/16/2025 19:51:41 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/16/2025 19:51:41 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/16/2025 19:51:41 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/16/2025 19:51:41 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/16/2025 19:51:41 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/16/2025 19:51:41 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/16/2025 19:51:41 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/16/2025 19:51:42 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/16/2025 19:51:42 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:51:42 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8712822198867798], 'bs_recall': [0.8869726657867432], 'bs_f1': [0.8790574073791504], 'be_sim': array([0.8898801], dtype=float32), 'ce_sim': array([0.84297013]), 'title': 'Contaminated Gloves Incident', 'report': 'The incident occurred on June 12, 2025, at 2:40 PM at the Grade A Filling Line, Sterile Suite A. Emily Zhang, the line operator, touched a non-sterile surface during setup, resulting in the contamination of gloves. The contaminated gloves were then used to fill vials. The incident was logged and the affected vials were quarantined. The line was stopped and gloves were changed to prevent further contamination.'}\n","08/16/2025 19:51:42 - mods.testBench - INFO - Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:51:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:51:50 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:51:51 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8429911136627197], 'bs_recall': [0.8287628889083862], 'bs_f1': [0.8358164429664612], 'be_sim': array([0.7417739], dtype=float32), 'ce_sim': array([0.51418984]), 'title': 'Report', 'report': \"The incident occurred at the Grade A Filling Line, Sterile Suite A, during aseptic filling. Emily Zhang, the line operator, was involved in the incident. She touched a non-sterile surface during setup. The incident was not reported to the sterile crew as she was unaware of the surface's sterility. The affected vials were quarantined and the incident was logged.\"}\n","08/16/2025 19:51:51 - mods.testBench - INFO - Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:51:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:51:53 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:51:53 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.6886374950408936], 'bs_recall': [0.6621334552764893], 'bs_f1': [0.6751254796981812], 'be_sim': array([0.1579286], dtype=float32), 'ce_sim': array([-0.8198359]), 'title': 'Contaminated Gloves Incident Report', 'report': 'What, when, where, who, how, why, and contingency actions are provided below:'}\n","08/16/2025 19:51:53 - mods.testBench - INFO - Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:51:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:51:55 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:51:55 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7284426689147949], 'bs_recall': [0.6811095476150513], 'bs_f1': [0.703981339931488], 'be_sim': array([0.2054334], dtype=float32), 'ce_sim': array([-0.79415405]), 'title': 'Report', 'report': 'Description about the event, including what, when, where, who, how, why, and contingency actions.'}\n","08/16/2025 19:51:55 - mods.testBench - INFO - Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:51:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:52:05 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:52:05 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8115625977516174], 'bs_recall': [0.8800305724143982], 'bs_f1': [0.8444109559059143], 'be_sim': array([0.8014202], dtype=float32), 'ce_sim': array([0.90247387]), 'title': 'Contaminated Gloves Incident Report', 'report': 'During the June 12, 2025, 2:40 PM aseptic filling, a contaminated glove was observed on the non-sterile surface during setup by Emily Zhang, the line operator. The incident occurred on Grade A Filling Line, Sterile Suite A. This incident highlights the importance of operator awareness and adherence to sterile procedures to prevent the contamination of sterile equipment and products. It has also prompted the line to be stopped, gloves to be changed, affected vials to be quarantined, and the incident has been logged. A thorough investigation will be conducted to determine the root cause of the incident and implement necessary corrective actions to prevent future occurrences.'}\n","08/16/2025 19:52:05 - mods.testBench - INFO - Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:52:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:52:12 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:52:12 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8439522385597229], 'bs_recall': [0.8917080163955688], 'bs_f1': [0.8671731352806091], 'be_sim': array([0.8684833], dtype=float32), 'ce_sim': array([0.88383967]), 'title': 'Report of Contaminated Gloves Incident', 'report': \"The contaminated gloves incident occurred on June 12, 2025, 2:40 PM in Grade A Filling Line, Sterile Suite A, when the line operator, Emily Zhang touched a non-sterile surface during the setup sequence. The operator was unaware of the contamination, and the gloves were contaminated, resulting in affected vials. To prevent recurrence, the line stopped, gloves were changed, affected vials were quarantined, and an incident was logged due to the operator's failure to follow proper aseptic procedures.}\"}\n","08/16/2025 19:52:12 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:52:12 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:52:12 - mods.testBench - INFO - Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:52:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:52:18 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:52:18 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8978418111801147], 'bs_recall': [0.8875453472137451], 'bs_f1': [0.892663836479187], 'be_sim': array([0.915562], dtype=float32), 'ce_sim': array([0.68868428]), 'title': 'Incident Report', 'report': 'On June 12, 2025, at 2:40 PM, Emily Zhang from Line Operator touched a non-sterile surface during setup, leading to the contamination of gloves. The incident resulted in the line being stopped, gloves changed, and affected vials quarantined. A log entry was also made.'}\n","08/16/2025 19:52:18 - mods.testBench - INFO - Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:52:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:52:23 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:52:23 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9130646586418152], 'bs_recall': [0.8837305903434753], 'bs_f1': [0.8981581926345825], 'be_sim': array([0.8661417], dtype=float32), 'ce_sim': array([0.78742039]), 'title': 'Incident Report', 'report': 'On June 12, 2025, at 2:40 PM, Emily Zhang from Line Operator touched a non-sterile surface during setup on Grade A Filling Line, Sterile Suite A. The incident was logged and gloves were changed. The affected vials were quarantined.'}\n","08/16/2025 19:52:23 - mods.testBench - INFO - Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:52:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:52:29 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:52:29 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8909750580787659], 'bs_recall': [0.9121838808059692], 'bs_f1': [0.901454746723175], 'be_sim': array([0.92277926], dtype=float32), 'ce_sim': array([0.93682992]), 'title': 'Incident Report', 'report': \"On June 12, 2025, at 2:40 PM, Emily Zhang, Line Operator, observed contaminated gloves during aseptic filling in Grade A Filling Line, Sterile Suite A. The incident was caused by Emily's unawareness of the non-sterile surface touched during setup. The line was stopped, gloves were changed, and vials were quarantined. An incident log was also created.\"}\n","08/16/2025 19:52:29 - mods.testBench - INFO - Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:52:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:52:32 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:52:32 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8627183437347412], 'bs_recall': [0.7983831167221069], 'bs_f1': [0.8293048739433289], 'be_sim': array([0.73093504], dtype=float32), 'ce_sim': array([0.57667691]), 'title': 'Incident Report', 'report': 'The following incident report for the Contaminated gloves incident occurred on June 12, 2025, at 2:40 PM, during the Grade A Filling Line, Sterile Suite A.'}\n","08/16/2025 19:52:32 - mods.testBench - INFO - Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:52:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:52:37 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:52:37 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8991818428039551], 'bs_recall': [0.8925055265426636], 'bs_f1': [0.8958312273025513], 'be_sim': array([0.84576976], dtype=float32), 'ce_sim': array([0.86652559]), 'title': 'Contaminated gloves', 'report': 'On June 12, 2025, at 2:40 PM, Emily Zhang from Line Operator setup and touched a non-sterile surface during the aseptic filling process on Grade A Filling Line, Sterile Suite A. She was unaware of the contaminated surface. The incident was logged and affected vials were quarantined.'}\n","08/16/2025 19:52:37 - mods.testBench - INFO - Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:52:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:52:38 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:52:38 - mods.testBench - INFO - results: {'report_idx': 1, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.5091110467910767], 'bs_recall': [0.4738360047340393], 'bs_f1': [0.4908405542373657], 'be_sim': array([0.09558412], dtype=float32), 'ce_sim': array([-0.7579627]), 'title': 'Incident Report', 'report': '{}'}\n","08/16/2025 19:52:38 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:52:38 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:52:38 - mods.testBench - INFO - Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:52:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:52:57 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1488 [type=json_invalid, input_value='{\"title\": \"Late Sampling... Ortiz, QC Analyst, who', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/16/2025 19:52:57 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:52:57 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7160652279853821], 'bs_recall': [0.6608035564422607], 'bs_f1': [0.6873254179954529], 'be_sim': array([0.15030116], dtype=float32), 'ce_sim': array([-0.43856061]), 'title': 'NO PYDANTIC TITLE', 'report': 'NO PYDANTIC REPORT'}\n","08/16/2025 19:52:57 - mods.testBench - INFO - Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:52:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:53:04 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:53:04 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8355857133865356], 'bs_recall': [0.8954019546508789], 'bs_f1': [0.8644603490829468], 'be_sim': array([0.7647108], dtype=float32), 'ce_sim': array([0.90377492]), 'title': 'Late Sampling Event Report', 'report': 'The late sampling event occurred on June 15, 2025, 11:00 AM in the Stability Room 3 of the Quality Control (QC) Lab. Daniel Ortiz, a QC Analyst, was involved in the sampling process. The delay was due to miscommunication in the sampling schedule, which led to a deviation from the planned sample collection time. The event resulted in additional sample points being added to the batch, and Daniel Ortiz was retrained to ensure the accuracy of the sampling process. The deviation was logged, and the analyst was retrained to prevent similar incidents in the future.'}\n","08/16/2025 19:53:04 - mods.testBench - INFO - Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:53:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:53:12 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:53:12 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8343574404716492], 'bs_recall': [0.9121727347373962], 'bs_f1': [0.87153160572052], 'be_sim': array([0.75362957], dtype=float32), 'ce_sim': array([0.88831502]), 'title': 'Late Sampling of Stability Chamber', 'report': 'The late sampling of the stability chamber was reported on June 15, 2025, at 11:00 AM in the QC Lab, Stability Room 3. The delay was caused by oversight due to miscommunication in the sampling schedule. The delay resulted in the delay of the sample collection by 24 hours. The quality control analyst, Daniel Ortiz, was notified of the delay and the necessary actions were taken. The delay was logged in the system and additional sample points were added to the collection process. The analyst was retrained to ensure that such delays do not happen again in the future.'}\n","08/16/2025 19:53:12 - mods.testBench - INFO - Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:53:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:53:17 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:53:17 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.87096107006073], 'bs_recall': [0.8633037805557251], 'bs_f1': [0.8671155571937561], 'be_sim': array([0.7767186], dtype=float32), 'ce_sim': array([0.83388001]), 'title': 'Late Sampling of Stability Chamber', 'report': 'Late sampling of stability chamber June 15, 2025, 11:00 AM, QC Lab, Stability Room 3, Daniel Ortiz, QC Analyst, delayed due to oversight in sampling schedule, deviation logged, additional sample points added, analyst retrained.'}\n","08/16/2025 19:53:17 - mods.testBench - INFO - Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:53:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:53:24 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:53:24 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8551168441772461], 'bs_recall': [0.8865934014320374], 'bs_f1': [0.8705707788467407], 'be_sim': array([0.75916886], dtype=float32), 'ce_sim': array([0.86158741]), 'title': 'Late Sampling of Stability Chamber Report', 'report': 'Late sampling of stability chamber occurred on June 15, 2025, 11:00 AM at the Quality Control Lab in the Stability Room 3. The delay was caused by miscommunication in the sampling schedule, resulting in a 24-hour delay in sample collection. Daniel Ortiz, QC Analyst, was involved in the delay. The laboratory will conduct additional sample points and retrain the analyst.'}\n","08/16/2025 19:53:24 - mods.testBench - INFO - Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:53:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:53:25 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:53:25 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.4379507303237915], 'bs_recall': [0.5150429010391235], 'bs_f1': [0.47337865829467773], 'be_sim': array([0.02804885], dtype=float32), 'ce_sim': array([-0.40778291]), 'title': 'Late Sampling of Stability Chamber', 'report': '...'}\n","08/16/2025 19:53:25 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:53:25 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:53:25 - mods.testBench - INFO - Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:53:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:53:29 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:53:29 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9016739130020142], 'bs_recall': [0.8854644298553467], 'bs_f1': [0.8934956192970276], 'be_sim': array([0.6961853], dtype=float32), 'ce_sim': array([0.75891721]), 'title': 'Late sampling of stability chamber', 'report': 'On June 15, 2025, at 11:00 AM, Daniel Ortiz from QC Lab retrained the QC Analyst after the delay in sample collection due to oversight in the sampling schedule. Additional sample points were added, and the analyst was retrained to improve the process.'}\n","08/16/2025 19:53:29 - mods.testBench - INFO - Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:53:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:53:34 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:53:34 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8923121094703674], 'bs_recall': [0.8999285697937012], 'bs_f1': [0.8961041569709778], 'be_sim': array([0.772418], dtype=float32), 'ce_sim': array([0.88518]), 'title': 'Late sampling of stability chamber', 'report': 'On June 15, 2025, at 11:00 AM, Daniel Ortiz from QC Lab discovered the issue of late sampling of stability chamber. The delay was caused by oversight due to miscommunication in the sampling schedule. The deviation was logged and the additional sample points were added to ensure the quality of the sample.'}\n","08/16/2025 19:53:34 - mods.testBench - INFO - Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:53:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:53:40 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:53:40 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9080890417098999], 'bs_recall': [0.8832629919052124], 'bs_f1': [0.8955039978027344], 'be_sim': array([0.706162], dtype=float32), 'ce_sim': array([0.79642576]), 'title': 'Late sampling of stability chamber', 'report': 'On June 15, 2025, at 11:00 AM, Daniel Ortiz from QC Lab retrained the analyst after logging the deviation due to oversight in the sampling schedule. Additional sample points were added to ensure the stability of the sample.'}\n","08/16/2025 19:53:40 - mods.testBench - INFO - Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:53:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:53:45 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:53:45 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8864820599555969], 'bs_recall': [0.9202128648757935], 'bs_f1': [0.9030325412750244], 'be_sim': array([0.809313], dtype=float32), 'ce_sim': array([0.92010206]), 'title': 'Late Sampling of Stability Chamber', 'report': 'On June 15, 2025, at 11:00 AM, Daniel Ortiz from the QC Lab collected the sample in the Stability Room 3. The delay was due to oversight and miscommunication in the sampling schedule. The delay of 24 hours caused disruption to downstream processes. Deviation was logged and analyzed. Further steps included additional sample points and retraining of Daniel Ortiz.'}\n","08/16/2025 19:53:45 - mods.testBench - INFO - Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:53:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:53:52 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:53:52 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8524346947669983], 'bs_recall': [0.8785194158554077], 'bs_f1': [0.8652805089950562], 'be_sim': array([0.8037044], dtype=float32), 'ce_sim': array([0.82770646]), 'title': 'Contingency Report', 'report': 'On June 15, 2025, at 11:00 AM, Daniel Ortiz from QC Lab collected a sample 24 hours later than scheduled due to oversight in the sampling schedule. The delay was not corrected by the end of the day, leading to an additional 24 hours of sample delay. A deviation was noted during sampling and the schedule was not corrected. A retraining of Daniel Ortiz was initiated to prevent similar delays in the future.'}\n","08/16/2025 19:53:52 - mods.testBench - INFO - Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:53:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:54:00 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:54:00 - mods.testBench - INFO - results: {'report_idx': 2, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7846006155014038], 'bs_recall': [0.8601137399673462], 'bs_f1': [0.8206236362457275], 'be_sim': array([0.6255559], dtype=float32), 'ce_sim': array([0.76698226]), 'title': 'Late sampling of stability chamber report', 'report': '{\"description\": \"On June 15, 2025, at 11:00 AM, Daniel Ortiz loaded the wrong tablet sampling solution during the stability sampling process in the Quality Control Lab, resulting in a delay of 24 hours. This mistake occurred due to an oversight in communication with the laboratory assistant responsible for performing the task. Despite corrective efforts, retraining of the analyst was not implemented sooner. The laboratory assistant has expressed their discomfort with the mistake, and no deviation from standard operating procedure has been recorded at this time.\",\"}          {'}\n","08/16/2025 19:54:00 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:54:00 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:54:00 - mods.testBench - INFO - Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:54:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:54:06 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:54:06 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8764459490776062], 'bs_recall': [0.8486065864562988], 'bs_f1': [0.8623015880584717], 'be_sim': array([0.831247], dtype=float32), 'ce_sim': array([0.82653189]), 'title': 'Temperature Excursion Report', 'report': 'Temperature excursion in cold room occurred on June 17, 2025, 6:00 AM  9:00 AM, in Cold Room 2, Warehouse Building 1. The event was detected by automated monitoring and was caused by an unexpected failure of the compressor unit. The root cause analysis is currently underway.'}\n","08/16/2025 19:54:06 - mods.testBench - INFO - Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:54:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:54:10 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:54:10 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8989217281341553], 'bs_recall': [0.8794968128204346], 'bs_f1': [0.8891031742095947], 'be_sim': array([0.7691173], dtype=float32), 'ce_sim': array([0.78658241]), 'title': 'Temperature Excursion in Cold Room', 'report': 'Temperature excursion in cold room occurred on June 17, 2025, between 6:00 AM and 9:00 AM due to a compressor unit failure in the HVAC system. The unexpected malfunction led to a temperature rise of 10C. The root cause analysis is underway to identify the underlying issue.'}\n","08/16/2025 19:54:10 - mods.testBench - INFO - Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:54:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:54:14 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:54:14 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8934177160263062], 'bs_recall': [0.8372418880462646], 'bs_f1': [0.8644181489944458], 'be_sim': array([0.83818024], dtype=float32), 'ce_sim': array([0.75934803]), 'title': 'Temperature Excursion Report', 'report': 'Temperature excursion in cold room on June 17, 2025, 6:00 AM  9:00 AM, in Cold Room 2, Warehouse Building 1, due to compressor unit failure and HVAC malfunction.'}\n","08/16/2025 19:54:14 - mods.testBench - INFO - Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:54:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:54:22 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:54:22 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8551095128059387], 'bs_recall': [0.877894401550293], 'bs_f1': [0.8663521409034729], 'be_sim': array([0.8436702], dtype=float32), 'ce_sim': array([0.89678836]), 'title': 'Temperature Excursion in Cold Room', 'report': 'Temperature excursion in cold room occurred during June 17, 2025, 6:00 AM  9:00 AM in Cold Room 2, Warehouse Building 1 due to compressor unit failure that caused a 10C rise. The automated monitoring system detected the issue and notified the responsible departments. Immediate repairs were initiated to address the root cause of the malfunction. The affected products were moved and inspected by QA before being returned to the production line.'}\n","08/16/2025 19:54:22 - mods.testBench - INFO - Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:54:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:54:27 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:54:27 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8718435764312744], 'bs_recall': [0.8813256025314331], 'bs_f1': [0.8765589594841003], 'be_sim': array([0.8330826], dtype=float32), 'ce_sim': array([0.85181284]), 'title': 'Temperature excursion in cold room', 'report': 'Temperature excursion in cold room occurred on June 17, 2025, 6:00 AM  9:00 AM in Cold Room 2, Warehouse Building 1. The malfunctioning compressor unit of HVAC caused the temperature to rise to 10C, as detected by automated monitoring. The root cause analysis is currently underway to identify the root cause of the malfunction.'}\n","08/16/2025 19:54:27 - mods.testBench - INFO - Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:54:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:54:34 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:54:34 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8125485777854919], 'bs_recall': [0.8010843992233276], 'bs_f1': [0.8067757487297058], 'be_sim': array([0.8403084], dtype=float32), 'ce_sim': array([0.40754819]), 'title': 'Temperature Excursion in Cold Room', 'report': 'There was a temperature excursion in the cold room. The temperature rose to 10C. This happened in Cold Room 2, Warehouse Building 1. We detected it by automated monitoring. The failure of the compressor unit was due to an unexpected malfunction. Repairs were made, and a second monitoring is in place. As a preventive measure, we are taking steps to ensure all our equipment is properly maintained.'}\n","08/16/2025 19:54:34 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:54:34 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:54:34 - mods.testBench - INFO - Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:54:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:54:41 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:54:41 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8885083198547363], 'bs_recall': [0.8809496760368347], 'bs_f1': [0.884712815284729], 'be_sim': array([0.83112264], dtype=float32), 'ce_sim': array([0.77092773]), 'title': 'Temperature excursion in cold room', 'report': 'On June 17, 2025, at 6:00 AM  9:00 AM, in Cold Room 2, Warehouse Building 1, an unexpected temperature rise of 10C was detected by automated monitoring. The compressor unit failed unexpectedly, causing the temperature to exceed 10C. The products were moved, and the root cause analysis was initiated.'}\n","08/16/2025 19:54:41 - mods.testBench - INFO - Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:54:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:54:45 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:54:45 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8785033822059631], 'bs_recall': [0.831416130065918], 'bs_f1': [0.854311466217041], 'be_sim': array([0.48081255], dtype=float32), 'ce_sim': array([0.27496839]), 'title': 'Temperature excursion in cold room', 'report': 'On June 17, 2025, at 6:00 AM  9:00 AM, products moved due to malfunction of compressor unit. The root cause analysis initiated by QA to determine the cause and prevent future occurrences.'}\n","08/16/2025 19:54:45 - mods.testBench - INFO - Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:54:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:54:51 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:54:52 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8537428975105286], 'bs_recall': [0.9027672410011292], 'bs_f1': [0.8775709271430969], 'be_sim': array([0.7492695], dtype=float32), 'ce_sim': array([0.6995979]), 'title': 'Temperature excursion in cold room', 'report': 'On June 17, 2025, at 6:00 AM  9:00 AM, the cold room was affected by a temperature excursion of 10C, caused by a malfunction of the HVAC system. The automated monitoring system detected the issue, and the products were moved to a safe location. The root cause analysis is ongoing, and the HVAC system is being repaired. QA was notified, and a plan is being implemented to prevent such incidents in the future.'}\n","08/16/2025 19:54:52 - mods.testBench - INFO - Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:54:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:54:57 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:54:57 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.888764500617981], 'bs_recall': [0.8969742059707642], 'bs_f1': [0.8928504586219788], 'be_sim': array([0.9193989], dtype=float32), 'ce_sim': array([0.72370577]), 'title': 'Temperature excursion in cold room', 'report': 'On June 17, 2025, at 6:00 AM - 9:00 AM, cold room 2, warehouse building 1, products were affected by a temperature rise of 10C due to an unexpected failure of the compressor unit in the HVAC system. Automated monitoring detected the issue and notified the QA team.'}\n","08/16/2025 19:54:57 - mods.testBench - INFO - Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:54:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:55:07 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:55:07 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.6872342228889465], 'bs_recall': [0.7584553956985474], 'bs_f1': [0.7210904359817505], 'be_sim': array([0.15703364], dtype=float32), 'ce_sim': array([-0.60091287]), 'title': 'Incorrect Tablet Counting', 'report': '{\"what\": \"Incorrect tablet count in bottle for Batch RX500 of Neurocet 50 mg\", \"when\": \"July 2, 2025, 3:30 PM\", \"where\": \"Bottle Packaging Line 2\", \"who\": \"Erik Hansen (Packaging Operator, loaded wrong counting disk); Sarah Yoon (QA, identified deviation during AQL sampling)\", \"how\": \"Counting disk set for 60-count instead of 30-count\", \"why\": \"Operator selected wrong format during changeover\", \"contingency actions\": {\"line\": \"Line stopped, 500 bottles segregated, rework initiated, operator retrained.\"}'}\n","08/16/2025 19:55:07 - mods.testBench - INFO - Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:55:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:55:13 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:55:13 - mods.testBench - INFO - results: {'report_idx': 3, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8651967644691467], 'bs_recall': [0.8760715126991272], 'bs_f1': [0.8706002235412598], 'be_sim': array([0.7328848], dtype=float32), 'ce_sim': array([0.63193715]), 'title': 'Temperature Excursion', 'report': 'On June 17, 2025, from 06:00 AM to 09:00 AM, automated monitoring detected a temperature excursion in the cold room. The temperature rose significantly to 10C, due to an unexpected failure of the compressor unit. The cause was investigated and corrective measures were taken. The room was reassigned to a different HVAC system and the faulty compressor unit was repaired.}'}\n","08/16/2025 19:55:13 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:55:13 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:55:13 - mods.testBench - INFO - Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:55:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:55:32 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1686 [type=json_invalid, input_value='{\"title\": \"Incorrect Mat...ented to prevent future', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/16/2025 19:55:32 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:55:32 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7120528817176819], 'bs_recall': [0.669949471950531], 'bs_f1': [0.6903598308563232], 'be_sim': array([0.18376675], dtype=float32), 'ce_sim': array([-0.78781962]), 'title': 'NO PYDANTIC TITLE', 'report': 'NO PYDANTIC REPORT'}\n","08/16/2025 19:55:32 - mods.testBench - INFO - Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:55:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:55:39 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:55:39 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8267532587051392], 'bs_recall': [0.8131099939346313], 'bs_f1': [0.8198748826980591], 'be_sim': array([0.76796913], dtype=float32), 'ce_sim': array([0.21826352]), 'title': 'Incorrect Material Label Incident Report', 'report': 'Incorrect material label applied to a shipment in the Material Receiving Area. The wrong label was selected from the batch printout, due to look-alike/sound-alike material names. This resulted in incorrect material handling. All affected labels were corrected, and the batch was quarantined as a precautionary measure. The barcode scanner was also checked to ensure accuracy. All affected personnel were notified and trained on the new material handling procedure.'}\n","08/16/2025 19:55:39 - mods.testBench - INFO - Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:55:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:55:47 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:55:47 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8366962671279907], 'bs_recall': [0.905348539352417], 'bs_f1': [0.8696696162223816], 'be_sim': array([0.8492068], dtype=float32), 'ce_sim': array([0.82205069]), 'title': 'Incorrect Material Label Application', 'report': \"Incorrect material label applied to a shipment of goods. The material was incorrectly labeled due to look-alike/sound-alike material names, which led to a mix-up in the shipment's contents. The material was handled by Alexandra Becker, Warehouse Operator, in the Material Receiving Area. The incident occurred on June 19, 2025, at 4:30 PM. The correct material was identified through a batch printout, and all affected labels were corrected. A batch was quarantined, and a barcode scanner check was implemented to prevent similar incidents in the future.\"}\n","08/16/2025 19:55:47 - mods.testBench - INFO - Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:55:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:55:55 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:55:55 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8184213042259216], 'bs_recall': [0.8275244235992432], 'bs_f1': [0.8229476809501648], 'be_sim': array([0.7266333], dtype=float32), 'ce_sim': array([0.48129359]), 'title': 'Incorrect Material Label Applied', 'report': 'Incorrect material label applied to 1000 units of material due to incorrect material name. The material was identified as \"Teflon Insulation\" but was labeled as \"Teflon Spray\" in the batch printout. The warehouse operator, Alexandra Becker, made the mistake by selecting the wrong material name. The incident occurred in the Material Receiving Area. The affected labels were quarantined and the barcode scanner check was implemented to prevent further incidents. All affected labels were corrected and the batch was re-printed to ensure accuracy.'}\n","08/16/2025 19:55:55 - mods.testBench - INFO - Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:55:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:56:01 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:56:01 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8051936030387878], 'bs_recall': [0.7906896471977234], 'bs_f1': [0.797875702381134], 'be_sim': array([0.7624872], dtype=float32), 'ce_sim': array([0.33199373]), 'title': 'Incorrect Material Label Application', 'report': 'Corrected the material label, updated the batch, and conducted barcode scanner checks on the entire receiving area. This step was performed by Alexandra Becker, the Warehouse Operator. The affected labels were all marked with a look-alike/sound-alike material name. The corrected labels are now being distributed to the receiving area. The contingency actions include all affected labels being corrected, the batch being quarantined, and the barcode scanner being checked. The results of these actions will be monitored and further actions may be taken if necessary.'}\n","08/16/2025 19:56:01 - mods.testBench - INFO - Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:56:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:56:10 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:56:10 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8306419849395752], 'bs_recall': [0.9054561257362366], 'bs_f1': [0.8664370775222778], 'be_sim': array([0.8243901], dtype=float32), 'ce_sim': array([0.83205593]), 'title': 'Incorrect material label', 'report': 'On June 19, 2025, at 4:30 PM, incorrectly applied label to a batch of material. The warehouse operator, Alexandra Becker, selected a wrong label from batch printout instead of applying the correct one. This resulted in a material being incorrectly labeled. The label selected was sound-alike with multiple similar names, leading to confusion. The correct label is already available, but the wrong one was not re-selected. Correcting the issue now, all affected labels will be corrected, the batch will be quarantined, and barcode scanner checks will be implemented as contingency measures to prevent similar errors in the future.'}\n","08/16/2025 19:56:10 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:56:10 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:56:10 - mods.testBench - INFO - Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:56:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:56:13 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:56:13 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8997435569763184], 'bs_recall': [0.8282240629196167], 'bs_f1': [0.862503707408905], 'be_sim': array([0.76312286], dtype=float32), 'ce_sim': array([0.75632405]), 'title': 'Material Label Error', 'report': 'On June 19, 2025, at 4:30 PM, Alexandra Becker loaded the wrong material label during material receiving. The wrong label selected from batch printout was'}\n","08/16/2025 19:56:13 - mods.testBench - INFO - Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:56:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:56:20 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:56:20 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8527333736419678], 'bs_recall': [0.8886247873306274], 'bs_f1': [0.8703091740608215], 'be_sim': array([0.7219039], dtype=float32), 'ce_sim': array([0.82219803]), 'title': 'Incorrect Material Label', 'report': 'On June 19, 2025, at 4:30 PM, Alexandra Becker loaded the wrong material label during material receiving on Material Receiving Area for Batch RX500 of Neurocet 50 mg. Alexandra Becker from Warehouse was responsible for loading the material. The wrong label selected from batch printout was identified by Alexandra Becker during quality control. The material was quarantined and barcode scanner check was implemented to prevent future occurrences.'}\n","08/16/2025 19:56:20 - mods.testBench - INFO - Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:56:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:56:27 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:56:27 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8447219133377075], 'bs_recall': [0.8896830677986145], 'bs_f1': [0.8666196465492249], 'be_sim': array([0.73481065], dtype=float32), 'ce_sim': array([0.84377635]), 'title': 'Material Labeling Error', 'report': 'On June 19, 2025, at 4:30 PM, Alexandra Becker loaded the wrong material label during material receiving on Material Receiving Area for Batch RX500 of Neurocet 50 mg. The wrong label selected was a look-alike/sound-alike material name, and Alexandra Becker from Warehouse Operator discovered the issue. The line was stopped, batch quarantined, and barcode scanner check implemented.'}\n","08/16/2025 19:56:27 - mods.testBench - INFO - Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:56:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:56:32 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:56:32 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8315364718437195], 'bs_recall': [0.8762977719306946], 'bs_f1': [0.8533305525779724], 'be_sim': array([0.8062277], dtype=float32), 'ce_sim': array([0.85440606]), 'title': 'Material Label Error', 'report': 'On June 19, 2025, at 4:30 PM, Alexandra Becker loaded the wrong material label from batch printout during material receiving. Alexandra Becker from Warehouse, selected material name that look-alike/sound-alike with another material name, thus, incorrect material label applied. The batch was quarantined and barcode scanner was checked, but since the label was selected from a batch printout, rework was not initiated.'}\n","08/16/2025 19:56:32 - mods.testBench - INFO - Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:56:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:56:36 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:56:36 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7058209180831909], 'bs_recall': [0.7287839651107788], 'bs_f1': [0.7171186208724976], 'be_sim': array([0.36057776], dtype=float32), 'ce_sim': array([-0.62651545]), 'title': 'Error Report', 'report': 'On [date], at [time], [location], the following error occurred: [description]}. All affected materials are listed below: [list of materials].'}\n","08/16/2025 19:56:36 - mods.testBench - INFO - Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:56:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:56:37 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:56:37 - mods.testBench - INFO - results: {'report_idx': 4, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.4954163134098053], 'bs_recall': [0.49111825227737427], 'bs_f1': [0.49325791001319885], 'be_sim': array([0.08054829], dtype=float32), 'ce_sim': array([-0.77905113]), 'title': 'Corrected Batch Material Label', 'report': '{'}\n","08/16/2025 19:56:38 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:56:38 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:56:38 - mods.testBench - INFO - Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:56:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:56:44 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:56:44 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8377703428268433], 'bs_recall': [0.8626539707183838], 'bs_f1': [0.8500300645828247], 'be_sim': array([0.8800295], dtype=float32), 'ce_sim': array([0.70730019]), 'title': 'HPLC System Shutdown Report', 'report': 'The HPLC system was shut down due to an unexpected software crash during the analysis on June 20, 2025, at 1:50 PM. The incident occurred in the Quality Control Laboratory (QC Lab 1) and was attributed to an outdated software version. The cause of the crash was not provided, but the IT department was notified, and a software upgrade is planned to prevent similar incidents in the future.'}\n","08/16/2025 19:56:44 - mods.testBench - INFO - Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:56:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:56:51 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:56:51 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8420738577842712], 'bs_recall': [0.8650902509689331], 'bs_f1': [0.8534269332885742], 'be_sim': array([0.909312], dtype=float32), 'ce_sim': array([0.76213908]), 'title': 'HPLC System Shutdown', 'report': 'HPLC system shutdown during analysis on June 20, 2025, 1:50 PM in QC Lab 1 due to unexpected software crash mid-run. The unexpected software crash was caused by an outdated software version that was prone to instability. As a result, samples were re-injected and IT was notified to upgrade the software. The incident was investigated and corrective actions were taken to prevent similar incidents in the future.'}\n","08/16/2025 19:56:51 - mods.testBench - INFO - Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:56:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:56:56 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:56:56 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8545892238616943], 'bs_recall': [0.8851460814476013], 'bs_f1': [0.8695992827415466], 'be_sim': array([0.8807088], dtype=float32), 'ce_sim': array([0.75861216]), 'title': 'HPLC System Shutdown Report', 'report': 'HPLC system shutdown occurred during analysis on June 20, 2025, at 1:50 PM in the QC Lab 1. The unexpected software crash mid-run was caused by an outdated software version. The IT department was notified, and a software upgrade is planned. The samples were re-injected to ensure continuity of the analysis. The incident was documented, and appropriate measures were taken to prevent similar incidents in the future.'}\n","08/16/2025 19:56:56 - mods.testBench - INFO - Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:56:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:57:04 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:57:04 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8481385707855225], 'bs_recall': [0.8562712669372559], 'bs_f1': [0.8521855473518372], 'be_sim': array([0.8433838], dtype=float32), 'ce_sim': array([0.67449009]), 'title': 'HPLC System Shutdown', 'report': 'HPLC system shutdown occurred on June 20, 2025, 1:50 PM in QC Lab 1 due to an unexpected software crash during a run. The cause was an outdated software version, which is prone to instability. The incident led to the shutdown of the HPLC system. The IT department was notified, and a software upgrade is planned for the following week.'}\n","08/16/2025 19:57:04 - mods.testBench - INFO - Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:57:08 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:57:08 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8729168772697449], 'bs_recall': [0.8692525029182434], 'bs_f1': [0.8710808157920837], 'be_sim': array([0.85666525], dtype=float32), 'ce_sim': array([0.78207898]), 'title': 'HPLC System Shutdown Report', 'report': 'Unexpected software crash mid-run in HPLC system during analysis, QC Lab 1, June 20, 2025, 1:50 PM, Mohammed El-Sayed, QC Chemist. Samples re-injected and IT notification made.'}\n","08/16/2025 19:57:08 - mods.testBench - INFO - Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:57:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:57:19 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:57:19 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7439683675765991], 'bs_recall': [0.8404012322425842], 'bs_f1': [0.7892500758171082], 'be_sim': array([0.74218637], dtype=float32), 'ce_sim': array([0.69137353]), 'title': 'HPLC system shutdown during analysis due to unexpected software crash mid-run', 'report': 'Highly Specific Event Report Generating a Report from Input Fields - HPLC System System Analysis Event Analysis Time (HH:mm:ss): 00:01:02, Event Source: HPLC System Shutdown: HPLC Laboratory ( QC Lab 1), Event Description: Analysis halted after unexpected software crash during one-run cycle on June 20, 2025 at 1:50 PM, Analysis Tool: HPLC System, Event Type: System, Cause: Software Outlay, Event Time: Midnight, Location: Holographic Process Area, Operator: Mohammed El-Sayed, Role: Senior Chemist, Cause of Event: Older software, Resultant Event: Unexpected software crash, Contingency Action: Sample Re-injection, IT Support, New Software Version: Planned Upgrade,'}\n","08/16/2025 19:57:20 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:57:20 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:57:20 - mods.testBench - INFO - Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:57:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:57:26 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:57:26 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8503092527389526], 'bs_recall': [0.9045634269714355], 'bs_f1': [0.8765977025032043], 'be_sim': array([0.55646217], dtype=float32), 'ce_sim': array([0.75759882]), 'title': 'HPLC system shutdown', 'report': 'On June 20, 2025, at 1:50 PM, Mohammed El-Sayed from QC Lab 1 noticed an unexpected software crash during the analysis of Batch RX500 of Neurocet 50 mg. The software version was outdated, and the IT department was notified to upgrade the software. The samples were re-injected, and the IT department planned to upgrade the software. The incident was documented and reported to the lab manager.'}\n","08/16/2025 19:57:26 - mods.testBench - INFO - Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:57:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:57:32 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:57:32 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8877401351928711], 'bs_recall': [0.9107146263122559], 'bs_f1': [0.8990806341171265], 'be_sim': array([0.78154117], dtype=float32), 'ce_sim': array([0.76836026]), 'title': 'HPLC system shutdown', 'report': 'Unexpected software crash during analysis on June 20, 2025, at 1:50 PM, in QC Lab 1, by Mohammed El-Sayed, QC Chemist. The unexpected software crash was caused by an outdated software version that was prone to instability. The samples were re-injected, and IT was notified, with a planned software upgrade.'}\n","08/16/2025 19:57:32 - mods.testBench - INFO - Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:57:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:57:37 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:57:37 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8689013123512268], 'bs_recall': [0.8996118307113647], 'bs_f1': [0.8839899301528931], 'be_sim': array([0.759122], dtype=float32), 'ce_sim': array([0.75602221]), 'title': 'Unexpected Software Crash', 'report': 'On June 20, 2025, at 1:50 PM, Mohammed El-Sayed, QC Chemist, experienced an unexpected software crash during analysis in QC Lab 1. The incident was caused by an outdated software version, which was prone to instability. The IT department was notified, and a software upgrade plan was implemented to prevent similar incidents in the future.'}\n","08/16/2025 19:57:37 - mods.testBench - INFO - Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:57:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:57:43 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:57:43 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9167099595069885], 'bs_recall': [0.9167442321777344], 'bs_f1': [0.9167271852493286], 'be_sim': array([0.76453835], dtype=float32), 'ce_sim': array([0.77303296]), 'title': 'HPLC system shutdown', 'report': 'On June 20, 2025, at 1:50 PM, Mohammed El-Sayed re-injected the samples due to unexpected software crash during the analysis in QC Lab 1. The IT team was notified, and a software upgrade was planned to resolve the issue.'}\n","08/16/2025 19:57:43 - mods.testBench - INFO - Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:57:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:57:49 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:57:49 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8312817811965942], 'bs_recall': [0.8815301656723022], 'bs_f1': [0.8556689023971558], 'be_sim': array([0.7238312], dtype=float32), 'ce_sim': array([0.7250042]), 'title': 'HPLC System Shutdown', 'report': \"On June 20, 2025, at 1:50 PM, Mohammed El-Sayed, QC Chemist, experienced an unexpected software crash during the analysis. The lab was notified, and the IT team was involved in resolving the issue, which required an immediate software upgrade to prevent any further disruptions. The shutdown was short-lived, but the incident highlighted the need for regular software updates to maintain the system's reliability.\"}\n","08/16/2025 19:57:49 - mods.testBench - INFO - Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:57:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:57:53 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:57:53 - mods.testBench - INFO - results: {'report_idx': 5, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8872756958007812], 'bs_recall': [0.8883454203605652], 'bs_f1': [0.8878101706504822], 'be_sim': array([0.9040684], dtype=float32), 'ce_sim': array([0.83374572]), 'title': 'HPLC system shutdown,', 'report': 'On June 20, 2025, at 1:50 PM, the HPLC system experienced an unexpected software crash during analysis in the QC Lab 1. Mohammed El-Sayed, the acting QC Chemist, promptly took corrective actions to rectify the issue.'}\n","08/16/2025 19:57:53 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:57:53 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:57:53 - mods.testBench - INFO - Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:57:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:58:00 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:58:00 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8775914907455444], 'bs_recall': [0.8831573724746704], 'bs_f1': [0.8803656697273254], 'be_sim': array([0.73312396], dtype=float32), 'ce_sim': array([0.77567029]), 'title': 'Spillage of Active Material during Weighing', 'report': 'Spillage of active material during weighing occurred on June 22, 2025, 10:05 AM at Dispensing Booth 1, Production Building 3. Lucia Conti, Production Technician, was involved in the incident. The event was contained, and the area was decontaminated. A revised PPE replacement frequency was implemented.'}\n","08/16/2025 19:58:00 - mods.testBench - INFO - Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:58:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:58:03 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:58:03 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8888261914253235], 'bs_recall': [0.8155741095542908], 'bs_f1': [0.8506260514259338], 'be_sim': array([0.65494496], dtype=float32), 'ce_sim': array([0.53630805]), 'title': 'Report', 'report': 'Spillage of active material during weighing at Dispensing Booth 1, Production Building 3 on June 22, 2025 at 10:05 AM.'}\n","08/16/2025 19:58:03 - mods.testBench - INFO - Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:58:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:58:11 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:58:11 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8551112413406372], 'bs_recall': [0.9032832384109497], 'bs_f1': [0.8785374164581299], 'be_sim': array([0.79899746], dtype=float32), 'ce_sim': array([0.85467893]), 'title': 'Spillage of Active Material during Weighing', 'report': \"Spillage of active material during weighing occurred on June 22, 2025, at 10:05 AM at Dispensing Booth 1, Production Building 3. Lucia Conti, a Production Technician, was involved in the incident. The spillage was caused by a worn glove that reduced the operator's grip, leading to the improper transfer of the active material. The incident was contained and decontaminated, and the production technician's PPE was replaced as per the revised frequency.\"}\n","08/16/2025 19:58:11 - mods.testBench - INFO - Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:58:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:58:18 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:58:18 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8177517056465149], 'bs_recall': [0.8582473993301392], 'bs_f1': [0.8375102877616882], 'be_sim': array([0.80597806], dtype=float32), 'ce_sim': array([0.70239896]), 'title': 'Spillage of Active Material during Weighing', 'report': \"Spillage of active material occurred during the weighing of a product in the Dispensing Booth 1, Production Building 3 at 10:05 AM. Lucia Conti, Production Technician reported this incident. The incident involved the scoop tool slipping during transfer, which resulted in a spill of active material. The incident was reported due to the worn glove not being replaced on time, which reduced the tool's grip. As a result, the containment measures were not effective. To prevent a similar incident in the future, the PPE replacement frequency will be revised.\"}\n","08/16/2025 19:58:18 - mods.testBench - INFO - Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:58:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:58:22 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:58:22 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8548071384429932], 'bs_recall': [0.7687600255012512], 'bs_f1': [0.809503436088562], 'be_sim': array([0.6977465], dtype=float32), 'ce_sim': array([0.2037106]), 'title': 'Report on Spillage of Active Material during Weighing', 'report': 'active material spilled in the dispensing booth 1, production building 3, due to unhygienic glove use.'}\n","08/16/2025 19:58:22 - mods.testBench - INFO - Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:58:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:58:28 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:58:28 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8633750081062317], 'bs_recall': [0.8871297836303711], 'bs_f1': [0.8750912547111511], 'be_sim': array([0.7985168], dtype=float32), 'ce_sim': array([0.62030822]), 'title': 'Spillage Report', 'report': 'Spillage of active material occurred during transfer of substance at Dispensing Booth 1, Production Building 3 on June 22, 2025 at 10:05 AM due to inadequate maintenance and usage of reusable handling tool, resulting from a failure to replace worn glove. The spillage was contained, decontaminated, and appropriate protective personal equipment (PPE) replacement frequency was revised.'}\n","08/16/2025 19:58:29 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:58:29 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:58:29 - mods.testBench - INFO - Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:58:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:58:34 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:58:34 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8695480227470398], 'bs_recall': [0.9039556980133057], 'bs_f1': [0.8864181041717529], 'be_sim': array([0.85728383], dtype=float32), 'ce_sim': array([0.87770194]), 'title': 'Event Report', 'report': 'On June 22, 2025, at 10:05 AM, Lucia Conti from Production Technician, scooped material during transfer, resulting in a spillage of active material. The incident occurred in Dispensing Booth 1, Production Building 3. The cause was attributed to worn glove, not replaced timely. The spillage was contained, and the area was decontaminated. The PPE replacement frequency was revised.'}\n","08/16/2025 19:58:34 - mods.testBench - INFO - Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:58:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:58:40 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:58:40 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.882514238357544], 'bs_recall': [0.8513849973678589], 'bs_f1': [0.8666701912879944], 'be_sim': array([0.7830403], dtype=float32), 'ce_sim': array([0.73798633]), 'title': 'Spillage of active material during weighing', 'report': 'On June 22, 2025, at 10:05 AM, Lucia Conti from Production Technician identified the spillage and contained the active material. The area was decontaminated and PPE was replaced. The frequency of PPE replacement was revised to prevent future incidents.'}\n","08/16/2025 19:58:40 - mods.testBench - INFO - Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:58:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:58:45 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:58:45 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8778713345527649], 'bs_recall': [0.8565977811813354], 'bs_f1': [0.8671040534973145], 'be_sim': array([0.69258434], dtype=float32), 'ce_sim': array([0.64992988]), 'title': 'Incident Report', 'report': 'On June 22, 2025, at 10:05 AM, Lucia Conti, Production Technician, scooped material during transfer, resulting in spillage during weighing. The incident was contained, and the area was decontaminated. Lucia Conti received PPE replacement training, and the frequency of PPE replacement was revised.'}\n","08/16/2025 19:58:45 - mods.testBench - INFO - Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:58:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:58:52 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:58:52 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.870893120765686], 'bs_recall': [0.901096522808075], 'bs_f1': [0.885737419128418], 'be_sim': array([0.83425754], dtype=float32), 'ce_sim': array([0.82030201]), 'title': 'Spillage during Weighing', 'report': \"On June 22, 2025, at 10:05 AM, Lucia Conti from Production Technician, was involved in a spillage during the production process at Dispensing Booth 1, Production Building 3. The spill contained active material, and Lucia Conti's worn glove was the cause of the accident. The spill was contained, and the area was decontaminated. The PPE was replaced, and the frequency was revised.\"}\n","08/16/2025 19:58:52 - mods.testBench - INFO - Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:58:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:58:55 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:58:55 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8405635356903076], 'bs_recall': [0.7566040754318237], 'bs_f1': [0.7963770627975464], 'be_sim': array([0.59817404], dtype=float32), 'ce_sim': array([-0.69076139]), 'title': 'Contingency Actions Report', 'report': 'The following contingency actions were taken due to the spillage: The spill was contained, and the area was decontaminated. The PPE replacement frequency was revised.'}\n","08/16/2025 19:58:55 - mods.testBench - INFO - Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:58:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:02 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:02 - mods.testBench - INFO - results: {'report_idx': 6, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8146875500679016], 'bs_recall': [0.8605760931968689], 'bs_f1': [0.8370033502578735], 'be_sim': array([0.36355278], dtype=float32), 'ce_sim': array([0.64047486]), 'title': 'Wrong tablet counting', 'report': 'On June 22, 2025, at 10:05 AM, Lucia Conti (Production Technician) loaded the wrong tablet counting disk during changeover on Dispensing Booth 1 for Batch RX500 of Neurocet 50 mg. Sarah Yoon from QA and Erik Hansen (Packaging Operator) independently uncovered the issue. The line suffered downtime, 300 bottles were segregated, and operator retraining was initiated.'}\n","08/16/2025 19:59:02 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:59:02 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:59:02 - mods.testBench - INFO - Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:07 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:07 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.882212221622467], 'bs_recall': [0.8730172514915466], 'bs_f1': [0.8775906562805176], 'be_sim': array([0.9308344], dtype=float32), 'ce_sim': array([0.84790605]), 'title': 'OOS Result for Dissolution Test', 'report': 'OOS Result for Dissolution Test Report: June 24, 2025, 3:30 PM, QC Lab 2, Alina Novak, QC Analyst, Tablet coating thickness variation, Batch on hold, retesting initiated, manufacturing records reviewed.'}\n","08/16/2025 19:59:07 - mods.testBench - INFO - Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:11 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:11 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8440110683441162], 'bs_recall': [0.7995543479919434], 'bs_f1': [0.8211814165115356], 'be_sim': array([0.7289497], dtype=float32), 'ce_sim': array([-0.03601626]), 'title': 'OOS Result for Dissolution Test', 'report': 'The dissolution test result for tablet coating failed to meet the release criteria. The tablet coating thickness variation was identified as the cause of the failure. The batch was placed on hold, and retesting will be initiated to ensure the quality of the tablet coating. The manufacturing records will be reviewed to identify any potential issues.'}\n","08/16/2025 19:59:11 - mods.testBench - INFO - Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:13 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:13 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8588608503341675], 'bs_recall': [0.7561622858047485], 'bs_f1': [0.8042463064193726], 'be_sim': array([0.7546116], dtype=float32), 'ce_sim': array([0.05320072]), 'title': 'OOS Report', 'report': 'OOS result for dissolution test: Batch on hold, retesting initiated, manufacturing records reviewed.'}\n","08/16/2025 19:59:13 - mods.testBench - INFO - Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:18 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:18 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8768551349639893], 'bs_recall': [0.8777503967285156], 'bs_f1': [0.8773025274276733], 'be_sim': array([0.94245297], dtype=float32), 'ce_sim': array([0.86013681]), 'title': 'Out of Specification Report', 'report': 'OOS: Dissolution Test - June 24, 2025, 3:30 PM, QC Lab 2, Alina Novak, QC Analyst, Tablet coating thickness variation, Result failed to meet release criteria, Batch on hold, retesting initiated, manufacturing records reviewed.'}\n","08/16/2025 19:59:18 - mods.testBench - INFO - Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:22 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:22 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.883379340171814], 'bs_recall': [0.8312629461288452], 'bs_f1': [0.8565291166305542], 'be_sim': array([0.91961265], dtype=float32), 'ce_sim': array([0.83696735]), 'title': 'OOS Result Report', 'report': 'OOS Result for Dissolution Test June 24, 2025, 3:30 PM, QC Lab 2, Alina Novak, QC Analyst, Tablet Coating Thickness Variation.'}\n","08/16/2025 19:59:22 - mods.testBench - INFO - Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:27 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:27 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8775965571403503], 'bs_recall': [0.898817777633667], 'bs_f1': [0.8880804181098938], 'be_sim': array([0.81991494], dtype=float32), 'ce_sim': array([0.7080192]), 'title': 'Dissolution Test Report', 'report': 'The result of the dissolution test on June 24, 2025, at 3:30 PM, QC Lab 2, revealed a failed to meet the release criteria. In order to fulfill the requirements, the batch was put on hold and retesting of the batch was initiated. Furthermore, the manufacturing records were reviewed to identify the reason of the variation in tablet coating thickness.'}\n","08/16/2025 19:59:27 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:59:27 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:59:27 - mods.testBench - INFO - Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:32 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:32 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9279844760894775], 'bs_recall': [0.9236809015274048], 'bs_f1': [0.92582768201828], 'be_sim': array([0.9552509], dtype=float32), 'ce_sim': array([0.94809043]), 'title': 'OOS Result', 'report': 'On June 24, 2025, at 3:30 PM, Alina Novak from QC Analyst discovered the OOS result for dissolution test in QC Lab 2. The batch was on hold, and retesting was initiated to reevaluate the tablet coating thickness variation.'}\n","08/16/2025 19:59:32 - mods.testBench - INFO - Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:36 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:36 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9432998895645142], 'bs_recall': [0.9283496141433716], 'bs_f1': [0.9357650279998779], 'be_sim': array([0.8406673], dtype=float32), 'ce_sim': array([0.88676989]), 'title': 'OOS Result', 'report': 'On June 24, 2025, at 3:30 PM, Alina Novak from QC Analyst discovered the OOS result for dissolution test in QC Lab 2. The batch was put on hold, and retesting was initiated to review manufacturing records.'}\n","08/16/2025 19:59:36 - mods.testBench - INFO - Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:41 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:41 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9359833598136902], 'bs_recall': [0.9383856654167175], 'bs_f1': [0.9371829628944397], 'be_sim': array([0.93250036], dtype=float32), 'ce_sim': array([0.96345723]), 'title': 'OOS Result', 'report': 'On June 24, 2025, at 3:30 PM, Alina Novak from QC Lab 2 discovered the OOS result for dissolution test due to tablet coating thickness variation. The batch was put on hold and retesting was initiated to ensure compliance with release criteria.'}\n","08/16/2025 19:59:41 - mods.testBench - INFO - Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:47 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:47 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9451436400413513], 'bs_recall': [0.9365774989128113], 'bs_f1': [0.9408410787582397], 'be_sim': array([0.958683], dtype=float32), 'ce_sim': array([0.95765853]), 'title': 'Out-of-Stock Result', 'report': 'On June 24, 2025, at 3:30 PM, Alina Novak, QC Analyst, discovered the OOS result for dissolution test in QC Lab 2 due to tablet coating thickness variation. The batch was put on hold and retesting was initiated.'}\n","08/16/2025 19:59:47 - mods.testBench - INFO - Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:50 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:50 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.894397497177124], 'bs_recall': [0.8714239597320557], 'bs_f1': [0.8827612400054932], 'be_sim': array([0.7910368], dtype=float32), 'ce_sim': array([0.68423736]), 'title': 'Out of Specification (OOS) Result', 'report': 'On June 24, 2025, at 3:30 PM, Alina Novak, QC Analyst, discovered the result failed to meet the release criteria. The tablet coating thickness variation was identified during testing in QC Lab 2.'}\n","08/16/2025 19:59:50 - mods.testBench - INFO - Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 19:59:56 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 19:59:56 - mods.testBench - INFO - results: {'report_idx': 7, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7426025867462158], 'bs_recall': [0.8335967063903809], 'bs_f1': [0.785473108291626], 'be_sim': array([0.5730724], dtype=float32), 'ce_sim': array([0.42248929]), 'title': 'OOS result for dissolution test', 'report': '{\"what\": \"OOS (Out of Specification) result\", \"when\": \"June 24, 2025, 3:30 PM\", \"where\": \"QC Lab 2\", \"how\": \"Result failed to meet release criteria\", \"why\": \"Tablet coating thickness variation\", \"contingency actions\": \"Batch on hold, retesting initiated, manufacturing records reviewed\"}'}\n","08/16/2025 19:59:56 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 19:59:56 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 19:59:56 - mods.testBench - INFO - Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 19:59:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:00:15 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1524 [type=json_invalid, input_value='{\"title\": \"Filter Integr...nel have been retrained', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/16/2025 20:00:15 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:00:15 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7282581329345703], 'bs_recall': [0.6752952337265015], 'bs_f1': [0.7007774114608765], 'be_sim': array([0.21253794], dtype=float32), 'ce_sim': array([-0.55627882]), 'title': 'NO PYDANTIC TITLE', 'report': 'NO PYDANTIC REPORT'}\n","08/16/2025 20:00:15 - mods.testBench - INFO - Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:00:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:00:20 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:00:20 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8774794340133667], 'bs_recall': [0.9209021329879761], 'bs_f1': [0.8986665606498718], 'be_sim': array([0.7763263], dtype=float32), 'ce_sim': array([0.87077767]), 'title': 'Filter Integrity Test Report', 'report': 'Filter integrity test failed post-sterilization in Sterile Filtration Area on June 25, 2025, 5:00 PM. The filter was damaged during installation and was rejected due to a bubble point below the acceptance threshold. The batch was then batch rejected and the filter handling SOP was updated. Personnel retrained. The test was conducted by Peter Ruan, Process Engineer.'}\n","08/16/2025 20:00:20 - mods.testBench - INFO - Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:00:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:00:28 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:00:28 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8321685791015625], 'bs_recall': [0.8196485638618469], 'bs_f1': [0.8258610963821411], 'be_sim': array([0.7443459], dtype=float32), 'ce_sim': array([0.11350384]), 'title': 'Filter Integrity Test Report', 'report': 'The Filter Integrity Test failed post-sterilization due to damage during installation. The filter was found to have a bubble point below the acceptance threshold. The cause of the damage is attributed to the filter being damaged during the installation process. As a result, the filter was rejected and a new one was placed in the batch. The Filter Handling SOP has been updated to include this incident, and personnel have been retrained on the new procedure.'}\n","08/16/2025 20:00:28 - mods.testBench - INFO - Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:00:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:00:32 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:00:32 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8545889854431152], 'bs_recall': [0.813443660736084], 'bs_f1': [0.8335088491439819], 'be_sim': array([0.7623239], dtype=float32), 'ce_sim': array([0.01099421]), 'title': 'Filter Integrity Test Result Report', 'report': 'Filter integrity test failed post-sterilization due to damage during installation. The filter was rejected and batch was impacted. Filter handling SOP has been updated, and personnel has been retrained to handle such issues in the future.'}\n","08/16/2025 20:00:32 - mods.testBench - INFO - Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:00:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:00:42 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:00:42 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8217429518699646], 'bs_recall': [0.9032806158065796], 'bs_f1': [0.8605847954750061], 'be_sim': array([0.7836683], dtype=float32), 'ce_sim': array([0.90517312]), 'title': 'Filter Integrity Test Report', 'report': \"This report summarizes the results of a filter integrity test performed on the sterile filtration area on June 25, 2025, at 5:00 PM. The filter, which was installed on June 25, 2025, 8:00 AM, failed the integrity test due to damage during installation. The test was conducted by Peter Ruan, Process Engineer, and was performed in the Sterile Filtration Area. The filter's bubble point, which is used to measure its integrity, was below the acceptance threshold at the time of the test. The filter's damage was confirmed by visual inspection and confirmed by testing. Therefore, the batch was rejected, and the filter handling SOP was updated. Furthermore, personnel were retrained to handle filters in the Sterile Filtration Area.\"}\n","08/16/2025 20:00:42 - mods.testBench - INFO - Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:00:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:00:50 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:00:51 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7710057497024536], 'bs_recall': [0.7850179672241211], 'bs_f1': [0.7779487371444702], 'be_sim': array([0.75627553], dtype=float32), 'ce_sim': array([0.03151013]), 'title': 'Filter Integrity Test Report', 'report': 'The Filter Integrity Test report details the failure to filter integrity after sterilization post-sterilization as mentioned above. The test was failed when due to filter damage during the installation, it is essential for the facility operators to update their handling SOP accordingly to guarantee a steady supply of sterilized filtrate. The updated SOP must also include the implementation of corrective measures to prevent the filter damage in the future, such as training personnel on filter maintenance, regular inspecting the filter, and implementing additional safety protocols on this particular filter. If the filter is not properly maintained, it might pose a threat to the overall filtration process.'}\n","08/16/2025 20:00:51 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 20:00:51 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 20:00:51 - mods.testBench - INFO - Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:00:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:00:56 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:00:56 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8978030681610107], 'bs_recall': [0.9128490686416626], 'bs_f1': [0.9052635431289673], 'be_sim': array([0.8006835], dtype=float32), 'ce_sim': array([0.74160182]), 'title': 'Filter Integrity Test Result', 'report': 'On June 25, 2025, at 5:00 PM, Peter Ruan from Process Engineer identified the filter damage during installation. The filter was rejected due to the low bubble point below the acceptance threshold. The process engineer updated the filter handling SOP, and personnel were retrained to handle the damaged filter.'}\n","08/16/2025 20:00:56 - mods.testBench - INFO - Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:00:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:01:01 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:01:01 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8820939064025879], 'bs_recall': [0.9309544563293457], 'bs_f1': [0.9058657884597778], 'be_sim': array([0.88045436], dtype=float32), 'ce_sim': array([0.88203442]), 'title': 'Filter Integrity Test', 'report': 'On June 25, 2025, at 5:00 PM, Peter Ruan, Process Engineer, initiated the filter integrity test in the Sterile Filtration Area. The test failed to meet the acceptance threshold due to damage during installation. The filter was rejected, and the Standard Operating Procedure (SOP) for filter handling was updated. Peter Ruan was retrained on the new SOP.'}\n","08/16/2025 20:01:01 - mods.testBench - INFO - Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:01:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:01:09 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:01:09 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8971531987190247], 'bs_recall': [0.9251804947853088], 'bs_f1': [0.9109513163566589], 'be_sim': array([0.8848332], dtype=float32), 'ce_sim': array([0.79107487]), 'title': 'Filter Integrity Test Result', 'report': 'On June 25, 2025, at 5:00 PM, Peter Ruan, Process Engineer, discovered that the filter failed the integrity test due to damage during installation. The filter was rejected and a new one was placed in the batch. The SOP was updated to reflect the damage, and personnel were retrained on the proper handling of filters.'}\n","08/16/2025 20:01:09 - mods.testBench - INFO - Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:01:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:01:12 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:01:12 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9251346588134766], 'bs_recall': [0.8921561241149902], 'bs_f1': [0.9083461165428162], 'be_sim': array([0.7974531], dtype=float32), 'ce_sim': array([0.64000911]), 'title': 'Filter Integrity Test Result', 'report': 'On June 25, 2025, at 5:00 PM, Peter Ruan retrained the process engineer after filter damage during installation. The batch was rejected and a new batch was taken to filter.'}\n","08/16/2025 20:01:12 - mods.testBench - INFO - Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:01:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:01:18 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:01:18 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8658449649810791], 'bs_recall': [0.91215580701828], 'bs_f1': [0.888397216796875], 'be_sim': array([0.88184065], dtype=float32), 'ce_sim': array([0.86221206]), 'title': 'Filter Integrity Test', 'report': 'On June 25, 2025, at 5:00 PM, Peter Ruan inspected the filter during process validation, and he found that it did not meet the filter integrity test requirements. He identified that the filter had a bubble point below the acceptance threshold. The filter was damaged during its installation process. He decided to reject the batch and update the Sterile Filtration Area handling SOP. Additionally, he trained personnel on filter maintenance procedures.'}\n","08/16/2025 20:01:18 - mods.testBench - INFO - Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:01:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:01:24 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:01:24 - mods.testBench - INFO - results: {'report_idx': 8, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8436571359634399], 'bs_recall': [0.8635727763175964], 'bs_f1': [0.8534988164901733], 'be_sim': array([0.7268744], dtype=float32), 'ce_sim': array([0.758255]), 'title': 'Filter integrity test passed successfully', 'report': 'On June 25, 2025, at 5:00 PM, Peter Ruan successfully performed the filter integrity test on the Filter integrity test failed post-sterilization for Batch RX3000. The test passed with no discrepancies and the batch was deemed clean and ready for packaging.'}\n","08/16/2025 20:01:24 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 20:01:24 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 20:01:24 - mods.testBench - INFO - Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:01:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:01:29 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:01:29 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.90846848487854], 'bs_recall': [0.9109044075012207], 'bs_f1': [0.9096848368644714], 'be_sim': array([0.9178405], dtype=float32), 'ce_sim': array([0.89711118]), 'title': 'Deviation from Cleaning Schedule', 'report': 'The cleaning activity was missed for one equipment in Production Suite C on June 27, 2025, 8:00 AM. The schedule was not updated after equipment maintenance, leading to a deviation from the cleaning schedule. The QA team was notified and the area was re-cleaned to correct the deviation.'}\n","08/16/2025 20:01:29 - mods.testBench - INFO - Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:01:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:01:35 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:01:35 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8798861503601074], 'bs_recall': [0.9066718816757202], 'bs_f1': [0.8930782675743103], 'be_sim': array([0.8679711], dtype=float32), 'ce_sim': array([0.81504554]), 'title': 'Deviation Report', 'report': 'The deviation from the cleaning schedule in Production Suite C occurred on June 27, 2025, at 8:00 AM. The equipment was not updated in the schedule after maintenance, resulting in a missed cleaning activity. The QA team was notified during the routine audit. The area was re-cleaned and the schedule corrected. The QA team was notified and scheduled to perform a re-audit to ensure compliance.'}\n","08/16/2025 20:01:35 - mods.testBench - INFO - Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:01:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:01:39 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:01:39 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7122712135314941], 'bs_recall': [0.7013897895812988], 'bs_f1': [0.7067885994911194], 'be_sim': array([0.077773], dtype=float32), 'ce_sim': array([-0.71495628]), 'title': 'Deviation Report', 'report': 'What, when, where, who, how, and why are the key elements of the report. The report should be presented in a clear and concise manner, and should not include any information that is not provided. The report should be formatted as JSON.'}\n","08/16/2025 20:01:39 - mods.testBench - INFO - Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:01:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:01:47 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:01:47 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8670085072517395], 'bs_recall': [0.9038485884666443], 'bs_f1': [0.8850453495979309], 'be_sim': array([0.8700842], dtype=float32), 'ce_sim': array([0.84248817]), 'title': 'Deviation from Cleaning Schedule', 'report': 'Background information: On June 27, 2025, at 8:00 AM, the production suite C was not cleaned as per the scheduled cleaning activity. The deviation was due to a missed cleaning activity that occurred after equipment maintenance. This incident was monitored by QA during a routine audit. The schedule was not updated after the equipment maintenance, leading to the missed cleaning. The QA team was notified and area re-cleaned and corrected, ensuring that the schedule was updated and all necessary tasks were completed.'}\n","08/16/2025 20:01:47 - mods.testBench - INFO - Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:01:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:01:58 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:01:58 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7909218072891235], 'bs_recall': [0.854403555393219], 'bs_f1': [0.8214380145072937], 'be_sim': array([0.7148454], dtype=float32), 'ce_sim': array([0.70812881]), 'title': 'Deviation from Cleaning Schedule', 'report': 'What: Deviation from cleaning schedule occurred in Production Suite C on June 27, 2025, 8:00 AM. This occurred because the schedule was not updated after equipment maintenance. The deviation is expected to be corrected in the next schedule update. The area affected is Production Suite C. The cause of the deviation was the schedule not being updated after equipment maintenance. The deviation is expected to be corrected in the next schedule update. The affected area is Production Suite C. The consequence of the deviation is that the equipment maintenance schedule was not updated. The area re-cleaned to correct the deviation, and QA was notified. The outcome is expected to be a correct schedule update for the equipment maintenance, and that QA will be notified to ensure the process is correct and up-to-date. The area is Production Suite C. The contingency actions are Area re-cleaned, schedule corrected, QA notified.'}\n","08/16/2025 20:01:58 - mods.testBench - INFO - Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:01:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:05 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:05 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8830929398536682], 'bs_recall': [0.9040719270706177], 'bs_f1': [0.8934592604637146], 'be_sim': array([0.6228667], dtype=float32), 'ce_sim': array([0.65375602]), 'title': 'Deviation from Cleaning Schedule', 'report': 'Deviation occurred on June 27, 2025, 8:00 AM in Production Suite C. It was monitored by QA during routine audit. The deviation resulted from not maintaining a schedule after equipment maintenance. The correct schedule was not updated appropriately. The area was re-cleaned, QA was notified, and a new schedule was set.}.'}\n","08/16/2025 20:02:06 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 20:02:06 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 20:02:06 - mods.testBench - INFO - Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:10 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:10 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9017889499664307], 'bs_recall': [0.9011270999908447], 'bs_f1': [0.9014579057693481], 'be_sim': array([0.9298543], dtype=float32), 'ce_sim': array([0.87486339]), 'title': 'Deviation from cleaning schedule', 'report': 'On June 27, 2025, at 8:00 AM, QA monitored the cleaning activity in Production Suite C. The cleaning activity missed one equipment, which resulted in a deviation from the cleaning schedule. The schedule was not updated after the equipment maintenance, leading to the missed cleaning. QA corrected the schedule and notified the affected area.'}\n","08/16/2025 20:02:10 - mods.testBench - INFO - Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:15 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:15 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.6443231105804443], 'bs_recall': [0.7194344997406006], 'bs_f1': [0.6798103451728821], 'be_sim': array([0.16715889], dtype=float32), 'ce_sim': array([-0.6049881]), 'title': 'Event Report', 'report': 'On {date} at {time}, {event_name} ({event_type}) occurred. The following information was verified: {what}, {when}, {who}, {how}, {why}, and {contingency_actions}.'}\n","08/16/2025 20:02:15 - mods.testBench - INFO - Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:20 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:20 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.913498044013977], 'bs_recall': [0.9304547309875488], 'bs_f1': [0.9218984246253967], 'be_sim': array([0.83250844], dtype=float32), 'ce_sim': array([0.85840178]), 'title': 'Report', 'report': 'On June 27, 2025, at 8:00 AM, a deviation from the cleaning schedule occurred during routine audit in Production Suite C. A cleaning activity was missed for one equipment. The schedule was not updated after the equipment maintenance, leading to the missed cleaning. The area was re-cleaned, and the schedule was corrected. QA was notified.'}\n","08/16/2025 20:02:20 - mods.testBench - INFO - Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:26 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:26 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9070152044296265], 'bs_recall': [0.9195361137390137], 'bs_f1': [0.9132327437400818], 'be_sim': array([0.88226163], dtype=float32), 'ce_sim': array([0.80657512]), 'title': 'Report', 'report': 'On June 27, 2025, at 8:00 AM, a deviation from the cleaning schedule was noticed in Production Suite C. QA monitored during routine audit, which identified the issue. The schedule was not updated after equipment maintenance, leading to the missed cleaning activity. The area was re-cleaned, and the schedule was corrected, with QA being notified.'}\n","08/16/2025 20:02:26 - mods.testBench - INFO - Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:32 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:32 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7056268453598022], 'bs_recall': [0.7421172857284546], 'bs_f1': [0.7234121561050415], 'be_sim': array([0.18121912], dtype=float32), 'ce_sim': array([-0.72844368]), 'title': 'Report', 'report': 'On [date], at [time], a [location] deviated from a [schedule] by [duration] minutes. A [person] monitored [activity] during [type of audit]. The deviation resulted in [result]. A [number] of [actions] were taken to address the issue. In the event of similar occurrences, it is recommended to [recommendation].'}\n","08/16/2025 20:02:32 - mods.testBench - INFO - Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:32 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:32 - mods.testBench - INFO - results: {'report_idx': 9, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.48649898171424866], 'bs_recall': [0.48669368028640747], 'bs_f1': [0.4865962862968445], 'be_sim': array([0.05315766], dtype=float32), 'ce_sim': array([-0.50264138]), 'title': '', 'report': '{'}\n","08/16/2025 20:02:33 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 20:02:33 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 20:02:33 - mods.testBench - INFO - Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:40 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:40 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8256518244743347], 'bs_recall': [0.854941725730896], 'bs_f1': [0.8400415778160095], 'be_sim': array([0.7931258], dtype=float32), 'ce_sim': array([0.7532807]), 'title': 'Unidentified Particulate Matter in Vial Post-Filling', 'report': 'Unidentified particulate matter observed in vial post-filling on June 28, 2025, 4:15 PM at Visual Inspection Line, Sterile Manufacturing. The root cause of this incident is not yet determined and the investigation is ongoing. The particulate matter is not present during formulation and was only observed post-filling. The incident is being investigated and contingency actions are being taken. The particulate matter is being sent to the lab for ID analysis. The incident is being investigated and contingency actions are being taken.'}\n","08/16/2025 20:02:40 - mods.testBench - INFO - Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:46 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:46 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'B', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8838913440704346], 'bs_recall': [0.8839714527130127], 'bs_f1': [0.8839313983917236], 'be_sim': array([0.81361663], dtype=float32), 'ce_sim': array([0.77452034]), 'title': 'Unidentified Particulate Matter Observation', 'report': 'Unidentified particulate matter observed in vial post-filling on June 28, 2025, 4:15 PM at Visual Inspection Line, Sterile Manufacturing. The cause of the observation is not yet determined, and the investigation is ongoing. Vial has been segregated and batch has been placed on hold. Particulate matter has been sent to the lab for analysis.'}\n","08/16/2025 20:02:46 - mods.testBench - INFO - Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:50 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:50 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8712608814239502], 'bs_recall': [0.8575714230537415], 'bs_f1': [0.8643619418144226], 'be_sim': array([0.78005], dtype=float32), 'ce_sim': array([0.77615941]), 'title': 'Report', 'report': 'Unidentified particulate matter observed in vial post-filling on June 28, 2025, 4:15 PM, at Visual Inspection Line, Sterile Manufacturing. The cause of the observation is unknown and the investigation is ongoing. Vial segregation and batch hold have been implemented as contingency actions.'}\n","08/16/2025 20:02:50 - mods.testBench - INFO - Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:53 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:54 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'B', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8829786777496338], 'bs_recall': [0.8185821771621704], 'bs_f1': [0.849561870098114], 'be_sim': array([0.7959404], dtype=float32), 'ce_sim': array([0.75064057]), 'title': 'Observation Report', 'report': 'Unidentified Particulate Matter Observed in Vial Post-Filling on June 28, 2025, 4:15 PM, at Visual Inspection Line, Sterile Manufacturing.'}\n","08/16/2025 20:02:54 - mods.testBench - INFO - Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:56 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:56 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.7202475666999817], 'bs_recall': [0.6392785310745239], 'bs_f1': [0.6773518919944763], 'be_sim': array([0.2773568], dtype=float32), 'ce_sim': array([-0.43595159]), 'title': 'Report: Unidentified Particulate Matter Observed in Vial Post-Filling', 'report': 'The report describes the events and actions taken following the incident.'}\n","08/16/2025 20:02:56 - mods.testBench - INFO - Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:02:57 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:02:57 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'B', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.699426531791687], 'bs_recall': [0.6397452354431152], 'bs_f1': [0.6682559847831726], 'be_sim': array([0.13665198], dtype=float32), 'ce_sim': array([-0.46332121]), 'title': 'Report for Identified Event', 'report': 'Your report description.'}\n","08/16/2025 20:02:57 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6/generation_config.json\n","08/16/2025 20:02:57 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 128000,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    128001,\n","    128008,\n","    128009\n","  ],\n","  \"temperature\": 0.6,\n","  \"top_p\": 0.9\n","}\n","\n","08/16/2025 20:02:57 - mods.testBench - INFO - Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:02:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:03:03 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:03:03 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9220151305198669], 'bs_recall': [0.9334357380867004], 'bs_f1': [0.927690327167511], 'be_sim': array([0.88919055], dtype=float32), 'ce_sim': array([0.96158421]), 'title': 'Observation Report', 'report': 'On June 28, 2025, at 4:15 PM, Karen Lee from QA observed unidentified particulate matter in vial post-filling during Visual Inspection Line for Sterile Manufacturing. The particles were not present during formulation. The root cause of the issue is unknown and investigation is ongoing. Vial was segregated, batch on hold, and particulate was sent to lab for ID analysis.'}\n","08/16/2025 20:03:03 - mods.testBench - INFO - Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:03:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:03:07 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:03:07 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'C', 'temperature': 0.5, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9062862992286682], 'bs_recall': [0.8648724555969238], 'bs_f1': [0.8850952386856079], 'be_sim': array([0.9056509], dtype=float32), 'ce_sim': array([0.85680103]), 'title': 'Observation Report', 'report': 'On June 28, 2025, at 4:15 PM, Karen Lee from QA Inspector observed the presence of unidentified particulate matter in a post-filled vial. The observation was noted due to the investigation ongoing.'}\n","08/16/2025 20:03:07 - mods.testBench - INFO - Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:03:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:03:13 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:03:13 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.907128632068634], 'bs_recall': [0.937471866607666], 'bs_f1': [0.9220507144927979], 'be_sim': array([0.88201696], dtype=float32), 'ce_sim': array([0.95408887]), 'title': 'Event Description', 'report': 'On June 28, 2025, at 4:15 PM, Karen Lee, QA Inspector, observed unidentified particulate matter in a vial post-filling during a visual inspection of Sterile Manufacturing. The particles were not present during the formulation process, and the root cause of the issue is unknown. Investigation is ongoing. The vial was segregated, and the batch was put on hold. Particulate matter was sent to the lab for analysis.'}\n","08/16/2025 20:03:13 - mods.testBench - INFO - Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:03:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:03:17 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:03:17 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'C', 'temperature': 0.7, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9130942821502686], 'bs_recall': [0.8908483982086182], 'bs_f1': [0.9018341898918152], 'be_sim': array([0.9119551], dtype=float32), 'ce_sim': array([0.91738838]), 'title': 'Unidentified particulate matter in vial post-filling', 'report': 'On June 28, 2025, at 4:15 PM, Karen Lee, QA Inspector, observed unidentified particulate matter in a vial post-filling on the Visual Inspection Line for Sterile Manufacturing. The particle was not present during formulation and investigation is ongoing.'}\n","08/16/2025 20:03:17 - mods.testBench - INFO - Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:03:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:03:23 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:03:23 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.8240277767181396], 'bs_recall': [0.833753228187561], 'bs_f1': [0.8288619518280029], 'be_sim': array([0.28634524], dtype=float32), 'ce_sim': array([0.4017809]), 'title': 'Wrong tablet counting', 'report': 'On June 28, 2025, at 4:15 PM, Erik Hansen loaded the wrong tablet counting disk during changeover on Bottle Packaging Line 2 for Batch RX500 of Neurocet 50 mg. Sarah Yoon from QA discovered the issue during AQL sampling. The line was stopped, 500 bottles were segregated, and rework and retraining were initiated.'}\n","08/16/2025 20:03:23 - mods.testBench - INFO - Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0} \n","08/16/2025 20:03:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/16/2025 20:03:27 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/16/2025 20:03:28 - mods.testBench - INFO - results: {'report_idx': 10, 'prompt_method': 'C', 'temperature': 1.0, 'top_p': 1.0, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0, 'bs_precision': [0.9000405073165894], 'bs_recall': [0.882063090801239], 'bs_f1': [0.8909611701965332], 'be_sim': array([0.8796941], dtype=float32), 'ce_sim': array([0.85907888]), 'title': 'Unidentified Particulate Matter', 'report': 'On June 28, 2025, at 4:15 PM, Karen Lee from QA inspected the batch of Sterile Manufacturing on Visual Inspection Line. The presence of an unidentified particulate matter was observed, which is not present during the formulation process. Investigation is underway to determine the root cause.'}\n","08/16/2025 20:03:28 - mods.dataHandler - INFO - Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench/tb-meta-llama-Llama-3.2-1B-Instruct-16-082025 20-03-28.xlsx\n","08/16/2025 20:03:28 - __main__ - INFO - reportParamGridSearch time --- 14.652793455123902 minutes ---\n","08/16/2025 20:03:28 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/16/2025 20:03:28 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/17/2025 05:40:09 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/17/2025 05:40:10 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n"]}],"source":["!cat app/logs/logfile.log"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"i79nllcxdMwy","executionInfo":{"status":"ok","timestamp":1755410059677,"user_tz":-120,"elapsed":10480,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}}},"outputs":[],"source":["!python projectSetup.py"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["2561fc1a46bd4a0f9684ee6852f00f6e","5ceecfa134d242d49aa0d6ab110dc682","b1738a0d752c41a8a0e914d732593934","0f5b9a3b4b564c0eafc027309a2bcba3","bde31a60391e4239aeae3961ed8f658c","8748d7c5b92041fd9f3461fc5bd85f77","800af15cffae466a9448abf2f7dddb4a","93b7f38d1698485eb3b636b46373cce6","bbbf2ed359ee4a3f900718cadf9f9c49","c07eca8b736a44439478af1369c9ac02","2ee00ff5bf6046df91eadb1b6121d95e"]},"executionInfo":{"elapsed":19564,"status":"ok","timestamp":1755410079245,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"},"user_tz":-120},"id":"Srd-wh7zg8Qt","outputId":"9d6b33b3-d86f-44f6-bf31-0bbc600ec94f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2561fc1a46bd4a0f9684ee6852f00f6e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["08/17/2025 05:54:39 - app.mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=gpt2\n","08/17/2025 05:54:39 - app.mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=gpt2\n","08/17/2025 05:54:39 - app.mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=gpt2\n"]}],"source":["import sys, os\n","import torch\n","from pathlib import Path\n","sys.path.append(os.getcwd())\n","sys.path.append(os.getcwd() + '/app')\n","\n","from app.mods.promptGenerator import PromptGenerator\n","from app.mods.modelLoader import ModelLoader\n","from app.mods.reportGenerator import ReportGenerator\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","torch_dtype = torch.float32 if torch.cuda.is_available() else torch.float32\n","\n","ml = ModelLoader(model_id=\"gpt2\", device=device, torch_dtype=torch_dtype)"]},{"cell_type":"markdown","metadata":{"id":"rfGNcNvTUhdj"},"source":["## microsoft/phi-2\n","It allocates 12 GB in RAM, the extra depends on the number of workers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"P57w5lE7nDyX","outputId":"fe8ae7f1-62f0-4e37-f24d-49a6cfad33c2","collapsed":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","08/15/2025 12:28:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=12\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:28:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:28:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=12\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:28:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:28:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","\n","08/15/2025 12:28:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:28:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","08/15/2025 12:28:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:28:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:28:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 313.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 633.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 628.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 619.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 609.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 610.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 608.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.93 GiB is allocated by PyTorch, and 604.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.92 GiB is allocated by PyTorch, and 607.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 588.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 279.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 574.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 573.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 571.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 296.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:29:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","\n","08/15/2025 12:29:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 581.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 581.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 581.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 306.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 577.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 564.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 562.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:13 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 310.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","08/15/2025 12:29:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","08/15/2025 12:29:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","08/15/2025 12:29:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 257.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 257.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 257.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 257.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 453.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 507.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:29:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.50 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:29:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:29:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:30:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:30:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:30:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:30:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:30:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=13\n","08/15/2025 12:30:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:30:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:30:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:31:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","\n","08/15/2025 12:31:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:31:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:31:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:31:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","\n","08/15/2025 12:31:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:31:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","\n","08/15/2025 12:31:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:31:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:31:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:31:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:31:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:31:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:31:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:31:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:31:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:07 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:32:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 251.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:32:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 290.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 259.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 482.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 478.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 483.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 482.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 481.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 479.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 481.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 256.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 499.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 494.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 498.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 500.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 479.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 477.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 472.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 478.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 478.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 473.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:14 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 273.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=14\n","08/15/2025 12:32:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:32:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:32:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:49 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","08/15/2025 12:32:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:32:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:32:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:32:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","\n","08/15/2025 12:32:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:33:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:33:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","\n","08/15/2025 12:33:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:33:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","\n","08/15/2025 12:33:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:33:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:33:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=14\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:33:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:33:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:33:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:33:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:33:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:34:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","\n","08/15/2025 12:34:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:34:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:34:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:34:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:34:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:34:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:34:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:34:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:34:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:34:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:34:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:35:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:35:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 501.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 510.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 508.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 505.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 510.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 500.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 459.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 450.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 430.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 430.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 269.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 438.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:35:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","\n","08/15/2025 12:35:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 246.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 429.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 435.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 428.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:35:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","\n","08/15/2025 12:35:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 423.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 245.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 245.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 417.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 414.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 411.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:15 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 411.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:35:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:35:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.33 GiB is allocated by PyTorch, and 191.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:35:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","08/15/2025 12:35:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:35:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","\n","08/15/2025 12:35:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 202.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:36:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=15\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:07 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:36:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","\n","08/15/2025 12:36:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1465 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...absent from the scanned', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:36:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1465 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...absent from the scanned', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:36:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","\n","08/15/2025 12:36:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:36:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:36:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:08 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1488 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...absent from the scanned', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:37:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:37:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:37:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1465 [type=json_invalid, input_value='{ \"title\": \"Report\", \"re...absent from the scanned', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=15\n","08/15/2025 12:37:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:37:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:37:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:37:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:37:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","\n","08/15/2025 12:37:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:37:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:37:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:38:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","\n","08/15/2025 12:38:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:38:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","\n","08/15/2025 12:38:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:38:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 486.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 498.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","08/15/2025 12:38:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 443.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 370.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 447.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","\n","08/15/2025 12:38:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 443.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","08/15/2025 12:38:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 353.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 442.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 441.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 441.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 439.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 439.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 441.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 325.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 438.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 440.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 439.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 438.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 435.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 328.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:16 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 430.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 299.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:38:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","\n","08/15/2025 12:38:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:38:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 279.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:38:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 275.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:38:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 468.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:38:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 455.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:38:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 253.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:38:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 256.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:38:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:38:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:38:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:39:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:39:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:39:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:39:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:39:43 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 285.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:43 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=16\n","08/15/2025 12:39:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:39:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","\n","08/15/2025 12:39:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:39:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:39:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=16\n","08/15/2025 12:40:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:40:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:40:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:40:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:40:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:41:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","\n","08/15/2025 12:41:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 536.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 300.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 418.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 417.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 727.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:41:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","\n","08/15/2025 12:41:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 639.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 470.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 470.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:36 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:36 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 293.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 321.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 299.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 407.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 407.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 401.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 405.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 282.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 403.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 399.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 397.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 401.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:41:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","\n","08/15/2025 12:41:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 335.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:41:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","\n","08/15/2025 12:41:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","Ref_row:17 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 612.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","08/15/2025 12:41:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:41 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.30 GiB is allocated by PyTorch, and 219.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:41:41 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=17\n","\n","08/15/2025 12:41:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:41:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:41:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:41:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:41:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:42:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:42:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:42:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=17\n","08/15/2025 12:42:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:42:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:42:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:42:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:43:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:43:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:43:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:43:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","\n","08/15/2025 12:43:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:43:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:43:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:44:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:44:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:44:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","\n","08/15/2025 12:44:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","\n","08/15/2025 12:44:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 269.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 499.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 504.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 503.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 502.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 498.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 272.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 493.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 255.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:28 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","\n","08/15/2025 12:44:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 504.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:29 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 509.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:29 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 527.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:29 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 695.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 269.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 333.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.72 GiB is allocated by PyTorch, and 803.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.68 GiB is allocated by PyTorch, and 840.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.69 GiB is allocated by PyTorch, and 836.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.68 GiB is allocated by PyTorch, and 842.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.75 GiB is allocated by PyTorch, and 778.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:44:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.75 GiB is allocated by PyTorch, and 778.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:18 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 278.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:44:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=18\n","\n","08/15/2025 12:44:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:44:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:44:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:44:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:45:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:45:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:45:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:45:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","\n","08/15/2025 12:45:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","08/15/2025 12:45:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:45:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:45:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:45:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=18\n","\n","08/15/2025 12:45:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:45:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:45:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:45:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:45:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:46:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:46:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:46:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:46:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:46:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","\n","08/15/2025 12:46:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:46:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:46:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:46:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:46:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:46:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:46:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:46:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","\n","08/15/2025 12:46:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:47:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:47:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:07 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 282.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.93 GiB is allocated by PyTorch, and 608.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 567.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 484.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","\n","08/15/2025 12:47:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 467.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 477.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 365.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 551.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 584.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 463.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 463.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 568.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.80 GiB is allocated by PyTorch, and 736.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 542.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 347.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:47:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 551.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 551.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:19 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 346.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:28 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","\n","08/15/2025 12:47:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:29 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 363.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:29 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 548.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 380.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 498.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 577.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 577.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 513.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 316.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 426.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 357.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 346.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 486.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 521.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 315.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 394.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:34 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 322.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:34 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 335.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 351.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 384.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 334.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 383.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:36 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:36 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 546.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:36 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 373.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 398.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 357.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 351.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 395.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 536.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 533.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:37 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 538.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 509.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 537.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 518.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 524.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 511.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 533.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 499.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 527.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 482.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 525.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 480.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 512.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 496.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 529.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 522.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 526.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 506.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 528.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 429.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 428.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 458.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:39 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 327.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:20 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:39 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","08/15/2025 12:47:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 300.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 448.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=20\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:41 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 273.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:41 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 300.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:41 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:41 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:42 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 275.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:42 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:42 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 270.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:42 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:43 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 414.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:43 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:43 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 291.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:43 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 297.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:43 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:43 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:44 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 270.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:44 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 289.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 357.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 349.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 360.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 354.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:45 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 346.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:45 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 338.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 260.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 333.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 332.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 260.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 333.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 290.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 332.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 337.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:47 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 336.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 258.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 325.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 294.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 299.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 295.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 297.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","\n","08/15/2025 12:47:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 299.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 297.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 292.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 296.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 296.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 297.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 290.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 295.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 297.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 295.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 283.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 290.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 291.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 291.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 290.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 290.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 283.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 288.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 289.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 286.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 290.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 268.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=21\n","08/15/2025 12:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 271.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 307.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 258.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 308.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 317.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 318.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 307.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 316.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 257.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 316.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 312.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 295.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 306.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 317.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 316.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 305.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 305.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 263.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 303.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 303.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 314.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 305.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 325.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 328.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 328.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 323.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 328.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 322.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 325.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 303.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 304.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 303.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 301.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 301.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 302.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 301.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 300.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 280.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 279.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 278.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 278.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 273.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 252.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 251.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 255.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 278.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 277.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 278.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=22\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 268.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 293.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 284.36 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 293.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 248.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 233.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 289.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 300.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 283.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 281.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 286.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 286.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 281.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 285.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 283.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 284.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 570.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 315.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 346.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:47:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:47:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:47:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:47:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.94 GiB is allocated by PyTorch, and 595.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:47:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 332.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 309.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 417.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:02 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 283.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:02 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 418.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 432.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 432.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 427.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 424.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 403.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 260.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 355.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 355.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 448.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.97 GiB is allocated by PyTorch, and 560.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 550.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 550.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 515.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 447.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 301.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 267.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 253.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 253.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:07 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 266.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:07 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:08 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 235.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:08 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:08 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 248.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:08 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 262.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 480.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 590.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 628.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:23 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.75 GiB is allocated by PyTorch, and 792.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.77 GiB is allocated by PyTorch, and 766.24 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.73 GiB is allocated by PyTorch, and 807.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:09 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=23\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:11 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 357.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:11 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:12 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 337.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:12 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 359.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:48:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 359.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","08/15/2025 12:48:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:48:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:48:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:49:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:49:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:49:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=19\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/15/2025 12:49:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:49:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:49:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:49:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=23\n","08/15/2025 12:49:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:49:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:49:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:49:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:49:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","\n","08/15/2025 12:49:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:50:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:50:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:50:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:50:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:50:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:50:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","\n","08/15/2025 12:50:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:50:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:50:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:50:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:50:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:50:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","\n","08/15/2025 12:50:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:51:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 247.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 494.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.31 GiB is allocated by PyTorch, and 216.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 254.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 509.19 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 507.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 500.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 504.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:51:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 504.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 233.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 486.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 489.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 404.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.13 GiB is allocated by PyTorch, and 404.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 358.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 515.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.30 GiB is allocated by PyTorch, and 229.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 374.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 636.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 627.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:51:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.86 GiB is allocated by PyTorch, and 663.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:24 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.90 GiB is allocated by PyTorch, and 620.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 250.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:51:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","\n","08/15/2025 12:51:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:30 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 477.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:30 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:51:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 250.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:51:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 482.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:51:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 259.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:34 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 314.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:34 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 264.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:36 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 272.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:37 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:51:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:38 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 290.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:38 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:51:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:51:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","\n","08/15/2025 12:51:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 310.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.21 GiB is allocated by PyTorch, and 309.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:51:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:51:40 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","\n","08/15/2025 12:51:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","08/15/2025 12:51:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:51:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:51:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:52:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.23 GiB is allocated by PyTorch, and 292.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:52:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=24\n","08/15/2025 12:52:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:52:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:52:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:52:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:52:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:52:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:52:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:52:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=24\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:52:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","08/15/2025 12:53:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","08/15/2025 12:53:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","08/15/2025 12:53:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 256.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:53:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 240.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 567.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 539.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 542.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 542.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.98 GiB is allocated by PyTorch, and 542.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:53:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","\n","08/15/2025 12:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 202.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:53:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.33 GiB is allocated by PyTorch, and 192.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:53:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 526.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:53:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 236.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:53:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:53:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:53:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 522.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:53:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","\n","08/15/2025 12:53:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 251.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:54:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 514.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.89 GiB is allocated by PyTorch, and 641.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:54:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 510.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:02 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 274.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:54:02 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","\n","08/15/2025 12:54:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:02 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 281.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:02 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","08/15/2025 12:54:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 432.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.95 GiB is allocated by PyTorch, and 573.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 228.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 344.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 344.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:25 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:07 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 271.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:07 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:54:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 267.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:54:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 445.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:35 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 445.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:54:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:35 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=25\n","08/15/2025 12:54:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:54:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:54:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:54:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","\n","\n","08/15/2025 12:54:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:54:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:54:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","\n","08/15/2025 12:54:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:55:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=25\n","\n","08/15/2025 12:55:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:55:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:55:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:55:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:55:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:55:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:55:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:55:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:55:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:55:49 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","\n","08/15/2025 12:55:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:55:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:55:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:56:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","\n","08/15/2025 12:56:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:56:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","\n","08/15/2025 12:56:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","\n","08/15/2025 12:56:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:56:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 308.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","\n","08/15/2025 12:56:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 238.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:56:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 508.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:56:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 507.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","\n","08/15/2025 12:56:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 508.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:56:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 508.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:56:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","\n","08/15/2025 12:56:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:56:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 509.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:56:59 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:56:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 236.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:00 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 224.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 498.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 496.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:01 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 497.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:01 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:03 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 238.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:03 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 242.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 494.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 495.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 495.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 489.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 496.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:04 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 499.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:04 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 265.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:05 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 506.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:26 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:05 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:06 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 287.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:06 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","08/15/2025 12:57:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","08/15/2025 12:57:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 330.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 330.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:57:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","\n","08/15/2025 12:57:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 285.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:57:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","\n","08/15/2025 12:57:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 306.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:57:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:33 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.96 GiB is allocated by PyTorch, and 566.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:33 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:57:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:57:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:57:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:58:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:58:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:58:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=26\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:58:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:58:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:58:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:58:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:58:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:58:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:58:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:58:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","\n","08/15/2025 12:58:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.06 GiB is allocated by PyTorch, and 462.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 13.73 GiB is allocated by PyTorch, and 812.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 371.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:48 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:49 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 262.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:49 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 265.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 370.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 369.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 372.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:50 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 374.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:50 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 374.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 370.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 369.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 8.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 241.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:51 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 671.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:51 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 674.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:52 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 372.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:52 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:53 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 267.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:53 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 257.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:54 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 435.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:54 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:55 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 517.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:55 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 277.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 12:59:56 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.16 GiB is allocated by PyTorch, and 360.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:27 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:56 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:57 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 321.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 12:59:57 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","08/15/2025 12:59:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 12:59:58 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.18 GiB is allocated by PyTorch, and 337.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 12:59:58 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","\n","08/15/2025 12:59:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:00:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 367.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:00:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.17 GiB is allocated by PyTorch, and 367.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:00:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:00:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:00:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:00:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:00:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 13:00:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:00:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:00:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:01:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","\n","08/15/2025 13:01:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:01:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:01:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:01:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:01:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","\n","08/15/2025 13:01:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:01:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:01:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:01:49 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:01:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","08/15/2025 13:02:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=27\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:02:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:02:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:02:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","\n","08/15/2025 13:02:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:02:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:02:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:02:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:02:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:03:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 10.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.24 GiB is allocated by PyTorch, and 287.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 462.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.05 GiB is allocated by PyTorch, and 491.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:14 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.93 GiB is allocated by PyTorch, and 590.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 419.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:03:15 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","\n","08/15/2025 13:03:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 239.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 221.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 226.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 396.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 20.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 397.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.12 GiB is allocated by PyTorch, and 397.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 364.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.19 GiB is allocated by PyTorch, and 324.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.14 GiB is allocated by PyTorch, and 374.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.88 GiB is allocated by PyTorch, and 649.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 613.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:03:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","\n","08/15/2025 13:03:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.88 GiB is allocated by PyTorch, and 640.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.91 GiB is allocated by PyTorch, and 612.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.88 GiB is allocated by PyTorch, and 648.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 499.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 253.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 248.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:28 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 301.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 18.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 220.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 308.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:03:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 509.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:03:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 509.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 507.04 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:03:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:31 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 85959 has 14.74 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 283.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:03:31 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:32 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.22 GiB is allocated by PyTorch, and 298.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:32 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:03:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:03:43 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 245.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:43 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","08/15/2025 13:03:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:03:51 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:03:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:04:08 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 264.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:04:08 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=28\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:04:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:04:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:04:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:04:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:04:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:04:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=28\n","08/15/2025 13:04:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:04:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:04:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","\n","08/15/2025 13:04:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:04:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:04:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:05:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:05:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:05:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:05:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:05:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:05:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:05:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:05:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:05:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","\n","08/15/2025 13:05:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:05:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:29 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:05:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:06:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.30 GiB is allocated by PyTorch, and 220.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:17 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 262.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:17 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 252.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 227.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 518.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 518.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 514.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 224.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 258.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 520.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 515.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:06:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 250.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 227.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 449.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 441.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 450.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:26 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 240.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:26 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 229.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 443.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 445.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:06:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:27 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 443.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:27 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.08 GiB is allocated by PyTorch, and 441.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:28 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 243.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:28 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.09 GiB is allocated by PyTorch, and 434.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:29 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:06:28 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","\n","08/15/2025 13:06:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:28 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.10 GiB is allocated by PyTorch, and 427.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:29 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:06:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 198.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:46 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=29\n","08/15/2025 13:06:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:06:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:06:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:06:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:07:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:07:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:07:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","\n","08/15/2025 13:07:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:07:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","\n","08/15/2025 13:07:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:07:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:07:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:07:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","\n","08/15/2025 13:07:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:07:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","\n","08/15/2025 13:07:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:08:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:08:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","08/15/2025 13:08:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:08:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:08:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","\n","08/15/2025 13:08:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:08:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:08:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:08:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:09:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:09:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:09:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:09:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:09:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:09:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:09:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:09:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=29\n","Ref_row:30 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:09:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 266.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 537.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 536.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 535.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.00 GiB is allocated by PyTorch, and 529.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 534.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 534.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 533.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:16 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 532.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:16 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:18 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 262.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:18 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.26 GiB is allocated by PyTorch, and 255.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:19 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 245.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 519.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:20 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 14.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 514.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:20 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:21 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 263.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:21 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.0, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 4.12 MiB is free. Process 85959 has 14.73 GiB memory in use. Of the allocated memory 14.31 GiB is allocated by PyTorch, and 220.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:22 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.01 GiB is allocated by PyTorch, and 510.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:22 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 506.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.3, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 506.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:10:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","\n","08/15/2025 13:10:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 505.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:23 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.29 GiB is allocated by PyTorch, and 228.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.6, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 488.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:24 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 496.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:24 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/15/2025 13:10:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.28 GiB is allocated by PyTorch, and 233.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","Ref_row:30 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/15/2025 13:10:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","\n","08/15/2025 13:10:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/15/2025 13:10:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_p': 0.9, 'top_k': 70, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}: CUDA out of memory. Tried to allocate 22.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 85959 has 14.72 GiB memory in use. Of the allocated memory 14.03 GiB is allocated by PyTorch, and 492.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","08/15/2025 13:10:25 - mods.testBench - ERROR - FAILED report export: cannot access local variable 'report' where it is not associated with a value on row=30\n","08/15/2025 13:10:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:10:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:11:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:11:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:11:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","08/15/2025 13:11:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1233 [type=json_invalid, input_value='{ \"title\": \"Dissolution ...D3204 of Painex 200 mg.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/15/2025 13:11:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=30\n","reportParamGridSearch time --- 86.03608230352401 minutes ---\n"]}],"source":["!python app/reportParamGridSearch.py --model_id microsoft/phi-2 --prompt_method B C --max_workers 12 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 30  --temperature 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 30 50 70 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","metadata":{"id":"vVTm54F1UaHq"},"source":["## HuggingFaceTB/SmolLM3-3B\n","It allocates 14 GB in RAM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2165754,"status":"ok","timestamp":1755207837726,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"DdF2u_DS5WaO","outputId":"ede959e3-d064-4eea-e89e-8025ea70472e","collapsed":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-08-14 21:07:57.091907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755205677.113963   69791 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755205677.121261   69791 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755205677.139998   69791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755205677.140022   69791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755205677.140024   69791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755205677.140027   69791 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-08-14 21:07:57.144843: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Parameters passed to main script: \n","{'max_workers': [12], 'model_id': ['HuggingFaceTB/SmolLM3-3B'], 'prompt_method': ['B', 'C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [21], 'end_idx': [22], 'temperature': [0.7, 0.9, 1.1, 1.3], 'top_p': [0.4, 0.5, 0.7, 0.8], 'top_k': [30, 40, 50, 60], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","08/14/2025 21:08:12 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","08/14/2025 21:08:12 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","08/14/2025 21:08:12 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","Generation parameters: \n","{'temperature': [0.7, 0.9, 1.1, 1.3], 'top_p': [0.4, 0.5, 0.7, 0.8], 'top_k': [30, 40, 50, 60], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.15s/it]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.53it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.22it/s]\n","Batches: 100% 1/1 [00:00<00:00,  5.41it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.18it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.33it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.51it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.76it/s]\n","Batches: 100% 1/1 [00:00<00:00,  5.36it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.35it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.66it/s]\n","Batches: 100% 1/1 [00:00<00:00,  4.19it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.17it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.47it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.03it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  2.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.16it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.24it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.31it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.63it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.88it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.43it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.62it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.38it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.20it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.13it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.93it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.30it/s]\n","Batches: 100% 1/1 [00:00<00:00,  3.10it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.85it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.07it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.41it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.88it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.74it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.86it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.34it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.10it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.49it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.28it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.75it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.02it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.34it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.98it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.75it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.19it/s]\n","Batches: 100% 1/1 [00:00<00:00,  6.72it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  4.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.71it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.70it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.70it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.26it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.04it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  4.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.53it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.56it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.32it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.03it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.55it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.24it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.77it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.25it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.92it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.09it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.58it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.68it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.27it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.21it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 12.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.58it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.76it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.05it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.21it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.77it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.84it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.10it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.59it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.56it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.99it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.89it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.74it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.86it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.64it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.73it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.70it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.01it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.92it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.52it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.53it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.55it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.43it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.49it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.41it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.55it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.12it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.07it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.89it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.15it/s]\n","Batches: 100% 1/1 [00:00<00:00,  5.28it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.02it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.85it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.74it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.24it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.74it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.17it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.29it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.04it/s]\n","Batches: 100% 1/1 [00:00<00:00,  2.70it/s]\n","Batches:   0% 0/1 [00:00<?, ?it/s]Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.16it/s]\n","Batches: 100% 1/1 [00:00<00:00,  4.14it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.34it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.97it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.96it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  4.91it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.52it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.15it/s]\n","Batches: 100% 1/1 [00:00<00:00,  4.10it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.03it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.35it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.00it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.91it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.30it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.42it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.41it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.84it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.31it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.76it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.57it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.00it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.14it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.19it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.64it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.93it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.37it/s]\n","Batches: 100% 1/1 [00:00<00:00,  6.88it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.58it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.20it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.91it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.13it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.73it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.33it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.79it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.68it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.60it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.51it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.53it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.22it/s]\n","Batches: 100% 1/1 [00:00<00:00,  3.85it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.57it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.18it/s]\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.57it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.73it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00,  3.87it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.11it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.74it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.83it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.74it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.74it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.24it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.28it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.75it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 12.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.52it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.65it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.18it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.96it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.03it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.52it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.96it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.16it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 13.17it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.57it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.19it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.89it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.39it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.54it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.61it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.75it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.13it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.51it/s]\n","Batches: 100% 1/1 [00:00<00:00,  6.82it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.76it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.78it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.28it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.78it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.64it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.16it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.93it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.07it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.58it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.97it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.10it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.58it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.81it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.60it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.32it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.65it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.28it/s]\n","Batches: 100% 1/1 [00:00<00:00,  6.87it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.03it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.67it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.33it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.50it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.85it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.22it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.77it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.25it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.78it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.42it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.97it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.47it/s]\n","Batches: 100% 1/1 [00:00<00:00,  7.01it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.64it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.04it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.01it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.24it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.63it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.25it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.84it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.80it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.74it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 17.00it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.55it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.73it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.46it/s]\n","Batches: 100% 1/1 [00:00<00:00,  9.09it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.25it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.84it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.01it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.70it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.64it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.55it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.86it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.41it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 13.13it/s]\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.32it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.49it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.95it/s]\n","Batches: 100% 1/1 [00:00<00:00,  3.26it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.95it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.01it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.86it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.11it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.38it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.34it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.62it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.43it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.43it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.26it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.50it/s]\n","Batches: 100% 1/1 [00:00<00:00,  5.97it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  4.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.13it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.32it/s]\n","Batches: 100% 1/1 [00:00<00:00,  3.11it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.42it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.78it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 12.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.56it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.00it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.76it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.56it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 16.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.14it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.62it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.80it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.18it/s]\n","Batches: 100% 1/1 [00:00<00:00,  6.02it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.15it/s]\n","Batches: 100% 1/1 [00:00<00:00,  4.30it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.22it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.54it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.43it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.91it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.39it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.58it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  2.31it/s]\n","Batches: 100% 1/1 [00:00<00:00,  4.57it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 14.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.18it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.19it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.43it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.11it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.39it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.61it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.64it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.07it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.57it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  8.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.04it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.80it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.72it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.01it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.82it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 16.12it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.54it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.53it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 10.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.87it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.75it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  2.42it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.36it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  3.44it/s]\n","Batches: 100% 1/1 [00:00<00:00,  2.03it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.67it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  9.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.50it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.22it/s]\n","Batches:   0% 0/1 [00:00<?, ?it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.57it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","\n","Batches: 100% 1/1 [00:00<00:00,  3.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 12.34it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 13.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.06it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  6.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.22it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.64it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 15.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.06it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 10.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 10.75it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  7.25it/s]\n","Batches: 100% 1/1 [00:00<00:00,  3.00it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 10.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.39it/s]\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00, 11.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.26it/s]\n","Batches: 100% 1/1 [00:00<00:00,  8.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 18.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 11.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 19.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 14.10it/s]\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 14.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 21.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 13.17it/s]\n","Batches:   0% 0/1 [00:00<?, ?it/s]\n","Batches: 100% 1/1 [00:00<00:00, 30.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 42.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 47.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 28.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 181.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 172.61it/s]\n","reportParamGridSearch time --- 35.87140321334203 minutes ---\n"]}],"source":["!python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM3-3B  --prompt_method B C --max_workers 12 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 21 --end_idx 22  --temperature 0.7 0.9 1.1 1.3 --top_p 0.4 0.5 0.7 0.8 --top_k 30 40 50 60 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","metadata":{"id":"GBWVQWjZUUrm"},"source":["## GPT2-XL\n","It allocates 7.2 GB in RAM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1725507,"status":"ok","timestamp":1755203612946,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"XmhpRs4v6SBP","outputId":"e1850293-6903-48b5-9120-f34eb5a75ab0","collapsed":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-08-14 20:04:52.560226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755201892.581674   54186 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755201892.588676   54186 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755201892.606844   54186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755201892.606870   54186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755201892.606873   54186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755201892.606876   54186 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-08-14 20:04:52.611403: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Parameters passed to main script: \n","{'max_workers': [12], 'model_id': ['openai-community/gpt2-xl'], 'prompt_method': ['B', 'C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [21], 'end_idx': [22], 'temperature': [0.7, 0.9, 1.1, 1.3], 'top_p': [0.4, 0.5, 0.7, 0.8], 'top_k': [30, 40, 50, 60], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","08/14/2025 20:05:07 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","08/14/2025 20:05:07 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","08/14/2025 20:05:07 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","Generation parameters: \n","{'temperature': [0.7, 0.9, 1.1, 1.3], 'top_p': [0.4, 0.5, 0.7, 0.8], 'top_k': [30, 40, 50, 60], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:05:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 13.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.83it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches:   0% 0/1 [00:00<?, ?it/s]08/14/2025 20:05:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 14.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 17.08it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:05:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 12.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.49it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:05:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 14.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.53it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:05:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches:   0% 0/1 [00:00<?, ?it/s]08/14/2025 20:05:59 - mods.testBench - ERROR - FAILED report export: write to closed file on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:00 - mods.testBench - ERROR - FAILED report export: Invalid IPC message: negative metadata length on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Batches: 100% 1/1 [00:00<00:00,  5.82it/s]\n","Batches:   0% 0/1 [00:00<?, ?it/s]08/14/2025 20:06:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00,  1.81it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 13.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 16.81it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:06:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","Batches:   0% 0/1 [00:00<?, ?it/s]08/14/2025 20:06:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 13.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 15.71it/s]\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:06:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:06:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:06:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:06:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:06:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:06:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:06:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:06:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:06:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:06:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:06:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:06:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:06:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:06:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:07:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:07:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:07:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:07:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:07:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:08:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:08:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:08:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:08:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:08:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:08:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:08:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:08:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:08:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:08:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:08:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:08:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:09:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:09:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:09:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:09:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:09:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:09:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:09:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:09:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:10:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:10:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:10:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:10:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:10:37 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1054 [type=json_invalid, input_value='{ \"title\": \"Batch B2001 ...tch B2001 OOS result in', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:10:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:10:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:10:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:10:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:10:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:10:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:11:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:11:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:11:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:11:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:11:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:11:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 949 [type=json_invalid, input_value='{ \"title\": \"OOS result i...ons-observations-observ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:11:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:11:53 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 357 [type=json_invalid, input_value='{ \"title\": \"OOS result i...-0-0-0-0-0-0-0-0-0-0-0-', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:11:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:11:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:12:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1392 [type=json_invalid, input_value='{ \"title\":\"Report Title\"...t Results: Test Results', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:12:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:12:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:12:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:12:07 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:12:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:12:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:14 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:12:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:12:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:12:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:12:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:12:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:12:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:12:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:13:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:13:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:13:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:13:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:34 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:13:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:13:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:14:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:14:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:14:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:14:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:14:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:14:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:14:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:14:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:14:39 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1299 [type=json_invalid, input_value='{ \"title\": \"Correct tabl... in the Microbiology QC', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:14:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:14:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:14:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:14:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:14:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:14:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:14:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:14:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:15:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:15:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:15:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:15:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:15:50 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:15:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:15:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:15:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:15:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:15:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:16:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:16:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:16:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:16:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:16:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:16:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:16:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:16:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:04 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:17:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:08 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1448 [type=json_invalid, input_value='{ \"title\": \"OOS result i...iately retested Batch B', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:17:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1196 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...sting of Batch B2001 of', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:17:25 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:17:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:21 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:17:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:17:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:17:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:17:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:17:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:17:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:18:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:18:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:09 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","08/14/2025 20:18:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:18:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1231 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...ted. The batch was held', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:18:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:18:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:18:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","\n","08/14/2025 20:18:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:18:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:18:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:18:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:18:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:18:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:19:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:19:47 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:19:49 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:19:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:19:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:19:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:19:58 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:19:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:20:55 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1310 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet... in the Microbiology QC', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:20:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=21\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:20:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:20:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:20:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:20:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:21:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:21:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:08 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:21:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:21:21 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:21:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:21:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:21:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:21:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:21:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:21:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:21:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:21:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:21:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:21:52 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:21:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:21:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:21:54 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:21:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:03 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:23 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:22:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:22:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:27 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:22:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:22:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:22:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:22:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:22:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:23:02 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1364 [type=json_invalid, input_value='{ \"title\": \"Humidity exc....\\\\\"} {\\\\\"title\\\\\": \\\\\"', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:23:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:23:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:23:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:23:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:23:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:23:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:23:33 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 510 [type=json_invalid, input_value='{ \"title\": \"Humidity exc...0-2, G-20-2, G-20-2, G-', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:23:33 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:23:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:24:13 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:24:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:24:19 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:24:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:24:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:24:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:24:53 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:24:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:25:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:25:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:25:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1126 [type=json_invalid, input_value='{ \"title\": \"Humidity exc...t. Report is in [title,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:25:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:25:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:25:31 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:25:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:25:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 731 [type=json_invalid, input_value='{ \"title\": \"HVAC Filter\"...s://www.google.com/url?', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:25:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:25:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:25:55 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:25:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:26:02 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:26:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:26:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:26:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:26:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:26:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:26:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:26:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:27:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:27:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:27:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:27:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:27:42 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:27:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:27:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1534 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...logs were reviewed. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:27:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:27:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:27:56 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:27:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:28:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1338 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...rantined and the filter', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.9, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:28:00 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:28:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:28:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:28:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:28:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:28:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:28:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:28:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:28:36 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:28:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:28:41 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:28:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:29:24 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:29:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:29:26 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:29:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:29:38 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 528 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...                       ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:29:38 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:29:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:29:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:29:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:29:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:29:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:10 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 644 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...                       ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:30:10 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:11 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:12 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:30 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 670 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...                       ', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:30:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.1, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:32 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:30:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:35 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:30:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:30:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:30:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1275 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...urned to 68% RH for the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:44 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:30:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:30:57 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.4, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:30:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:31:01 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:31:06 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:31:20 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:31:22 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.5, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:37 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:31:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:31:43 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:31:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:32:05 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:32:15 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.7, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:32:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 30, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:32:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:32:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 40, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}08/14/2025 20:32:28 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","\n","08/14/2025 20:32:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:32:30 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1288 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...ity decreased to 33% RH', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:32:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Ref_row:22 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.8, 'top_k': 60, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/14/2025 20:32:39 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 20:32:40 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:45 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:46 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:48 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:32:59 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:16 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:17 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:18 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:29 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1262 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...om humidity rose to 68%', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:33:29 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","08/14/2025 20:33:30 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1519 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...onmental logs reviewed.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 20:33:30 - mods.testBench - ERROR - FAILED report export: 'NoneType' object is not iterable on row=22\n","reportParamGridSearch time --- 28.532266501585642 minutes ---\n"]}],"source":["!python app/reportParamGridSearch.py --model_id openai-community/gpt2-xl --prompt_method B C --max_workers 12 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 21 --end_idx 22  --temperature 0.7 0.9 1.1 1.3 --top_p 0.4 0.5 0.7 0.8 --top_k 30 40 50 60 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","source":["## meta-llama/Llama-3.2-1B-Instruct\n","It allocates 5.9 GB in RAM"],"metadata":{"id":"FOyyMTHXHNsm"}},{"cell_type":"code","source":["from huggingface_hub import login\n","login(token=userdata.get('HF_TOKEN'))  # insert your Hugging Face token here\n","\n","# !python app/reportParamGridSearch.py --model_id microsoft/phi-2 --prompt_method B C --max_workers 12 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 30  --temperature 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 30 50 70 --max_new_tokens 300 --do_sample True\n","\n","!python app/reportParamGridSearch.py --model_id meta-llama/Llama-3.2-1B-Instruct --non-threaded --prompt_method B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 80  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y4e6CcfAHN_z","executionInfo":{"status":"ok","timestamp":1755426028575,"user_tz":-120,"elapsed":12595134,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}},"outputId":"795b4bde-c006-4523-f65a-275d9c0327e4"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Batches: 100% 1/1 [00:00<00:00, 127.50it/s]\n","Ref_row:40 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:29:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.82it/s]\n","Ref_row:40 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:29:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.84it/s]\n","Ref_row:40 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:30:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.62it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:30:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.59it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:30:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.10it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:30:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.48it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:30:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 103.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.60it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:30:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.92it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:30:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.92it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:30:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.93it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:30:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 91.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.19it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:31:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.97it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:31:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 100.43it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:31:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.41it/s]\n","Ref_row:40 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:31:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.21it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:31:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 81.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.86it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:31:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.65it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:31:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.21it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:31:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.96it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:32:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.89it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:32:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.01it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:32:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.99it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:32:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.14it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:32:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.21it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:32:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.01it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:32:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 95.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.12it/s]\n","Ref_row:41 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:32:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.55it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:32:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.15it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:32:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.53it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:32:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.66it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:33:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 103.93it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:33:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.67it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:33:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 110.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.18it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:33:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.72it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:33:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.07it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:33:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.98it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:33:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.39it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:33:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.87it/s]\n","Ref_row:41 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:33:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.90it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:34:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 104.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.21it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:34:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.47it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:34:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.28it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:34:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.01it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:34:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.33it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:34:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.12it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:34:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.53it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:34:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.42it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:34:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.54it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.53it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 105.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 106.80it/s]\n","Ref_row:42 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.85it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.82it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 103.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 96.11it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 97.70it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.87it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.42it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 109.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.83it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 98.48it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 107.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.98it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:35:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.25it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:36:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 141.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.02it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:36:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 97.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.65it/s]\n","Ref_row:42 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:36:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 141.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.82it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:36:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 95.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.06it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:36:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.22it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:36:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.88it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:36:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 91.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.64it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:37:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.15it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:37:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.43it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:37:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.08it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:37:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.75it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:37:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.69it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:37:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.33it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:37:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.24it/s]\n","Ref_row:43 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:37:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.75it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:37:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.14it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:37:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.31it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:37:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 73.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.39it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:38:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.92it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:38:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.52it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:38:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 144.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 148.60it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:38:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.24it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:38:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 86.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 100.38it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:38:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 08:38:38 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 508 [type=json_invalid, input_value='{\"title\": \"Quality Issue...f: DEV-2025-1117).\\\\\"}}', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 133.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.99it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:38:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 78.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.72it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:38:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 74.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.77it/s]\n","Ref_row:43 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:38:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 76.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.54it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:38:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 08:39:15 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1498 [type=json_invalid, input_value='{\"title\": \"Shipping Dela...nt, Packaging Facility.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 128.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.52it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:39:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.31it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:39:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.88it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:39:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 08:39:47 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1498 [type=json_invalid, input_value='{\"title\": \"Shipping Dela...nt, Packaging Facility.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 137.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.85it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:39:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.32it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:39:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.56it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:39:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 64.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 92.78it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:40:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.48it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:40:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.00it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:40:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 08:40:28 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1625 [type=json_invalid, input_value='{\"title\": \"Shipping Dela...y and the deviation was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 132.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.71it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:40:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 90.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 103.08it/s]\n","Ref_row:44 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:40:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.22it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:40:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 103.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.47it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:40:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 143.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.14it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:40:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.37it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:40:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.18it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:40:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.46it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:41:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.40it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:41:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.97it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:41:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.93it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:41:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.33it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:41:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.09it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:41:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.39it/s]\n","Ref_row:44 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:41:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 98.35it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:41:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.77it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:41:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.97it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:41:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.26it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:41:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.21it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:42:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.23it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:42:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.47it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:42:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.67it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:42:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.53it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:42:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.44it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:42:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.57it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:42:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 83.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.96it/s]\n","Ref_row:45 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:42:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.17it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:42:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.60it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:42:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 92.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.36it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:42:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.62it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:43:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 110.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.53it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:43:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.04it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:43:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.93it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:43:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 106.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 98.27it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:43:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.57it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:43:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 100.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 101.00it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:43:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.69it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:43:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.21it/s]\n","Ref_row:45 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:43:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.99it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:43:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 100.06it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:43:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.11it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:44:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.77it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:44:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.26it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:44:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 96.03it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:44:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.10it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:44:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.91it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:44:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.08it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:44:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.02it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:44:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.66it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:45:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.33it/s]\n","Ref_row:46 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:45:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.89it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:45:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.66it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:45:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.54it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:45:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.76it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:45:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.33it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:45:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.45it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:45:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.91it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:45:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.88it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:46:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 60.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 63.24it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:46:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 08:46:31 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1732 [type=json_invalid, input_value='{\"title\": \"Contamination...cise reporting based on', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 73.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.02it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:46:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 103.50it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:46:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.10it/s]\n","Ref_row:46 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:46:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.58it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:47:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.32it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:47:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.95it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:47:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.36it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:47:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.58it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:47:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.25it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:47:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 104.61it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 106.74it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:48:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 112.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.10it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:48:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.64it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:48:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 08:48:27 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1615 [type=json_invalid, input_value='{\"title\": \"Incorrect Rec...corrective actions were', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 129.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.75it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:48:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 68.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.26it/s]\n","Ref_row:47 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:48:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.50it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:48:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.90it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:48:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.56it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:48:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.04it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:48:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.77it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:49:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.33it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:49:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.14it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:49:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.81it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:49:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.83it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:49:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.99it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:49:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.26it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:49:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.33it/s]\n","Ref_row:47 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:49:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 69.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 100.20it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:49:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.56it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:50:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.53it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:50:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.55it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:50:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.20it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:50:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.46it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:50:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.10it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:50:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.58it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:51:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 97.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.47it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.76it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:51:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.61it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:51:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 89.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.59it/s]\n","Ref_row:48 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:51:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.51it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:51:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.78it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:51:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.50it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:52:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.37it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:52:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.96it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:52:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 86.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.26it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:52:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 85.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.57it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:52:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.77it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:52:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 71.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 53.22it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:53:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 85.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.55it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:53:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.58it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:53:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.86it/s]\n","Ref_row:48 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:53:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.38it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:53:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 89.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.82it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:53:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 101.71it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:53:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.74it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:54:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 08:54:19 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1606 [type=json_invalid, input_value='{\"title\": \"Delayed Shipm...ated to ensure that all', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 130.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.72it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:54:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.21it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:54:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.57it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:54:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.28it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:54:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.70it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:54:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.35it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:54:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 08:55:16 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1227 [type=json_invalid, input_value='{\"title\": \"Delayed Shipm...s $X. The investigation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 133.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.30it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:55:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.41it/s]\n","Ref_row:49 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:55:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.33it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:55:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.69it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:55:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.28it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:55:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.89it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:55:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.81it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:55:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.11it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:56:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.94it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:56:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 107.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.89it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:56:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.85it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:56:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 133.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.58it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:56:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.95it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:56:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.94it/s]\n","Ref_row:49 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:56:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.63it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:56:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.04it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:56:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 86.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.78it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:57:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.87it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:57:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.29it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:57:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.27it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:57:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.41it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:57:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 89.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.03it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:57:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.87it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:57:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.94it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:57:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 100.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.02it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:58:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 104.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 98.94it/s]\n","Ref_row:50 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:58:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.42it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:58:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.52it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:58:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 101.62it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:58:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 104.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.08it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:58:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 82.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.06it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:58:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.66it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:58:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 78.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.36it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:58:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.96it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:58:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.00it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:58:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.16it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:59:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.49it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:59:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.93it/s]\n","Ref_row:50 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:59:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.05it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:59:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.04it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:59:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.41it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:59:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 77.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.37it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:59:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.70it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 08:59:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.36it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:00:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.33it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:00:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.76it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:00:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.73it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:00:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.46it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:00:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.82it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:00:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.57it/s]\n","Ref_row:51 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:00:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.07it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:00:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.58it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:01:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.15it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:01:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.94it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:01:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.09it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:01:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.53it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:01:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.81it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:01:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.25it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:01:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.65it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:01:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.39it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:01:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.92it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:02:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.14it/s]\n","Ref_row:51 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:02:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.75it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:02:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.71it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:02:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.09it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:02:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.76it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:02:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.68it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:02:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:03:05 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1353 [type=json_invalid, input_value='{\"title\": \"Mislabeling o...mples were re-verified.', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 92.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.78it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:03:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.78it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:03:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.03it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:03:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.25it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:03:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.79it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:03:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.15it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:03:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.33it/s]\n","Ref_row:52 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:03:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 101.96it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:03:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.59it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:03:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.87it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:03:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.24it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:03:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 95.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.28it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:04:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.76it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:04:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.09it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:04:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.88it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:04:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.40it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:04:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 102.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.77it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:04:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.44it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:04:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.23it/s]\n","Ref_row:52 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:04:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.53it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:04:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.46it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:04:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:05:08 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1507 [type=json_invalid, input_value='{\"title\": \"Temperature B...d, and an investigation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 75.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 97.36it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:05:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.04it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:05:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 89.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.83it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:05:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.27it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:05:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 92.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.57it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:05:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.12it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:05:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.14it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:05:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.91it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:05:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.30it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:05:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 100.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 92.92it/s]\n","Ref_row:53 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:06:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.23it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:06:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.82it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:06:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.66it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:06:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.07it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:06:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.72it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:06:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.15it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:06:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.56it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:06:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.31it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:06:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.65it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:07:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.30it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:07:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.81it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:07:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.09it/s]\n","Ref_row:53 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:07:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 110.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.58it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:07:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 97.55it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:07:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.07it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:07:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.96it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:07:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 67.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 70.05it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:08:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.58it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:08:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.96it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:08:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 64.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.43it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:08:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.94it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:08:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 107.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.75it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:08:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 51.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.76it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:08:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.87it/s]\n","Ref_row:54 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:08:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.91it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:08:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.21it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:09:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 142.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.29it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:09:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.22it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:09:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.75it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:09:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.73it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:09:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.06it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:09:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.95it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:09:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.66it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:09:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.39it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:09:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.17it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:10:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.38it/s]\n","Ref_row:54 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:10:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.56it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:10:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.56it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:10:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 95.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.93it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:10:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.53it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:10:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.38it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:10:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.64it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:10:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.14it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:10:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.81it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:10:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 43.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.25it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:11:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.29it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:11:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.81it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:11:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.15it/s]\n","Ref_row:55 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:11:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.97it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:11:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.30it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:11:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.49it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:11:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 147.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.19it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:11:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.47it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:11:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.36it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:12:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.09it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:12:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.32it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:12:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 90.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.62it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:12:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.21it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:12:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 93.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.49it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:12:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.75it/s]\n","Ref_row:55 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:12:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.89it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:12:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.12it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:12:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.78it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:12:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.54it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:13:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:13:23 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1600 [type=json_invalid, input_value='{\"title\": \"Personal Prot...ill be reviewed for any', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 119.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.76it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:13:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 82.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.84it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:13:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.50it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:13:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.97it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:13:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.04it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:13:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.65it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 74.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.48it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.20it/s]\n","Ref_row:56 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 99.82it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.02it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.03it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.90it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.78it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.96it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 93.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 98.59it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 106.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.48it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.92it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:14:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.07it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:15:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.63it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:15:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.84it/s]\n","Ref_row:56 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:15:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.58it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:15:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.50it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:15:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.18it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:15:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:15:47 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1624 [type=json_invalid, input_value='{\"title\": \"Humidity Excu... is under investigation', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 127.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.42it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:15:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.57it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:15:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.01it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:15:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.13it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:16:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 100.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.69it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:16:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.31it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:16:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.48it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:16:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:16:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1580 [type=json_invalid, input_value='{\"title\": \"Humidity Excu...estigation was launched', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 136.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.71it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:16:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.47it/s]\n","Ref_row:57 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:16:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.51it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:16:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.46it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:16:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.02it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:17:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.50it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:17:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.27it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:17:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.23it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:17:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 81.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.56it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:17:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.87it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:17:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.96it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:17:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.43it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:17:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 109.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.04it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:17:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.50it/s]\n","Ref_row:57 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:18:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.50it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:18:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 66.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.03it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:18:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:18:22 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1289 [type=json_invalid, input_value='{\"title\": \"Report\", \"rep... as follows: The report', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 92.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 104.07it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:18:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.93it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:18:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.58it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:18:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:18:43 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1289 [type=json_invalid, input_value='{\"title\": \"Report\", \"rep... as follows: The report', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 135.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.57it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:18:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 73.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.87it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:18:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.39it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:18:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 76.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.21it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:18:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.54it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:19:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.61it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:19:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.92it/s]\n","Ref_row:58 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:19:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.22it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:19:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.27it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:19:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.79it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:19:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.83it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:19:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.22it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:19:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.08it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:19:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 77.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.89it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:19:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 144.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.42it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:20:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 97.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.84it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:20:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.98it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:20:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 78.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.73it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:20:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.06it/s]\n","Ref_row:58 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:20:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.93it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:20:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.10it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:20:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.38it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:20:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.29it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:20:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.90it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:20:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:21:11 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1451 [type=json_invalid, input_value='{\"title\": \"Autoclave Ste...uspected to be a faulty', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 107.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.63it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:21:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.29it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:21:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.93it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:21:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 103.75it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:21:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 143.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.75it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:21:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.66it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:21:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 141.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.42it/s]\n","Ref_row:59 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:21:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.95it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:21:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.34it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:22:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.47it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:22:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 88.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.57it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:22:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.47it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:22:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 103.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.04it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:22:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.45it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:22:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 83.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.84it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:22:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.08it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:22:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 87.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.24it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:22:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 111.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.21it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:23:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.23it/s]\n","Ref_row:59 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:23:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.45it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:23:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:23:33 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1589 [type=json_invalid, input_value='{\"title\": \"Failure to Co...th a new batch. The SOP', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 135.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.61it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:23:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:23:50 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1633 [type=json_invalid, input_value='{\"title\": \"Failure to Co...the failure. The report', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 104.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.22it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:23:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.57it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:23:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:24:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1631 [type=json_invalid, input_value='{\"title\": \"Failure to Co...y investigated, and the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 132.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.16it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:24:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.64it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:24:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.43it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:24:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.54it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:24:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.16it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:24:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.27it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:24:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.26it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:24:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.00it/s]\n","Ref_row:60 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:24:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.83it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:24:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.59it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:24:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.96it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:25:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.24it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:25:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.43it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:25:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.34it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:25:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.88it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:25:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.12it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:25:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.51it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:25:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.32it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:25:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.98it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:25:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.90it/s]\n","Ref_row:60 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:25:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.10it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:26:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.79it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:26:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.83it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:26:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.99it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:26:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.50it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:26:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.82it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:26:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 73.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.04it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:26:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.59it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:27:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.93it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:27:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.12it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:27:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.35it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:27:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.33it/s]\n","Ref_row:61 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:27:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.51it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:27:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.39it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:27:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.98it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:27:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.52it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:28:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.10it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:28:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 103.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.59it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:28:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.05it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:28:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 87.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.24it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:28:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.07it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:28:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 100.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.84it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:28:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.81it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:28:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.99it/s]\n","Ref_row:61 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:28:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.78it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:28:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.82it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:29:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.48it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:29:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:29:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1696 [type=json_invalid, input_value='{\"title\": \"HPLC System F... being investigated and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 139.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.54it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:29:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.72it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:29:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 80.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.01it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:29:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.99it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:29:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 70.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.89it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:29:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.88it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:29:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.90it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:29:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.00it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:30:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.78it/s]\n","Ref_row:62 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:30:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.64it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:30:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.76it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:30:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.12it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:30:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.64it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:30:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.10it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:30:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.47it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:30:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.88it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:30:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 103.26it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:30:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 93.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.45it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:30:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 90.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 99.21it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:31:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.11it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:31:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 77.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.35it/s]\n","Ref_row:62 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:31:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.12it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:31:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 62.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.74it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:31:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.11it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:31:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 97.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.56it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:31:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.06it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:31:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.68it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:32:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.28it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:32:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.87it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:32:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.29it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:32:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.58it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:32:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.96it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:32:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.84it/s]\n","Ref_row:63 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:32:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.47it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:32:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.92it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:33:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.93it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:33:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.43it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:33:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 73.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.59it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:33:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.81it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:33:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 80.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.90it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:33:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.40it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:33:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 90.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.90it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:33:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.12it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:33:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.42it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:33:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 106.64it/s]\n","Ref_row:63 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:33:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.53it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:34:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 141.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.97it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:34:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:34:17 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1324 [type=json_invalid, input_value='{\"title\": \"Report\", \"rep...he event: The following', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 137.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.12it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:34:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 71.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.82it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:34:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.53it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:34:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.58it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:34:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.05it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:34:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 83.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 89.54it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:34:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.81it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:34:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.81it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:35:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.99it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:35:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.36it/s]\n","Ref_row:64 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:35:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 75.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.09it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:35:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.90it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:35:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 97.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.74it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:35:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 111.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.30it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:35:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 107.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.36it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:35:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.70it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:35:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.92it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:35:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.34it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:36:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.67it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:36:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 78.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.01it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:36:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.15it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:36:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 107.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 100.01it/s]\n","Ref_row:64 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:36:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.54it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:36:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.10it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:36:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 76.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.66it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:36:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:37:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1741 [type=json_invalid, input_value='{\"title\": \"Incomplete Cl...he relevant authorities', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 144.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.68it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:37:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 73.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.64it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:37:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:37:31 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1555 [type=json_invalid, input_value='{\"title\": \"Fermenter F-1...e amended and retrained', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 132.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.21it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:37:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 144.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.06it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:37:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 108.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.32it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:37:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:38:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1662 [type=json_invalid, input_value='{\"title\": \"Fermenter F-1.... The incident has been', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 140.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.69it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:38:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.23it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:38:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.38it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:38:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 47.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 51.07it/s]\n","Ref_row:65 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:38:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.63it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:38:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.61it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:38:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 112.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.43it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:38:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 81.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.71it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:38:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.60it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:38:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.65it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:38:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 77.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.94it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.88it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.98it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 105.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.29it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.92it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 102.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.03it/s]\n","Ref_row:65 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.75it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.11it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 98.92it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.49it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.79it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 80.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.64it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:39:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.58it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:40:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 83.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 82.09it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:40:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 147.71it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:40:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.28it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:40:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.62it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:40:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.63it/s]\n","Ref_row:66 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:40:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.33it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:40:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.73it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:40:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.42it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:40:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.14it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:41:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.86it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:41:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 73.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.29it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:41:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.43it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:41:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.96it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:41:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 102.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 102.11it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:41:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.75it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:41:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.52it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:41:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.78it/s]\n","Ref_row:66 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:41:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.61it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:42:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.16it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:42:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.13it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:42:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.79it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:42:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.01it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:42:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:42:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1714 [type=json_invalid, input_value='{\"title\": \"Missed Schedu...ing technician, and the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 106.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.02it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:42:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 84.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.33it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:42:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.18it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:42:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 109.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.72it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:43:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.81it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:43:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.72it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:43:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.88it/s]\n","Ref_row:67 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:43:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.59it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:43:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.99it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:43:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.06it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:43:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.47it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:43:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.47it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:43:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.00it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:43:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.53it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:43:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 111.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.26it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:44:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.98it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:44:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 88.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.09it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:44:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 102.90it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:44:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 98.86it/s]\n","Ref_row:67 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:44:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.95it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:44:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.67it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:44:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.50it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:44:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.84it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:44:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.92it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:45:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.36it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:45:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.46it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:45:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 79.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.26it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:45:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 103.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.01it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:45:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 62.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.52it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:45:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.47it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:45:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.27it/s]\n","Ref_row:68 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:45:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.92it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:45:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 97.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.79it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.52it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.98it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.91it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.58it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.46it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.14it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.09it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 91.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.95it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.43it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 106.20it/s]\n","Ref_row:68 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.19it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:46:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.28it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:47:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.53it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:47:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.90it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:47:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.66it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:47:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:47:41 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1625 [type=json_invalid, input_value='{\"title\": \"Documentation... document was corrected', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 119.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.59it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:47:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.05it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:47:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.12it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:47:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.46it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:47:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.20it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:47:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.04it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:48:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.39it/s]\n","Ref_row:69 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:48:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 84.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.33it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:48:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.32it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:48:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 106.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 102.77it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:48:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.70it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:48:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.20it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:48:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.82it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:48:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.68it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:48:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.75it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:48:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 84.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.01it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:48:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 103.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 103.75it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:49:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.25it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:49:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 107.98it/s]\n","Ref_row:69 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:49:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 115.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.60it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:49:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.95it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:49:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.98it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:49:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 103.30it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:49:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.13it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:49:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:50:07 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1547 [type=json_invalid, input_value='{\"title\": \"Temperature E...was not a result of any', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 129.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.43it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:50:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:50:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1674 [type=json_invalid, input_value='{\"title\": \"Temperature E...tive actions were taken', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 128.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.02it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:50:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.47it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:50:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.27it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:50:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.47it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:50:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.91it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:51:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 106.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.16it/s]\n","Ref_row:70 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:51:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.54it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:51:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.81it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:51:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 104.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.41it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:51:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 110.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 107.61it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:51:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.42it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:52:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.88it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:52:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.74it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:52:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.04it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:52:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.51it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:52:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:52:45 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 618 [type=json_invalid, input_value='{\"title\": \"Raw Material ...EV-2025-0663).\\\\\",\\\\\"}}', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 130.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.48it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:52:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 75.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 66.21it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:52:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 104.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 99.20it/s]\n","Ref_row:70 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:53:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.74it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:53:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.46it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:53:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.72it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:53:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:53:34 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1512 [type=json_invalid, input_value='{\"title\": \"Equipment Cal...duled for April 15, 202', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 131.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.76it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:53:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.40it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:53:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 83.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.40it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:53:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 108.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.72it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 70.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.61it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:53:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.31it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:54:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.15it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:54:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 102.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.18it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:54:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 78.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.38it/s]\n","Ref_row:71 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:54:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 66.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.52it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:54:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.48it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:54:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.63it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:54:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 84.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 85.52it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:54:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.35it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:54:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 86.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 92.62it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:55:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 107.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.97it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:55:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.38it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:55:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.20it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:55:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.83it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:55:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.41it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:55:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.15it/s]\n","Ref_row:71 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:55:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.17it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:55:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.05it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:55:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.01it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:56:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.41it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:56:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.30it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:56:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.51it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:56:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.94it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:56:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.82it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:56:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.99it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:56:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.70it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:56:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.73it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:57:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.94it/s]\n","Ref_row:72 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:57:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 108.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 92.94it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:57:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.47it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:57:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 86.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.23it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:57:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.36it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:57:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.68it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:57:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.10it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:57:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.97it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:57:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.97it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:57:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.05it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:57:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.41it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:58:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.30it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:58:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 78.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.43it/s]\n","Ref_row:72 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:58:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.35it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:58:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.87it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:58:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.55it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:58:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:58:53 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1729 [type=json_invalid, input_value='{\"title\": \"Deviation Rep...ocedures and regulatory', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 122.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.84it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:58:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.25it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:59:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 09:59:19 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1691 [type=json_invalid, input_value='{\"title\": \"Deviation Rep...g personnel involved in', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 125.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.93it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:59:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.33it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:59:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.60it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:59:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.54it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:59:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.45it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:59:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.13it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:59:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.75it/s]\n","Ref_row:73 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 09:59:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.96it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:00:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.18it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:00:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.91it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:00:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.51it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:00:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.17it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:00:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.84it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:00:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.72it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:00:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.53it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:00:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.77it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:00:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.48it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:00:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.34it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:00:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.20it/s]\n","Ref_row:73 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:01:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 80.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 97.85it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:01:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.98it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:01:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.71it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:01:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.27it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:01:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.11it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:01:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.52it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:01:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.93it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:01:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.87it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:01:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.17it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:02:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 84.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 100.30it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:02:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 10:02:26 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1671 [type=json_invalid, input_value='{\"title\": \"Mislabeling o...abase. The incident was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 136.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.44it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:02:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.19it/s]\n","Ref_row:74 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:02:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 10:02:47 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1482 [type=json_invalid, input_value='{\"title\": \"Report: Misla...d have an action on the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 136.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.47it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:02:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.08it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:02:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.33it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:02:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.00it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:03:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.08it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:03:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 115.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.49it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:03:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 142.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.31it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.83it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:03:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.82it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:03:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.68it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:03:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 102.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 77.44it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:03:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.19it/s]\n","Ref_row:74 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:03:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.56it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:03:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.60it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:03:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 69.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.63it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:04:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.42it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:04:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 10:04:32 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1644 [type=json_invalid, input_value='{\"title\": \"Unexpected Fa... assessment is expected', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 132.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.95it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:04:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.02it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:04:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.15it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:04:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.47it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:04:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.03it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.13it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:05:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 100.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.46it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:05:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.56it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:05:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.33it/s]\n","Ref_row:75 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:05:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.79it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:05:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 104.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.35it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:05:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 91.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.08it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:05:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 112.11it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:05:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.38it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:05:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.80it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:06:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.88it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:06:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.86it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:06:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 97.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 92.87it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:06:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.56it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:06:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.76it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:06:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.04it/s]\n","Ref_row:75 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:06:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.72it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:07:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 107.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 100.50it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:07:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.29it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:07:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.20it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:07:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.79it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:07:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.57it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:07:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.84it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:07:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.56it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:07:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.99it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:07:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 90.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.03it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:08:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 10:08:24 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1505 [type=json_invalid, input_value='{\"title\": \"Use of Expire...olved. The incident was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 124.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.61it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:08:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.51it/s]\n","Ref_row:76 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:08:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 86.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.54it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:08:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.83it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:08:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.07it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:08:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.88it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:08:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.11it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:08:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.84it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:09:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.15it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:09:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.14it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:09:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 107.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.30it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:09:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 102.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.57it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:09:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 110.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 106.54it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:09:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 46.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 96.97it/s]\n","Ref_row:76 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:09:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.75it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:09:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 10:10:06 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1384 [type=json_invalid, input_value='{\"title\": \"Sample Miside...man labeling error. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 103.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 99.98it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:10:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 10:10:23 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1384 [type=json_invalid, input_value='{\"title\": \"Sample Miside...man labeling error. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 133.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.64it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:10:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.99it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:10:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.38it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:10:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.68it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:10:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 93.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.78it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:10:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 107.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 101.73it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:11:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.47it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:11:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 112.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 105.71it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:11:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 110.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.60it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:11:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 91.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 79.36it/s]\n","Ref_row:77 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:11:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 102.77it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:11:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.49it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:11:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 78.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 67.24it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:11:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.36it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:11:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.16it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:12:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.46it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:12:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.36it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:12:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 78.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 83.29it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:12:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.21it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:12:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 102.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 100.42it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:12:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.24it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:12:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 108.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 107.53it/s]\n","Ref_row:77 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:12:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.36it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:12:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.02it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:12:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 115.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.43it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:13:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.94it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:13:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.63it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:13:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 77.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.55it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:13:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.97it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:13:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.09it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:13:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.31it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:13:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 109.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.65it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:13:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.08it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:13:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.98it/s]\n","Ref_row:78 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.43it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.63it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 95.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.77it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.93it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.50it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.85it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.13it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 102.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 99.49it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.97it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.70it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 107.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.11it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:14:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.48it/s]\n","Ref_row:78 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:15:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 135.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.50it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:15:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 88.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.68it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:15:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.41it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:15:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.38it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:15:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 10:15:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1532 [type=json_invalid, input_value='{\"title\": \"Report\", \"rep...raining of the cleaning', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 135.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.72it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:15:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.14it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:15:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 104.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 100.62it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:15:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 82.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.07it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:15:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.74it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:15:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.49it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:15:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.85it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:16:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.82it/s]\n","Ref_row:79 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:16:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.83it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:16:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.36it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:16:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.96it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:16:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.51it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:16:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.48it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:16:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.96it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:17:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.39it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:17:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.89it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:17:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 91.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.46it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:17:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.80it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:17:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 82.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 84.27it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:17:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.10it/s]\n","Ref_row:79 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:17:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 142.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.05it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:17:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 73.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.67it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:17:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.01it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:18:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.88it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:18:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.51it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:18:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.83it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:18:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.83it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:18:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.90it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:18:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.62it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:18:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.97it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:18:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.77it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:19:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.18it/s]\n","Ref_row:80 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:19:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.60it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:19:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.21it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:19:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.81it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:19:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.87it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:19:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.99it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:19:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.08it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:19:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.54it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:19:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.43it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:19:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.98it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:19:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.36it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:20:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 113.73it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:20:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.97it/s]\n","Ref_row:80 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 10:20:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.20it/s]\n","***** Starting statistical analyisis for the experiment_id=meta-llama-Llama-3.2-1B-Instruct-17-082025_06-51-03 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-meta-llama-Llama-3.2-1B-Instruct-17-082025_06-51-03.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_B-meta-llama-Llama-3.2-1B-Instruct-17-082025_06-51-03.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_C-meta-llama-Llama-3.2-1B-Instruct-17-082025_06-51-03.xlsx\n","reportParamGridSearch time --- 209.5303919951121 minutes ---\n"]}]},{"cell_type":"code","source":["from huggingface_hub import login\n","login(token=userdata.get('HF_TOKEN'))  # insert your Hugging Face token here\n","\n","# !python app/reportParamGridSearch.py --model_id microsoft/phi-2 --prompt_method B C --max_workers 12 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 30  --temperature 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 30 50 70 --max_new_tokens 300 --do_sample True\n","\n","!python app/reportParamGridSearch.py --model_id meta-llama/Llama-3.2-3B-Instruct --non-threaded --prompt_method B C --max_workers 4 --dataset_filename pharma_dev_reports_collection.xlsx --start_idx 1 --end_idx 10  --temperature 0.3 0.7 1.0 1.3 --top_p 0.3 0.6 0.9 --top_k 50 --max_new_tokens 300 --do_sample True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fo_UwI91mmXR","executionInfo":{"status":"ok","timestamp":1755431134172,"user_tz":-120,"elapsed":2829718,"user":{"displayName":"Samd Guizani","userId":"17513899638189689902"}},"outputId":"7fe94694-fd49-44e8-deb8-6b46a3dad44b"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-08-17 10:58:31.469653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755428311.491132   84464 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755428311.498196   84464 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755428311.518168   84464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755428311.518206   84464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755428311.518211   84464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755428311.518215   84464 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","Parameters passed to main script: \n","{'max_workers': [4], 'threaded': False, 'model_id': ['meta-llama/Llama-3.2-3B-Instruct'], 'prompt_method': ['B', 'C'], 'dataset_filename': 'pharma_dev_reports_collection.xlsx', 'start_idx': [1], 'end_idx': [10], 'temperature': [0.3, 0.7, 1.0, 1.3], 'top_p': [0.3, 0.6, 0.9], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","generation_config.json: 100% 189/189 [00:00<00:00, 1.28MB/s]\n","08/17/2025 10:58:47 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=meta-llama/Llama-3.2-3B-Instruct\n","08/17/2025 10:58:47 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=meta-llama/Llama-3.2-3B-Instruct\n","08/17/2025 10:58:47 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=meta-llama/Llama-3.2-3B-Instruct\n","Generation parameters: \n","{'temperature': [0.3, 0.7, 1.0, 1.3], 'top_p': [0.3, 0.6, 0.9], 'top_k': [50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","tokenizer_config.json: 100% 54.5k/54.5k [00:00<00:00, 2.86MB/s]\n","tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 30.8MB/s]\n","special_tokens_map.json: 100% 296/296 [00:00<00:00, 1.95MB/s]\n","config.json: 100% 878/878 [00:00<00:00, 5.51MB/s]\n","model.safetensors.index.json: 100% 20.9k/20.9k [00:00<00:00, 72.6MB/s]\n","Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n","model-00002-of-00002.safetensors:   0% 0.00/1.46G [00:00<?, ?B/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   0% 354k/4.97G [00:00<3:44:53, 368kB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:   0% 5.82M/1.46G [00:01<05:49, 4.16MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   1% 12.1M/1.46G [00:01<03:01, 7.95MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   2% 24.1M/1.46G [00:02<01:59, 12.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   2% 35.8M/1.46G [00:02<01:15, 18.8MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   3% 48.3M/1.46G [00:02<00:55, 25.3MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:   0% 6.59M/4.97G [00:02<34:15, 2.41MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:   4% 54.6M/1.46G [00:03<00:58, 24.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   5% 73.0M/1.46G [00:04<00:59, 23.3MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   8% 109M/1.46G [00:04<00:30, 43.8MB/s] \u001b[A\n","model-00002-of-00002.safetensors:   8% 122M/1.46G [00:04<00:36, 37.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:   9% 136M/1.46G [00:05<00:43, 30.2MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  11% 159M/1.46G [00:06<00:49, 26.6MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  14% 201M/1.46G [00:06<00:25, 49.0MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  15% 220M/1.46G [00:06<00:23, 52.9MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  17% 249M/1.46G [00:07<00:23, 52.5MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  18% 261M/1.46G [00:08<00:30, 39.0MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:   1% 38.6M/4.97G [00:08<16:41, 4.92MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   1% 44.8M/4.97G [00:09<15:28, 5.30MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  20% 286M/1.46G [00:09<00:41, 28.6MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:   1% 63.8M/4.97G [00:09<09:12, 8.87MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   1% 70.0M/4.97G [00:10<10:22, 7.87MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   3% 144M/4.97G [00:12<04:11, 19.2MB/s] \u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  24% 353M/1.46G [00:13<00:51, 21.4MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  26% 382M/1.46G [00:13<00:39, 27.4MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:   3% 162M/4.97G [00:13<03:53, 20.6MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   3% 172M/4.97G [00:13<03:32, 22.5MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   5% 239M/4.97G [00:14<01:47, 44.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   5% 260M/4.97G [00:14<01:46, 44.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   5% 267M/4.97G [00:14<01:47, 43.9MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  31% 449M/1.46G [00:14<00:29, 34.2MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:   6% 276M/4.97G [00:15<01:51, 42.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   6% 285M/4.97G [00:19<07:34, 10.3MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  33% 478M/1.46G [00:19<00:55, 17.6MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:   6% 294M/4.97G [00:19<06:21, 12.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   7% 326M/4.97G [00:19<03:23, 22.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   7% 363M/4.97G [00:23<05:28, 14.0MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  37% 545M/1.46G [00:23<00:54, 16.8MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:   8% 388M/4.97G [00:24<04:21, 17.5MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:   9% 455M/4.97G [00:24<02:30, 30.0MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  42% 612M/1.46G [00:25<00:39, 21.6MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  10% 500M/4.97G [00:25<02:00, 37.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  12% 573M/4.97G [00:26<01:19, 55.1MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  44% 641M/1.46G [00:26<00:37, 22.1MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  12% 592M/4.97G [00:27<01:56, 37.4MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  45% 663M/1.46G [00:27<00:39, 20.0MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  13% 652M/4.97G [00:28<01:37, 44.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  13% 668M/4.97G [00:28<01:37, 44.2MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  47% 691M/1.46G [00:28<00:35, 21.5MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  14% 674M/4.97G [00:29<01:58, 36.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  14% 716M/4.97G [00:29<01:18, 54.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  15% 768M/4.97G [00:31<01:27, 47.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  16% 784M/4.97G [00:31<01:20, 51.7MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  16% 816M/4.97G [00:31<01:26, 48.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  17% 828M/4.97G [00:32<01:53, 36.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  17% 835M/4.97G [00:32<01:52, 36.6MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  51% 746M/1.46G [00:33<00:41, 17.1MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  55% 800M/1.46G [00:35<00:35, 18.4MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  18% 895M/4.97G [00:35<02:34, 26.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  18% 908M/4.97G [00:35<02:20, 29.0MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  58% 849M/1.46G [00:37<00:30, 19.8MB/s]\u001b[A\n","model-00002-of-00002.safetensors:  62% 904M/1.46G [00:38<00:19, 28.2MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  19% 921M/4.97G [00:38<04:09, 16.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  19% 953M/4.97G [00:38<02:51, 23.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  21% 1.02G/4.97G [00:39<01:29, 44.1MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  66% 959M/1.46G [00:39<00:17, 29.0MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  21% 1.04G/4.97G [00:40<01:46, 36.7MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:40<01:15, 51.6MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  23% 1.14G/4.97G [00:40<00:57, 66.6MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  74% 1.08G/1.46G [00:43<00:12, 29.7MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  24% 1.21G/4.97G [00:43<01:42, 36.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  25% 1.24G/4.97G [00:44<01:22, 45.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  26% 1.28G/4.97G [00:48<02:41, 22.9MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  27% 1.35G/4.97G [00:48<01:39, 36.3MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  78% 1.14G/1.46G [00:48<00:15, 21.4MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  29% 1.42G/4.97G [00:54<02:49, 20.9MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  82% 1.20G/1.46G [00:54<00:14, 17.2MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  29% 1.44G/4.97G [00:54<02:47, 21.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  30% 1.50G/4.97G [01:00<03:39, 15.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  32% 1.57G/4.97G [01:01<02:31, 22.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  33% 1.62G/4.97G [01:03<02:24, 23.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  33% 1.66G/4.97G [01:03<01:58, 28.0MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  35% 1.72G/4.97G [01:04<01:31, 35.6MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  36% 1.79G/4.97G [01:04<01:02, 50.8MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  86% 1.26G/1.46G [01:05<00:19, 10.5MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  37% 1.84G/4.97G [01:05<00:56, 55.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  37% 1.85G/4.97G [01:05<00:55, 55.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  38% 1.91G/4.97G [01:06<00:49, 61.8MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  91% 1.33G/1.46G [01:06<00:09, 14.0MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  39% 1.92G/4.97G [01:06<00:51, 58.7MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  39% 1.93G/4.97G [01:07<00:53, 56.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  40% 1.99G/4.97G [01:09<01:19, 37.5MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  40% 2.01G/4.97G [01:09<01:10, 42.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  41% 2.03G/4.97G [01:09<00:57, 51.0MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  41% 2.05G/4.97G [01:09<00:56, 51.7MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  43% 2.12G/4.97G [01:10<00:40, 69.6MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  44% 2.19G/4.97G [01:14<01:33, 29.7MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  95% 1.39G/1.46G [01:14<00:05, 11.6MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  45% 2.26G/4.97G [01:15<01:10, 38.5MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  46% 2.28G/4.97G [01:15<01:06, 40.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  46% 2.29G/4.97G [01:16<01:07, 39.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  46% 2.31G/4.97G [01:18<02:03, 21.6MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  48% 2.38G/4.97G [01:19<01:18, 32.9MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  48% 2.39G/4.97G [01:20<01:10, 36.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  49% 2.42G/4.97G [01:20<00:57, 44.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  49% 2.45G/4.97G [01:20<00:47, 53.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  50% 2.48G/4.97G [01:20<00:36, 68.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  51% 2.55G/4.97G [01:26<02:03, 19.5MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors:  95% 1.39G/1.46G [01:27<00:05, 11.6MB/s]\u001b[A\n","\n","model-00001-of-00002.safetensors:  52% 2.59G/4.97G [01:27<01:30, 26.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  52% 2.61G/4.97G [01:27<01:29, 26.5MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  54% 2.67G/4.97G [01:28<00:57, 40.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  55% 2.74G/4.97G [01:29<00:50, 44.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  56% 2.77G/4.97G [01:30<00:45, 47.9MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  57% 2.83G/4.97G [01:30<00:36, 58.1MB/s]\u001b[A\u001b[A\n","model-00002-of-00002.safetensors: 100% 1.46G/1.46G [01:37<00:00, 15.0MB/s]\n","\n","\n","model-00001-of-00002.safetensors:  57% 2.85G/4.97G [01:37<02:09, 16.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  59% 2.92G/4.97G [01:37<01:15, 27.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  61% 3.01G/4.97G [01:41<01:20, 24.5MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  62% 3.06G/4.97G [01:41<00:59, 32.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  62% 3.09G/4.97G [01:41<00:49, 37.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  63% 3.11G/4.97G [01:41<00:44, 41.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  63% 3.13G/4.97G [01:42<00:40, 45.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  63% 3.14G/4.97G [01:42<00:38, 47.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  65% 3.21G/4.97G [01:47<01:25, 20.6MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  66% 3.30G/4.97G [01:47<00:46, 35.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  67% 3.32G/4.97G [01:48<00:45, 35.9MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  68% 3.38G/4.97G [01:50<00:47, 33.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  69% 3.45G/4.97G [01:51<00:33, 44.7MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  71% 3.50G/4.97G [01:51<00:25, 58.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  72% 3.57G/4.97G [01:52<00:20, 68.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  73% 3.63G/4.97G [01:55<00:40, 33.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  74% 3.69G/4.97G [01:56<00:27, 46.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  75% 3.75G/4.97G [01:56<00:21, 55.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  77% 3.81G/4.97G [01:57<00:19, 58.6MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  78% 3.88G/4.97G [01:57<00:13, 82.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  79% 3.93G/4.97G [02:01<00:30, 34.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  80% 3.98G/4.97G [02:07<00:51, 19.0MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  81% 4.04G/4.97G [02:08<00:35, 26.0MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  83% 4.10G/4.97G [02:08<00:26, 33.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  84% 4.17G/4.97G [02:09<00:19, 41.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  85% 4.24G/4.97G [02:10<00:13, 53.3MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  87% 4.30G/4.97G [02:16<00:27, 24.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  88% 4.37G/4.97G [02:16<00:18, 32.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  89% 4.43G/4.97G [02:16<00:12, 43.6MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  91% 4.50G/4.97G [02:17<00:08, 55.5MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  92% 4.57G/4.97G [02:19<00:08, 47.4MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  93% 4.63G/4.97G [02:19<00:05, 60.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  95% 4.70G/4.97G [02:20<00:04, 65.2MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  96% 4.76G/4.97G [02:20<00:02, 82.8MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  97% 4.83G/4.97G [02:21<00:01, 97.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors:  99% 4.90G/4.97G [02:28<00:02, 25.1MB/s]\u001b[A\u001b[A\n","\n","model-00001-of-00002.safetensors: 100% 4.97G/4.97G [02:28<00:00, 33.4MB/s]\n","Fetching 2 files: 100% 2/2 [02:29<00:00, 74.55s/it] \n","Loading checkpoint shards: 100% 2/2 [00:26<00:00, 13.30s/it]\n","Results file is expected to have 240 rows.\n","******* Starting GRID SEARCH ***********\n","******* Starting NOT THREADED PROCESS ************\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:01:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 48.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 58.76it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:02:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.16it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:02:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.70it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:02:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.66it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:02:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.51it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:02:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.45it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:03:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.39it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:03:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.81it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:03:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.91it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:03:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 73.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.77it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:03:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.72it/s]\n","Ref_row:1 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:04:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 95.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.81it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:04:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.98it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:04:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.33it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:04:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 115.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.17it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:04:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 95.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 97.47it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:05:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.30it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:05:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.81it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:05:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.61it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:05:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.62it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:05:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.72it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:05:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.72it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:06:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 93.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.68it/s]\n","Ref_row:1 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:06:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.58it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:06:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 111.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.18it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:06:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.88it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:06:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 61.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.52it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:06:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.70it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:07:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.83it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:07:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.52it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:07:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.03it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:07:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.71it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:07:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 73.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.04it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:08:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.71it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:08:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.49it/s]\n","Ref_row:2 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:08:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.39it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:08:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.51it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:08:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 71.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.34it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:08:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.33it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:09:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.24it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:09:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.26it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:09:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.44it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:09:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 70.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 80.90it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:09:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.39it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:09:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.42it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:10:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 76.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.23it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:10:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.55it/s]\n","Ref_row:2 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:10:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.70it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:10:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.02it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:10:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.70it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:10:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 71.20it/s]\n","Batches: 100% 1/1 [00:00<00:00, 74.29it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:11:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 66.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.49it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:11:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.37it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:11:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.64it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:11:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.01it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:11:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.87it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:12:06 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.04it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:12:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.44it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:12:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 75.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.36it/s]\n","Ref_row:3 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:12:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 65.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 64.52it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:12:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.81it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.15it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:13:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 90.06it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:13:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.99it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:13:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 77.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 60.08it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:13:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.43it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:13:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.33it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:13:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 81.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.41it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:14:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.82it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:14:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.18it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:14:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 75.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.98it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:14:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 69.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.38it/s]\n","Ref_row:3 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:14:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.21it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:14:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.91it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:14:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.96it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:15:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 85.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 89.16it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:15:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.56it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:15:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.26it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:15:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.03it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:15:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 48.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 65.67it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:15:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 110.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.85it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:16:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.87it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:16:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 74.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.45it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:16:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 92.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.45it/s]\n","Ref_row:4 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:16:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 96.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 100.38it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:16:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.87it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:17:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 73.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.21it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:17:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.37it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:17:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 50.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.61it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:17:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.41it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:17:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.20it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:17:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.39it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:18:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.79it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:18:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.48it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:18:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 76.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 68.52it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:18:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.37it/s]\n","Ref_row:4 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:18:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.95it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:18:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.94it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:19:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.75it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:19:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 75.66it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:19:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.48it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:19:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.33it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:19:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 115.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 81.80it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:20:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.97it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:20:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.22it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:20:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 109.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.54it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:20:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.17it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:20:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 90.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.66it/s]\n","Ref_row:5 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:21:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 64.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.37it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:21:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.76it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:21:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.86it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:21:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.88it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:21:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.92it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:21:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 90.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 97.36it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:22:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.01it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:22:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.52it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:22:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 118.84it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:22:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.61it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:22:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 64.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 72.94it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:23:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.39it/s]\n","Ref_row:5 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:23:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.91it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:23:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 104.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.30it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:23:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.67it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:23:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.34it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:24:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.25it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:24:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 109.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.25it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:24:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.07it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:24:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.07it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:24:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 115.68it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:25:02 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 84.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 94.11it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:25:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 93.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 96.25it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:25:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.06it/s]\n","Ref_row:6 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:25:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 88.69it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:25:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 106.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.35it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:26:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.23it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:26:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.72it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:26:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.61it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:26:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.06it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:26:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 70.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.98it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:27:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 66.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 71.20it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:27:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.66it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:27:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.79it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:27:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.04it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:27:46 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 104.09it/s]\n","Ref_row:6 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:27:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.65it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:28:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.54it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:28:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.75it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:28:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 90.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 92.28it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:28:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.21it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.35it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:28:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 83.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 55.02it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:29:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 84.25it/s]\n","Batches: 100% 1/1 [00:00<00:00, 69.36it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:29:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.79it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:29:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.29it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:29:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 67.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 95.39it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:29:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.85it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:29:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.21it/s]\n","Ref_row:7 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:29:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 86.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 86.34it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:29:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.56it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:30:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 86.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.43it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:30:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.13it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:30:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.97it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:30:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.58it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:30:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.18it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:31:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 88.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 91.13it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:31:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.91it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:31:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 115.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.36it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:31:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.63it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:31:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.64it/s]\n","Ref_row:7 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:31:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 77.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 78.86it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:32:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 69.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 87.52it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:32:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 97.33it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:32:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.03it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:32:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 73.95it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:32:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.82it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:33:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.45it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:33:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.46it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:33:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.97it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.58it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:33:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.98it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:34:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.53it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:34:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.65it/s]\n","Ref_row:8 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:34:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.85it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:34:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 110.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 99.51it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:34:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 87.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 59.36it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:34:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.83it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:35:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.50it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:35:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 115.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.32it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:35:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.53it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:35:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 106.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 106.17it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:35:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 109.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.69it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:36:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.28it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:36:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 66.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.54it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:36:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.62it/s]\n","Ref_row:8 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:36:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 88.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 76.97it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:36:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.65it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:36:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 110.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.24it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:37:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.30it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:37:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.06it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:37:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.32it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:37:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.23it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:37:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.51it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:37:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.94it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:38:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 115.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 117.68it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:38:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.75it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:38:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.62it/s]\n","Ref_row:9 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:38:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 108.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 109.14it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:38:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.66it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:39:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.17it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:39:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.80it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:39:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.68it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:39:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.99it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:39:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 78.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 89.19it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:39:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.13it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:39:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.82it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:40:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.86it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:40:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 79.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.20it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:40:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.15it/s]\n","Ref_row:9 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:40:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.74it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:40:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 114.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.62it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:40:59 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 111.01it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:41:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.50it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:41:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 99.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 99.49it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:41:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","08/17/2025 11:41:59 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1457 [type=json_invalid, input_value='{\"title\": \"Unidentified ...vestigation is ongoing,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","Batches: 100% 1/1 [00:00<00:00, 103.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 102.16it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:42:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.14it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:42:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.82it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:42:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.80it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:42:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.73it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:42:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.18it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:42:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 73.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 97.52it/s]\n","Ref_row:10 & prompt_method=B: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:43:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 98.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 92.31it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:43:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.65it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:43:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.88it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:43:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.12it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:43:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.27it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:44:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.04it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:44:14 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 101.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 98.56it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:44:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 125.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.21it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:44:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 119.78it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.0, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:44:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.53it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.3, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:44:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 90.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.34it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.6, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:45:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 100.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 93.69it/s]\n","Ref_row:10 & prompt_method=C: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_p': 0.9, 'top_k': 50, 'max_new_tokens': 300.0, 'do_sample': True, 'repetition_penalty': 1.0}\n","08/17/2025 11:45:17 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.28it/s]\n","***** Starting statistical analyisis for the experiment_id=meta-llama-Llama-3.2-3B-Instruct-17-082025_11-01-47 ***** \n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-mean-meta-llama-Llama-3.2-3B-Instruct-17-082025_11-01-47.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_B-meta-llama-Llama-3.2-3B-Instruct-17-082025_11-01-47.xlsx\n","Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/analysis/an-stats-pm_C-meta-llama-Llama-3.2-3B-Instruct-17-082025_11-01-47.xlsx\n","reportParamGridSearch time --- 46.83900725841522 minutes ---\n"]}]},{"cell_type":"markdown","metadata":{"id":"JozIHMbjZGtI"},"source":["# Several models run in parallel\n","In linux command with the & we can run several programs in parallel\n","**Careful it fills GPU RAM quickly if models are > 2 B parameters**\n","\n","**NOTA:** Better use the Colab L4 GPU for charging two models in parallel. The L4 GPU size is 25 GB, so it could admit up to 5B parameters more or less, i.e.:\n","\n","- two models of at most 2.5 B parameters.\n","- Three models of 1 B parameters each\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yCGd0oBQZbTL"},"outputs":[],"source":["!python app/reportParamGridSearch.py --model_id microsoft/phi-2 --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True & python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM3-3B --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j0P1LHHOXp0d"},"outputs":[],"source":["# KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","\n","from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2561fc1a46bd4a0f9684ee6852f00f6e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ceecfa134d242d49aa0d6ab110dc682","IPY_MODEL_b1738a0d752c41a8a0e914d732593934","IPY_MODEL_0f5b9a3b4b564c0eafc027309a2bcba3"],"layout":"IPY_MODEL_bde31a60391e4239aeae3961ed8f658c"}},"5ceecfa134d242d49aa0d6ab110dc682":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8748d7c5b92041fd9f3461fc5bd85f77","placeholder":"","style":"IPY_MODEL_800af15cffae466a9448abf2f7dddb4a","value":"generation_config.json:100%"}},"b1738a0d752c41a8a0e914d732593934":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93b7f38d1698485eb3b636b46373cce6","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bbbf2ed359ee4a3f900718cadf9f9c49","value":124}},"0f5b9a3b4b564c0eafc027309a2bcba3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c07eca8b736a44439478af1369c9ac02","placeholder":"","style":"IPY_MODEL_2ee00ff5bf6046df91eadb1b6121d95e","value":"124/124[00:00&lt;00:00,13.1kB/s]"}},"bde31a60391e4239aeae3961ed8f658c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8748d7c5b92041fd9f3461fc5bd85f77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"800af15cffae466a9448abf2f7dddb4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93b7f38d1698485eb3b636b46373cce6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbbf2ed359ee4a3f900718cadf9f9c49":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c07eca8b736a44439478af1369c9ac02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ee00ff5bf6046df91eadb1b6121d95e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/olonok69/LLM_Notebooks/blob/main/ml_tricks/colab/colab_connect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"ubgiAABGmadO"},"source":["# This Colab notebook is ment to be executed from a GPU resources in Colab\n","The idea is to remotely execute the repo files (classes, main, etc)"]},{"cell_type":"markdown","metadata":{"id":"Kc3SGLPSX_7E"},"source":["# 0: Before starting, verifiy that: **After pulling the repo, you have copied the .env file into the Github repo !**"]},{"cell_type":"markdown","metadata":{"id":"r0sG1QyylCkW"},"source":["# 1 Mount Google Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1719,"status":"ok","timestamp":1755182121808,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"w69hWB_qkwLI","outputId":"887d8507-d403-45b4-fb9f-6dca909ed73d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"FPK232yxlHD1"},"source":["# 2 config Git"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":283,"status":"ok","timestamp":1755182122094,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"GradLm7NkxKN"},"outputs":[],"source":["!git config --global user.name \"zbotta\"\n","!git config --global user.email \"zbotta@proton.me\""]},{"cell_type":"markdown","metadata":{"id":"tQRJVtrHynSw"},"source":["## RUN THIS CELL ONLY ONCE!\n","To clone the Github repo"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":884,"status":"ok","timestamp":1755182122986,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"E1otG_64l1Qu"},"outputs":[],"source":["from google.colab import userdata\n","github_token = userdata.get('zbotta_token')\n","\n","token = github_token\n","username = \"zbotta\"\n","repo = 'reportingAgent'\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1614,"status":"ok","timestamp":1755168347891,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"PQgo_qSExuXm","outputId":"3e7d64bc-ce9a-4f1b-fa22-fb0dc7b6e70f"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path '/content/drive/MyDrive/GitHub/reportingAgent' already exists and is not an empty directory.\n"]}],"source":["!git clone https://{username}:{github_token}@github.com/{username}/{repo}.git /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83,"status":"ok","timestamp":1755182124648,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"ze8qK-7nmx_O","outputId":"91f5c87d-1bda-49aa-8375-23dd26ff34ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVgHJTXrHLIX"},"outputs":[],"source":["#!git remote set-url origin https://{username}:{github_token}@github.com/{username}/{repo}.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Rwp4A70m_2U"},"outputs":[],"source":["#!git remote get-url origin"]},{"cell_type":"markdown","metadata":{"id":"q0fNz_r7m4UG"},"source":["# Use git commands"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1072,"status":"ok","timestamp":1755182126573,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"ybZIZbg2-qhU","outputId":"3a4bfc97-006d-4c5c-d925-4ea6fae850de"},"outputs":[{"output_type":"stream","name":"stdout","text":["M\tPoC/PoC_Prompt and report gen.ipynb\n","M\tPoC/reportAgent-remote.ipynb\n","Already on 'dev'\n","Your branch is up to date with 'origin/dev'.\n"]}],"source":["!git fetch\n","!git checkout dev"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":879,"status":"ok","timestamp":1755182129062,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"g9D7zP_oRsaN","outputId":"79528b7d-b620-44ab-9fb9-ca6980caee97"},"outputs":[{"output_type":"stream","name":"stdout","text":["Already up to date.\n"]}],"source":["!git pull"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":580,"status":"ok","timestamp":1755168441953,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"ooqu1-hlm7Af","outputId":"b62b6c7a-e962-4106-f4f2-df257c6418dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["On branch dev\n","Your branch is behind 'origin/dev' by 4 commits, and can be fast-forwarded.\n","  (use \"git pull\" to update your local branch)\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   PoC/PoC_Prompt and report gen.ipynb\u001b[m\n","\t\u001b[31mmodified:   PoC/reportAgent-remote.ipynb\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}],"source":["!git status"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1754480985734,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"BsN8-zMZ8Z-n","outputId":"3e5c50cb-1172-4f17-da0f-f1e10830014c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Everything up-to-date\n"]}],"source":["!git push origin dev\n"]},{"cell_type":"markdown","metadata":{"id":"nXz98pxKo8oH"},"source":["# Using project scripts\n","\n","Reference : [Importing python library from Drive](https://colab.research.google.com/drive/12qC2abKAIAlUM_jNAokGlooKY-idbSxi#scrollTo=prUMpfLaB-D7)"]},{"cell_type":"markdown","metadata":{"id":"mvLEUb7VzVD0"},"source":["## Install project dependencies\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1755168444795,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"S6sPDJjVSHQf","outputId":"8fca2771-b2e5-4572-9863-bdd4491b53f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":202526,"status":"ok","timestamp":1755182112674,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"-lZSS9qFzSLk","outputId":"a24275b6-7fc9-4c5c-e704-4011171bafa5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: aiohappyeyeballs==2.6.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.1)\n","Collecting aiohttp==3.12.14 (from -r requirements.txt (line 2))\n","  Downloading aiohttp-3.12.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n","Requirement already satisfied: aiosignal==1.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.4.0)\n","Collecting airportsdata==20250706 (from -r requirements.txt (line 4))\n","  Downloading airportsdata-20250706-py3-none-any.whl.metadata (9.1 kB)\n","Requirement already satisfied: annotated-types==0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.7.0)\n","Collecting anyio==4.9.0 (from -r requirements.txt (line 6))\n","  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: attrs==25.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (25.3.0)\n","Collecting bert-score==0.3.13 (from -r requirements.txt (line 8))\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Collecting certifi==2025.7.14 (from -r requirements.txt (line 9))\n","  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n","Collecting charset-normalizer==3.4.2 (from -r requirements.txt (line 10))\n","  Downloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n","Requirement already satisfied: click==8.2.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (8.2.1)\n","Requirement already satisfied: cloudpickle==3.1.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (3.1.1)\n","Collecting colorama==0.4.6 (from -r requirements.txt (line 13))\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Collecting contourpy==1.3.2 (from -r requirements.txt (line 14))\n","  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n","Requirement already satisfied: cycler==0.12.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.12.1)\n","Requirement already satisfied: datasets==4.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (4.0.0)\n","Requirement already satisfied: dill==0.3.8 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (0.3.8)\n","Collecting diskcache==5.6.3 (from -r requirements.txt (line 18))\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: distro==1.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (1.9.0)\n","Requirement already satisfied: docstring_parser==0.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (0.17.0)\n","Collecting dotenv==0.9.9 (from -r requirements.txt (line 21))\n","  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n","Requirement already satisfied: et_xmlfile==2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (2.0.0)\n","Collecting evaluate==0.4.5 (from -r requirements.txt (line 23))\n","  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: fastapi==0.116.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (0.116.1)\n","Requirement already satisfied: filelock==3.18.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (3.18.0)\n","Requirement already satisfied: fonttools==4.59.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (4.59.0)\n","Requirement already satisfied: frozenlist==1.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (1.7.0)\n","Requirement already satisfied: fsspec==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (2025.3.0)\n","Collecting genson==1.3.0 (from -r requirements.txt (line 29))\n","  Downloading genson-1.3.0-py3-none-any.whl.metadata (28 kB)\n","Collecting groq==0.26.0 (from -r requirements.txt (line 30))\n","  Downloading groq-0.26.0-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: h11==0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (0.16.0)\n","Requirement already satisfied: httpcore==1.0.9 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (1.0.9)\n","Requirement already satisfied: httpx==0.28.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 33)) (0.28.1)\n","Collecting huggingface-hub==0.33.4 (from -r requirements.txt (line 34))\n","  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: idna==3.10 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 35)) (3.10)\n","Requirement already satisfied: iniconfig==2.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 36)) (2.1.0)\n","Collecting instructor==1.10.0 (from -r requirements.txt (line 37))\n","  Downloading instructor-1.10.0-py3-none-any.whl.metadata (11 kB)\n","Collecting interegular==0.3.3 (from -r requirements.txt (line 38))\n","  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n","Collecting iso3166==2.1.1 (from -r requirements.txt (line 39))\n","  Downloading iso3166-2.1.1-py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: Jinja2==3.1.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 40)) (3.1.6)\n","Requirement already satisfied: jiter==0.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 41)) (0.10.0)\n","Requirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 42)) (1.5.1)\n","Collecting jsonpath-ng==1.7.0 (from -r requirements.txt (line 43))\n","  Downloading jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: jsonschema==4.25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 44)) (4.25.0)\n","Requirement already satisfied: jsonschema-specifications==2025.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 45)) (2025.4.1)\n","Collecting kiwisolver==1.4.8 (from -r requirements.txt (line 46))\n","  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n","Collecting lark==1.2.2 (from -r requirements.txt (line 47))\n","  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n","Collecting markdown-it-py==3.0.0 (from -r requirements.txt (line 48))\n","  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: MarkupSafe==3.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 49)) (3.0.2)\n","Collecting matplotlib==3.10.3 (from -r requirements.txt (line 50))\n","  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: mdurl==0.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 51)) (0.1.2)\n","Requirement already satisfied: mpmath==1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 52)) (1.3.0)\n","Collecting multidict==6.6.3 (from -r requirements.txt (line 53))\n","  Downloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n","Requirement already satisfied: multiprocess==0.70.16 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 54)) (0.70.16)\n","Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 55)) (1.6.0)\n","Requirement already satisfied: networkx==3.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 56)) (3.5)\n","Collecting numpy==2.3.1 (from -r requirements.txt (line 57))\n","  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting openai==1.97.1 (from -r requirements.txt (line 58))\n","  Downloading openai-1.97.1-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: openpyxl==3.1.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 59)) (3.1.5)\n","Collecting outlines==1.1.1 (from -r requirements.txt (line 60))\n","  Downloading outlines-1.1.1-py3-none-any.whl.metadata (27 kB)\n","Collecting outlines_core==0.1.26 (from -r requirements.txt (line 61))\n","  Downloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: packaging==25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 62)) (25.0)\n","Collecting pandas==2.3.1 (from -r requirements.txt (line 63))\n","  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow==11.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 64)) (11.3.0)\n","Requirement already satisfied: pluggy==1.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 65)) (1.6.0)\n","Requirement already satisfied: ply==3.11 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 66)) (3.11)\n","Requirement already satisfied: propcache==0.3.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 67)) (0.3.2)\n","Collecting pyarrow==21.0.0 (from -r requirements.txt (line 68))\n","  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Requirement already satisfied: pydantic==2.11.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 69)) (2.11.7)\n","Requirement already satisfied: pydantic_core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 70)) (2.33.2)\n","Requirement already satisfied: Pygments==2.19.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 71)) (2.19.2)\n","Requirement already satisfied: pyparsing==3.2.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 72)) (3.2.3)\n","Requirement already satisfied: pytest==8.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 73)) (8.4.1)\n","Requirement already satisfied: python-dateutil==2.9.0.post0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 74)) (2.9.0.post0)\n","Collecting python-dotenv==1.1.1 (from -r requirements.txt (line 75))\n","  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: pytz==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 76)) (2025.2)\n","Requirement already satisfied: PyYAML==6.0.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 77)) (6.0.2)\n","Requirement already satisfied: referencing==0.36.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 78)) (0.36.2)\n","Requirement already satisfied: regex==2024.11.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 79)) (2024.11.6)\n","Collecting requests==2.32.4 (from -r requirements.txt (line 80))\n","  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n","Collecting rich==14.0.0 (from -r requirements.txt (line 81))\n","  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n","Collecting rpds-py==0.26.0 (from -r requirements.txt (line 82))\n","  Downloading rpds_py-0.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n","Collecting safetensors==0.5.3 (from -r requirements.txt (line 83))\n","  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Collecting scikit-learn==1.7.1 (from -r requirements.txt (line 84))\n","  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Collecting scipy==1.16.0 (from -r requirements.txt (line 85))\n","  Downloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentence-transformers==5.0.0 (from -r requirements.txt (line 86))\n","  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: shellingham==1.5.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 87)) (1.5.4)\n","Requirement already satisfied: six==1.17.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 88)) (1.17.0)\n","Requirement already satisfied: sniffio==1.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 89)) (1.3.1)\n","Requirement already satisfied: starlette==0.47.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 90)) (0.47.2)\n","Collecting sympy==1.14.0 (from -r requirements.txt (line 91))\n","  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: tenacity==9.1.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 92)) (9.1.2)\n","Requirement already satisfied: threadpoolctl==3.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 93)) (3.6.0)\n","Collecting tokenizers==0.21.2 (from -r requirements.txt (line 94))\n","  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting torch==2.7.1 (from -r requirements.txt (line 95))\n","  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n","Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 96)) (4.67.1)\n","Collecting transformers==4.53.3 (from -r requirements.txt (line 97))\n","  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer==0.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 98)) (0.16.0)\n","Requirement already satisfied: typing-inspection==0.4.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 99)) (0.4.1)\n","Requirement already satisfied: typing_extensions==4.14.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 100)) (4.14.1)\n","Requirement already satisfied: tzdata==2025.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 101)) (2025.2)\n","Requirement already satisfied: urllib3==2.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 102)) (2.5.0)\n","Requirement already satisfied: xxhash==3.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 103)) (3.5.0)\n","Requirement already satisfied: yarl==1.20.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 104)) (1.20.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub==0.33.4->-r requirements.txt (line 34)) (1.1.7)\n","Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.6.4.1 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.3.0.4 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.7.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.3 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting nvidia-nccl-cu12==2.26.2 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.6.77 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufile-cu12==1.11.1.6 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.3.1 (from torch==2.7.1->-r requirements.txt (line 95))\n","  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch==2.7.1->-r requirements.txt (line 95)) (75.2.0)\n","Downloading aiohttp-3.12.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading airportsdata-20250706-py3-none-any.whl (912 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading charset_normalizer-3.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.3/147.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n","Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading genson-1.3.0-py3-none-any.whl (21 kB)\n","Downloading groq-0.26.0-py3-none-any.whl (129 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.6/129.6 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading instructor-1.10.0-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.5/119.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n","Downloading iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n","Downloading jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\n","Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multidict-6.6.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.6/246.6 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.97.1-py3-none-any.whl (764 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.4/764.4 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading outlines-1.1.1-py3-none-any.whl (100 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading outlines_core-0.1.26-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n","Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rpds_py-0.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (383 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.9/383.9 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.53.3-py3-none-any.whl (10.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, genson, triton, sympy, safetensors, rpds-py, python-dotenv, pyarrow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, multidict, markdown-it-py, lark, kiwisolver, jsonpath-ng, iso3166, interegular, diskcache, colorama, charset-normalizer, certifi, anyio, airportsdata, scipy, rich, requests, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, dotenv, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, huggingface-hub, aiohttp, torch, tokenizers, openai, groq, transformers, outlines_core, instructor, sentence-transformers, outlines, evaluate, bert-score\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n","    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.1\n","    Uninstalling sympy-1.13.1:\n","      Successfully uninstalled sympy-1.13.1\n","  Attempting uninstall: safetensors\n","    Found existing installation: safetensors 0.6.2\n","    Uninstalling safetensors-0.6.2:\n","      Successfully uninstalled safetensors-0.6.2\n","  Attempting uninstall: rpds-py\n","    Found existing installation: rpds-py 0.27.0\n","    Uninstalling rpds-py-0.27.0:\n","      Successfully uninstalled rpds-py-0.27.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 18.1.0\n","    Uninstalling pyarrow-18.1.0:\n","      Successfully uninstalled pyarrow-18.1.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.127\n","    Uninstalling nvidia-nvtx-cu12-12.4.127:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: multidict\n","    Found existing installation: multidict 6.6.4\n","    Uninstalling multidict-6.6.4:\n","      Successfully uninstalled multidict-6.6.4\n","  Attempting uninstall: markdown-it-py\n","    Found existing installation: markdown-it-py 4.0.0\n","    Uninstalling markdown-it-py-4.0.0:\n","      Successfully uninstalled markdown-it-py-4.0.0\n","  Attempting uninstall: kiwisolver\n","    Found existing installation: kiwisolver 1.4.9\n","    Uninstalling kiwisolver-1.4.9:\n","      Successfully uninstalled kiwisolver-1.4.9\n","  Attempting uninstall: charset-normalizer\n","    Found existing installation: charset-normalizer 3.4.3\n","    Uninstalling charset-normalizer-3.4.3:\n","      Successfully uninstalled charset-normalizer-3.4.3\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2025.8.3\n","    Uninstalling certifi-2025.8.3:\n","      Successfully uninstalled certifi-2025.8.3\n","  Attempting uninstall: anyio\n","    Found existing installation: anyio 4.10.0\n","    Uninstalling anyio-4.10.0:\n","      Successfully uninstalled anyio-4.10.0\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.16.1\n","    Uninstalling scipy-1.16.1:\n","      Successfully uninstalled scipy-1.16.1\n","  Attempting uninstall: rich\n","    Found existing installation: rich 13.9.4\n","    Uninstalling rich-13.9.4:\n","      Successfully uninstalled rich-13.9.4\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.32.3\n","    Uninstalling requests-2.32.3:\n","      Successfully uninstalled requests-2.32.3\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: contourpy\n","    Found existing installation: contourpy 1.3.3\n","    Uninstalling contourpy-1.3.3:\n","      Successfully uninstalled contourpy-1.3.3\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.6.1\n","    Uninstalling scikit-learn-1.6.1:\n","      Successfully uninstalled scikit-learn-1.6.1\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.10.0\n","    Uninstalling matplotlib-3.10.0:\n","      Successfully uninstalled matplotlib-3.10.0\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.34.4\n","    Uninstalling huggingface-hub-0.34.4:\n","      Successfully uninstalled huggingface-hub-0.34.4\n","  Attempting uninstall: aiohttp\n","    Found existing installation: aiohttp 3.12.15\n","    Uninstalling aiohttp-3.12.15:\n","      Successfully uninstalled aiohttp-3.12.15\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.21.4\n","    Uninstalling tokenizers-0.21.4:\n","      Successfully uninstalled tokenizers-0.21.4\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.99.8\n","    Uninstalling openai-1.99.8:\n","      Successfully uninstalled openai-1.99.8\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.55.0\n","    Uninstalling transformers-4.55.0:\n","      Successfully uninstalled transformers-4.55.0\n","  Attempting uninstall: sentence-transformers\n","    Found existing installation: sentence-transformers 5.1.0\n","    Uninstalling sentence-transformers-5.1.0:\n","      Successfully uninstalled sentence-transformers-5.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n","google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n","bigframes 2.14.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n","cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n","gradio 5.42.0 requires huggingface-hub<1.0,>=0.33.5, but you have huggingface-hub 0.33.4 which is incompatible.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n","opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.1 which is incompatible.\n","sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n","dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.1 which is incompatible.\n","torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n","opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.1 which is incompatible.\n","pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aiohttp-3.12.14 airportsdata-20250706 anyio-4.9.0 bert-score-0.3.13 certifi-2025.7.14 charset-normalizer-3.4.2 colorama-0.4.6 contourpy-1.3.2 diskcache-5.6.3 dotenv-0.9.9 evaluate-0.4.5 genson-1.3.0 groq-0.26.0 huggingface-hub-0.33.4 instructor-1.10.0 interegular-0.3.3 iso3166-2.1.1 jsonpath-ng-1.7.0 kiwisolver-1.4.8 lark-1.2.2 markdown-it-py-3.0.0 matplotlib-3.10.3 multidict-6.6.3 numpy-2.3.1 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 openai-1.97.1 outlines-1.1.1 outlines_core-0.1.26 pandas-2.3.1 pyarrow-21.0.0 python-dotenv-1.1.1 requests-2.32.4 rich-14.0.0 rpds-py-0.26.0 safetensors-0.5.3 scikit-learn-1.7.1 scipy-1.16.0 sentence-transformers-5.0.0 sympy-1.14.0 tokenizers-0.21.2 torch-2.7.1 transformers-4.53.3 triton-3.3.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["certifi","kiwisolver","matplotlib","mpl_toolkits","numpy"]},"id":"1b1be473fabe482288e69e7c02b0ec3d"}},"metadata":{}}],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167739,"status":"ok","timestamp":1755182302072,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"7jbcWB1PGsoj","outputId":"3ed7a0f7-b4df-45fa-8e7c-8dbd99c14ca9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.7.1)\n","Collecting torch\n","  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Collecting torchvision\n","  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n","  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n","  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n","Collecting nvidia-nccl-cu12==2.27.3 (from torch)\n","  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n","  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==3.4.0 (from torch)\n","  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.4.0->torch) (75.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.3.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n","    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.3.1\n","    Uninstalling triton-3.3.1:\n","      Successfully uninstalled triton-3.3.1\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.6.77\n","    Uninstalling nvidia-nvtx-cu12-12.6.77:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n","    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.26.2\n","    Uninstalling nvidia-nccl-cu12-2.26.2:\n","      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufile-cu12\n","    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n","    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n","      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n","    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.7.1\n","    Uninstalling torch-2.7.1:\n","      Successfully uninstalled torch-2.7.1\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.21.0+cu124\n","    Uninstalling torchvision-0.21.0+cu124:\n","      Successfully uninstalled torchvision-0.21.0+cu124\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n","cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\n","dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.8.0 which is incompatible.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.8.0 torchvision-0.23.0 triton-3.4.0\n"]}],"source":["!pip install --upgrade torch torchvision"]},{"cell_type":"markdown","metadata":{"id":"-2wXabjgzszt"},"source":["## Import a python script from project"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":318,"status":"ok","timestamp":1755183435324,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"a8WjxROAzwUT","outputId":"e7a0a62b-74ee-432c-93eb-d80caf80edc9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/GitHub/reportingAgent\n"]}],"source":["%cd /content/drive/MyDrive/GitHub/{repo}"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5074,"status":"ok","timestamp":1755185329607,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"-oZlOWBODm4M","outputId":"464c40f5-7f2e-45ac-999a-8ab01576673a"},"outputs":[{"output_type":"stream","name":"stdout","text":["remote: Enumerating objects: 7, done.\u001b[K\n","remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n","remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n","remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n","Unpacking objects: 100% (4/4), 367 bytes | 0 bytes/s, done.\n","From https://github.com/zbotta/reportingAgent\n","   b43a357..f0ab292  dev        -> origin/dev\n","Updating b43a357..f0ab292\n","Fast-forward\n"," app/reportParamGridSearch.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"]}],"source":["!git pull"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2312,"status":"ok","timestamp":1755127732579,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"4r_Hif3sXJDN","outputId":"860f44ae-05d9-4a41-a2d6-6d8c1d3d8714"},"outputs":[{"output_type":"stream","name":"stdout","text":["08/13/2025 12:15:59 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:15:59 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:17:05 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:17:07 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:17:08 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:17:08 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:17:09 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:17:10 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:17:10 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpq5628o7m\n","08/13/2025 12:17:10 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpq5628o7m/_remote_module_non_scriptable.py\n","08/13/2025 12:17:17 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:17:17 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:17:17 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:17:20 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:17:20 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:17:21 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:17:21 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:17:21 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:18:33 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:18:35 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:18:35 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:18:35 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:18:36 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:18:37 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:18:37 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpwkv8wcnc\n","08/13/2025 12:18:37 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpwkv8wcnc/_remote_module_non_scriptable.py\n","08/13/2025 12:18:44 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:18:44 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:18:44 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:18:46 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:18:46 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:18:46 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:18:46 - huggingface_hub.file_download - INFO - Downloading 'generation_config.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/cf4ba0e4ebd236033b404876c972eac1be74b969.incomplete'\n","08/13/2025 12:18:46 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/cf4ba0e4ebd236033b404876c972eac1be74b969\n","08/13/2025 12:18:46 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:18:46 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:18:46 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:18:46 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:18:46 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:18:46 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:18:46 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:18:46 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:18:46 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:19:28 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:19:31 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:19:31 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:19:31 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:19:32 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:19:33 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:19:33 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmptn19p4br\n","08/13/2025 12:19:33 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmptn19p4br/_remote_module_non_scriptable.py\n","08/13/2025 12:19:40 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:19:40 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:19:40 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:19:41 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:19:41 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:19:41 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:19:41 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:19:41 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:19:41 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:19:41 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:19:41 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:19:41 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:19:41 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:19:41 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:19:41 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:22:11 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:22:13 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:22:13 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:22:13 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:22:14 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:22:15 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:22:15 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmp_u6jw64o\n","08/13/2025 12:22:15 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmp_u6jw64o/_remote_module_non_scriptable.py\n","08/13/2025 12:22:23 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:22:23 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:22:23 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:22:24 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:22:25 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:22:25 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:22:25 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:22:25 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:22:25 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:22:25 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:22:25 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:22:25 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:22:25 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:22:25 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/5145e0895f2fe7f1ccb3eb9da69ec74ec9c680db.incomplete'\n","08/13/2025 12:22:25 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/5145e0895f2fe7f1ccb3eb9da69ec74ec9c680db\n","08/13/2025 12:22:28 - transformers.configuration_utils - INFO - Model config PhiConfig {\n","  \"architectures\": [\n","    \"PhiForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 50256,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_size\": 2560,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 10240,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"phi\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"partial_rotary_factor\": 0.4,\n","  \"qk_layernorm\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","da48b7be36cc\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/c1148447551675ea739c440ee3e247df9f354d8f.incomplete'\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/c1148447551675ea739c440ee3e247df9f354d8f\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Downloading 'added_tokens.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7debb4784a7d53328d4d021fc46314bec4af3833.incomplete'\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7debb4784a7d53328d4d021fc46314bec4af3833\n","08/13/2025 12:22:27 - huggingface_hub.file_download - INFO - Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/0204ed10c186a4c7c68f55dff8f26087a45898d6.incomplete'\n","08/13/2025 12:22:28 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/0204ed10c186a4c7c68f55dff8f26087a45898d6\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/vocab.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/merges.txt\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/added_tokens.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/special_tokens_map.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer_config.json\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:22:28 - transformers.tokenization_utils_base - INFO - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","08/13/2025 12:22:28 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/011968cc02a5cca9fd46ecd994fc961e6906c0bd.incomplete'\n","08/13/2025 12:22:28 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/011968cc02a5cca9fd46ecd994fc961e6906c0bd\n","08/13/2025 12:22:28 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/config.json\n","08/13/2025 12:22:32 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors.index.json' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/fd9e856cfa23cd4bd5122f982feba515d23260a5.incomplete'\n","08/13/2025 12:22:32 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/fd9e856cfa23cd4bd5122f982feba515d23260a5\n","08/13/2025 12:22:32 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/model.safetensors.index.json\n","08/13/2025 12:22:33 - huggingface_hub.file_download - INFO - Downloading 'model-00002-of-00002.safetensors' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/17b98759e4b7953cbcf63ec49be7edbc9b863b57c207d84a52f5d2f5bcfcf6b4.incomplete'\n","08/13/2025 12:22:33 - huggingface_hub.file_download - INFO - Downloading 'model-00001-of-00002.safetensors' to '/root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7fbcdefa72edf7527bf5da40535b57d9f5bd3d16829b94a9d25d2b457df62e84.incomplete'\n","08/13/2025 12:23:22 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/17b98759e4b7953cbcf63ec49be7edbc9b863b57c207d84a52f5d2f5bcfcf6b4\n","08/13/2025 12:24:20 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--microsoft--phi-2/blobs/7fbcdefa72edf7527bf5da40535b57d9f5bd3d16829b94a9d25d2b457df62e84\n","08/13/2025 12:24:20 - transformers.modeling_utils - INFO - Instantiating PhiForCausalLM model under default dtype torch.float32.\n","08/13/2025 12:24:20 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:24:44 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing PhiForCausalLM.\n","\n","08/13/2025 12:24:44 - transformers.modeling_utils - INFO - All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n","08/13/2025 12:24:45 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:24:45 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:24:46 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:24:46 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:24:46 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:24:47 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:24:47 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:24:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:24:55 - evaluate.utils.file_utils - INFO - https://huggingface.co/spaces/evaluate-metric/bertscore/resolve/main/bertscore.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/evaluate/downloads/tmp_hwmv5h2\n","08/13/2025 12:24:55 - evaluate.utils.file_utils - INFO - storing https://huggingface.co/spaces/evaluate-metric/bertscore/resolve/main/bertscore.py in cache at /root/.cache/huggingface/evaluate/downloads/2fb1405ec250844307c59b3bad240bb98cf25006711d08add1afcaf365fa5899.a4fc40aee04c356ddb89f55c1f36393b16fa831e9b5a9daa80e5e77cac867d1c.py\n","08/13/2025 12:24:55 - evaluate.utils.file_utils - INFO - creating metadata file for /root/.cache/huggingface/evaluate/downloads/2fb1405ec250844307c59b3bad240bb98cf25006711d08add1afcaf365fa5899.a4fc40aee04c356ddb89f55c1f36393b16fa831e9b5a9daa80e5e77cac867d1c.py\n","08/13/2025 12:24:55 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/e5c73d8a50df1f56fb5b0b8002d7cf4010afdccb.incomplete'\n","08/13/2025 12:24:55 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/e5c73d8a50df1f56fb5b0b8002d7cf4010afdccb\n","08/13/2025 12:24:56 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/150367d8744161cd17b3f6462a14f3a9648752da.incomplete'\n","08/13/2025 12:24:56 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/150367d8744161cd17b3f6462a14f3a9648752da\n","08/13/2025 12:24:56 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:24:56 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:24:56 - huggingface_hub.file_download - INFO - Downloading 'vocab.txt' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.incomplete'\n","08/13/2025 12:24:57 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938\n","08/13/2025 12:24:57 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/949a6f013d67eb8a5b4b5b46026217b888021b88.incomplete'\n","08/13/2025 12:24:57 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/949a6f013d67eb8a5b4b5b46026217b888021b88\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:24:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:24:57 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:24:57 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:24:58 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:24:58 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:24:59 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/5e3f1108e3cb34ee048634875d8482665b65ac713291a7e32396fb18f6ff0063.incomplete'\n","08/13/2025 12:25:06 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--distilbert-base-uncased/blobs/5e3f1108e3cb34ee048634875d8482665b65ac713291a7e32396fb18f6ff0063\n","08/13/2025 12:25:06 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:06 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:06 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:06 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:06 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './modules.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/952a9b81c0bfd99800fabf352f69c7ccd46c5e43.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/952a9b81c0bfd99800fabf352f69c7ccd46c5e43\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './config_sentence_transformers.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fd1b291129c607e5d49799f87cb219b27f98acdf.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fd1b291129c607e5d49799f87cb219b27f98acdf\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './README.md' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/58d4a9a45664eb9e12de9549c548c09b6134c17f.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/58d4a9a45664eb9e12de9549c548c09b6134c17f\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading './sentence_bert_config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/59d594003bf59880a884c574bf88ef7555bb0202.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/59d594003bf59880a884c574bf88ef7555bb0202\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/72b987fd805cfa2b58c4c8c952b274a11bfd5a00.incomplete'\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/72b987fd805cfa2b58c4c8c952b274a11bfd5a00\n","08/13/2025 12:25:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:07 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/53aa51172d142c89d9012cce15ae4d6cc0ca6895895114379cacb4fab128d9db.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/53aa51172d142c89d9012cce15ae4d6cc0ca6895895114379cacb4fab128d9db\n","08/13/2025 12:25:09 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:09 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:09 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/c79f2b6a0cea6f4b564fed1938984bace9d30ff0.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/c79f2b6a0cea6f4b564fed1938984bace9d30ff0\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'vocab.txt' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/cb202bfe2e3c98645018a6d12f182a434c9d3e02.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/cb202bfe2e3c98645018a6d12f182a434c9d3e02\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5.incomplete'\n","08/13/2025 12:25:09 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/e7b0375001f109a6b8873d756ad4f7bbb15fbaa5\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:10 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Downloading '1_Pooling/config.json' to '/root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/d1514c3162bbe87b343f565fadc62e6c06f04f03.incomplete'\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/blobs/d1514c3162bbe87b343f565fadc62e6c06f04f03\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Downloading 'config.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/88bc4f74b33a2073abc9a66cb532b889448ac3ed.incomplete'\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/88bc4f74b33a2073abc9a66cb532b889448ac3ed\n","08/13/2025 12:25:10 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:10 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:10 - huggingface_hub.file_download - INFO - Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/821d1aa69520101d6e0737f78a042ae25b19e5cb9160701909d10434f4aeb0ae.incomplete'\n","08/13/2025 12:25:12 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/821d1aa69520101d6e0737f78a042ae25b19e5cb9160701909d10434f4aeb0ae\n","08/13/2025 12:25:12 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:12 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:12 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/a2435fedfac32b9ad70f052d4f84007730cd3109.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/a2435fedfac32b9ad70f052d4f84007730cd3109\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'vocab.txt' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/fb140275c155a9c7c5a3b3e0e77a9e839594a938\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/688882a79f44442ddc1f60d70334a7ff5df0fb47.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/688882a79f44442ddc1f60d70334a7ff5df0fb47\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/7520992f25914d962f0e2fd0e0566fc33d19ec59.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/7520992f25914d962f0e2fd0e0566fc33d19ec59\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:13 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Downloading './README.md' to '/root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/4783b64ce66b94c7c387672de541d10678980574.incomplete'\n","08/13/2025 12:25:13 - huggingface_hub.file_download - INFO - Download complete. Moving file to /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/blobs/4783b64ce66b94c7c387672de541d10678980574\n","08/13/2025 12:25:13 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:14 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:15 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:15 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:22 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:22 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:23 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:23 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:23 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:24 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:24 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:24 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:24 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:25 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:25 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:25 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:26 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:26 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:26 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:26 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:26 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:34 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:34 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:34 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:34 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:34 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:34 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:35 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:35 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:35 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:35 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:35 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:35 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:36 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:36 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:36 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:36 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:36 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:36 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:36 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:36 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:36 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:44 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:45 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:45 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:45 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:45 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:45 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:45 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:45 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:46 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:46 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:47 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:47 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:25:47 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:47 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:25:55 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:25:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:55 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:25:55 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:25:55 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:25:55 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:25:55 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:25:56 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:25:56 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:56 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:25:56 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:25:56 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:57 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:25:57 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:25:57 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:25:57 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:25:57 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:25:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:25:57 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:25:57 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:25:58 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:25:58 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:25:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:26:05 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:06 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:06 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:06 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:26:06 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:26:06 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:26:06 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:26:06 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:26:07 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:26:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:26:07 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:26:08 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:08 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:26:08 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:26:08 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:26:08 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:26:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:26:15 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:15 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:26:16 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:26:16 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:26:16 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:26:16 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:26:17 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:26:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:26:17 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:26:18 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:18 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:26:18 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:26:18 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:26:18 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:26:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:26:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:25 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:26:26 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:26:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:26 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:26:26 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:26:26 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:26:26 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:26:26 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:26:27 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:26:27 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:26:27 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:26:28 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:26:28 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:26:28 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:26:28 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:26:28 - mods.dataHandler - WARNING - Folder does not exist, creating new folder in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench\n","08/13/2025 12:26:28 - mods.dataHandler - INFO - Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench/tb-experiment-13-082025 12-26-28.xlsx\n","08/13/2025 12:26:28 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:26:28 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:39:31 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:39:33 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:39:34 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:39:34 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:39:34 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:39:35 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:39:35 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmp59ngi2rd\n","08/13/2025 12:39:35 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmp59ngi2rd/_remote_module_non_scriptable.py\n","08/13/2025 12:39:43 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:39:43 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:39:43 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:39:44 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 12:39:44 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 12:39:44 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 12:39:45 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:39:45 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:39:45 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:39:45 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:39:45 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 12:39:45 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 12:39:45 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/vocab.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/merges.txt\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/added_tokens.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/special_tokens_map.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer_config.json\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:39:45 - transformers.tokenization_utils_base - INFO - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","08/13/2025 12:39:45 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/config.json\n","08/13/2025 12:39:45 - transformers.configuration_utils - INFO - Model config PhiConfig {\n","  \"architectures\": [\n","    \"PhiForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 50256,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_size\": 2560,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 10240,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"phi\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"partial_rotary_factor\": 0.4,\n","  \"qk_layernorm\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","08/13/2025 12:39:47 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/model.safetensors.index.json\n","08/13/2025 12:39:47 - transformers.modeling_utils - INFO - Instantiating PhiForCausalLM model under default dtype torch.float32.\n","08/13/2025 12:39:47 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:11 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing PhiForCausalLM.\n","\n","08/13/2025 12:40:11 - transformers.modeling_utils - INFO - All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:12 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:12 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:21 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:21 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:22 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:22 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:23 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:23 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:23 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:23 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:23 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:24 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:24 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:24 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:24 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:24 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:24 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:25 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:25 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:25 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:25 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:25 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:25 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:25 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:25 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:25 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:25 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:34 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:34 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:34 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:34 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:34 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:34 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:34 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:35 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:35 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:35 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:35 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:35 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:35 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:36 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:36 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:36 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:36 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:36 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:36 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:36 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:36 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:36 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:44 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:44 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:44 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:45 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:45 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:45 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:45 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:46 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:46 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:46 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:47 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:47 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:47 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:47 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:47 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:47 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:47 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:40:55 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:40:55 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:55 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:40:55 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:40:55 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:40:55 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:40:55 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:40:56 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:40:56 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:56 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:40:56 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:40:56 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:57 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:40:57 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:40:57 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:40:57 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:40:57 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:40:57 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:40:57 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 12:40:58 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:40:58 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:40:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:05 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:05 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:05 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:05 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:05 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:05 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:05 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:06 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:06 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:06 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:06 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:06 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:07 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:07 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:07 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:07 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:07 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:07 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:07 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:07 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:41:08 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:41:08 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:41:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:15 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:15 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:15 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:16 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:16 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:16 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:16 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:17 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:17 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:17 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:18 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:18 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:18 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:41:18 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:41:18 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:41:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:26 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:26 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:26 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:26 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:26 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:26 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:26 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:27 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:27 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:27 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:27 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:27 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:27 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:28 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:28 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:28 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:28 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:28 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:28 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:28 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:28 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1, 'repetition_penalty': 1.0, 'do_sample': False, 'max_new_tokens': 300}\n","08/13/2025 12:41:28 - transformers.generation.configuration_utils - WARNING - The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","08/13/2025 12:41:28 - transformers.generation.configuration_utils - INFO - - `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `1.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n","08/13/2025 12:41:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/vocab.txt\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at None\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer_config.json\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/tokenizer.json\n","08/13/2025 12:41:36 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/config.json\n","08/13/2025 12:41:36 - transformers.configuration_utils - INFO - Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.53.3\",\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:36 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/12040accade4e8a0f71eabdb258fecc2e7e948be/model.safetensors\n","08/13/2025 12:41:36 - transformers.modeling_utils - INFO - All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n","08/13/2025 12:41:36 - evaluate.module - INFO - Removing /root/.cache/huggingface/metrics/bert_score/default/default_experiment-1-0.arrow\n","08/13/2025 12:41:36 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cuda:0\n","08/13/2025 12:41:36 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n","08/13/2025 12:41:37 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json\n","08/13/2025 12:41:37 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:37 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/model.safetensors\n","08/13/2025 12:41:37 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertModel.\n","\n","08/13/2025 12:41:37 - transformers.modeling_utils - INFO - All the weights of BertModel were initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/vocab.txt\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/special_tokens_map.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:38 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/config.json\n","08/13/2025 12:41:38 - transformers.configuration_utils - INFO - Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 384,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1536,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"sbert_ce_default_activation_function\": \"torch.nn.modules.linear.Identity\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","08/13/2025 12:41:38 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/model.safetensors\n","08/13/2025 12:41:38 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing BertForSequenceClassification.\n","\n","08/13/2025 12:41:38 - transformers.modeling_utils - INFO - All the weights of BertForSequenceClassification were initialized from the model checkpoint at cross-encoder/ms-marco-MiniLM-L6-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/vocab.txt\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file added_tokens.json from cache at None\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/special_tokens_map.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--cross-encoder--ms-marco-MiniLM-L6-v2/snapshots/ce0834f22110de6d9222af7a7a03628121708969/tokenizer_config.json\n","08/13/2025 12:41:38 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 12:41:38 - sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cuda:0\n","08/13/2025 12:41:39 - mods.dataHandler - INFO - Saving df to excel in: /content/drive/MyDrive/GitHub/reportingAgent/app/results/test-bench/tb-experiment-13-082025 12-41-39.xlsx\n","08/13/2025 12:41:39 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:41:39 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 12:44:13 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 12:44:15 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 12:44:16 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:44:16 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:44:16 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 12:44:18 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 12:44:18 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmp92hg4lkh\n","08/13/2025 12:44:18 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmp92hg4lkh/_remote_module_non_scriptable.py\n","08/13/2025 12:44:26 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 12:44:26 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 12:44:26 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 12:44:27 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 12:44:27 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 13:26:51 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 13:26:53 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 13:26:53 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:26:53 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:26:54 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 13:26:55 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 13:26:55 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpehk321xo\n","08/13/2025 13:26:55 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpehk321xo/_remote_module_non_scriptable.py\n","08/13/2025 13:27:02 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 13:27:02 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:27:02 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:27:03 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 13:27:03 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n","08/13/2025 13:28:55 - __main__ - INFO - Added ENV = /content/drive/MyDrive/GitHub/reportingAgent\n","08/13/2025 13:28:57 - numexpr.utils - INFO - NumExpr defaulting to 2 threads.\n","08/13/2025 13:28:57 - datasets - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:28:57 - datasets - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:28:58 - transformers.utils.import_utils - INFO - JAX version 0.5.3, Flax version 0.10.6 available.\n","08/13/2025 13:29:00 - torch._inductor.config - INFO - compile_threads set to 2\n","08/13/2025 13:29:00 - torch.distributed.nn.jit.instantiator - INFO - Created a temporary directory at /tmp/tmpc6veeyyt\n","08/13/2025 13:29:00 - torch.distributed.nn.jit.instantiator - INFO - Writing /tmp/tmpc6veeyyt/_remote_module_non_scriptable.py\n","08/13/2025 13:29:06 - evaluate.config - INFO - PyTorch version 2.8.0 available.\n","08/13/2025 13:29:06 - evaluate.config - INFO - TensorFlow version 2.19.0 available.\n","08/13/2025 13:29:06 - evaluate.config - INFO - JAX version 0.5.3 available.\n","08/13/2025 13:29:07 - projectSetup - INFO - Loading device and environment variables:\n","               device=cuda, torch_dtype=torch.float32\n","08/13/2025 13:29:07 - projectSetup - INFO - Loading environment variables from: /content/drive/MyDrive/GitHub/reportingAgent/.env\n","08/13/2025 13:29:08 - mods.dataHandler - INFO - Dataset loaded from path : /content/drive/MyDrive/GitHub/reportingAgent/app/datasets/Reports_dataset.xlsx\n","08/13/2025 13:29:08 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 13:29:08 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:08 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 13:29:08 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 13:29:08 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=microsoft/phi-2\n","08/13/2025 13:29:08 - mods.modelLoader - INFO - The default parameters of the model are:\n"," {'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': False}\n","08/13/2025 13:29:08 - mods.testBench - INFO - Test Bench loaded\n","08/13/2025 13:29:08 - transformers.configuration_utils - INFO - Model config PhiConfig {\n","  \"architectures\": [\n","    \"PhiForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.0,\n","  \"eos_token_id\": 50256,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_size\": 2560,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 10240,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 2048,\n","  \"model_type\": \"phi\",\n","  \"num_attention_heads\": 32,\n","  \"num_hidden_layers\": 32,\n","  \"num_key_value_heads\": 32,\n","  \"partial_rotary_factor\": 0.4,\n","  \"qk_layernorm\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 10000.0,\n","  \"tie_word_embeddings\": false,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.53.3\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","t /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/added_tokens.json\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/special_tokens_map.json\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/tokenizer_config.json\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - loading file chat_template.jinja from cache at None\n","08/13/2025 13:29:08 - transformers.tokenization_utils_base - INFO - Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","08/13/2025 13:29:08 - transformers.configuration_utils - INFO - loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/config.json\n","08/13/2025 13:29:10 - transformers.modeling_utils - INFO - loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/model.safetensors.index.json\n","08/13/2025 13:29:10 - transformers.modeling_utils - INFO - Instantiating PhiForCausalLM model under default dtype torch.float32.\n","08/13/2025 13:29:10 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:40 - transformers.modeling_utils - INFO - All model checkpoint weights were used when initializing PhiForCausalLM.\n","\n","08/13/2025 13:29:40 - transformers.modeling_utils - INFO - All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23/generation_config.json\n","08/13/2025 13:29:40 - transformers.generation.configuration_utils - INFO - Generate config GenerationConfig {\n","  \"bos_token_id\": 50256,\n","  \"eos_token_id\": 50256\n","}\n","\n","08/13/2025 13:29:40 - mods.testBench - INFO - Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 0.5, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/13/2025 13:29:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/13/2025 13:29:40 - torch._dynamo.eval_frame - INFO - TorchDynamo attempted to trace the following frames: [\n","\n","]\n","08/13/2025 13:29:40 - torch._dynamo.utils - INFO - TorchDynamo compilation metrics:\n","Function    Runtimes (s)\n","----------  --------------\n"]}],"source":["!cat app/logs/logfile.log"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8008,"status":"ok","timestamp":1755168831969,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"i79nllcxdMwy"},"outputs":[],"source":["!python projectSetup.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205,"referenced_widgets":["0b15381868984f48a958540b50f785c6","6f06d02f23654471b0c1377b8d7f3203","73ddf9e17e6a4cfe85cb47a2625303e8","b5157bdd857148fb9d5c6488691a81b8","c260a953e8564a31bd2d64fbb3445f84","9e33742605ca4f12863776e771c9dc77","e54018a2d58a4c1e95410f5c9bc419e1","6c7b05ef240b4737948d1a48b423624e","3a0a7902dddf42028ff6c54a3c2af8d2","b91e110c59a2464284d4d00d3204eca4","aa7496c29bef49fc8a29d3102800ce4f"]},"executionInfo":{"elapsed":3162,"status":"ok","timestamp":1755087390807,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"},"user_tz":-120},"id":"Srd-wh7zg8Qt","outputId":"f4af760b-37a5-4868-e580-65d4c763e782"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b15381868984f48a958540b50f785c6","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=gpt2\n","08/13/2025 12:16:31 - app.mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=gpt2\n"]}],"source":["import sys, os\n","import torch\n","from pathlib import Path\n","sys.path.append(os.getcwd())\n","sys.path.append(os.getcwd() + '/app')\n","\n","from app.mods.promptGenerator import PromptGenerator\n","from app.mods.modelLoader import ModelLoader\n","from app.mods.reportGenerator import ReportGenerator\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","torch_dtype = torch.float32 if torch.cuda.is_available() else torch.float32\n","\n","ml = ModelLoader(model_id=\"gpt2\", device=device, torch_dtype=torch_dtype)"]},{"cell_type":"markdown","source":["## microsoft/phi-2\n","It allocates 12 GB in RAM"],"metadata":{"id":"rfGNcNvTUhdj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"P57w5lE7nDyX"},"outputs":[],"source":["!python app/reportParamGridSearch.py --model_id microsoft/phi-2 --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True"]},{"cell_type":"markdown","source":["## HuggingFaceTB/SmolLM3-3B\n","It allocates 14 GB in RAM"],"metadata":{"id":"vVTm54F1UaHq"}},{"cell_type":"code","source":["!python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM3-3B --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DdF2u_DS5WaO","executionInfo":{"status":"ok","timestamp":1755186374260,"user_tz":-120,"elapsed":1039688,"user":{"displayName":"Mati Bottarini","userId":"12309550559523072958"}},"outputId":"1e0cc896-63ca-4d93-e485-4d1356904f37"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-08-14 15:29:00.367493: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755185340.390151   17735 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755185340.397858   17735 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755185340.416121   17735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755185340.416160   17735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755185340.416163   17735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755185340.416166   17735 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-08-14 15:29:00.420745: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Parameters passed to main script: \n","{'model_id': ['HuggingFaceTB/SmolLM3-3B'], 'start_idx': [20], 'end_idx': [22], 'temperature': [0.3, 0.7, 1.3, 2.0], 'top_p': [0.2, 0.5, 0.8, 1.0], 'top_k': [10, 30, 50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","generation_config.json: 100% 182/182 [00:00<00:00, 1.71MB/s]\n","08/14/2025 15:29:20 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","08/14/2025 15:29:20 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","08/14/2025 15:29:20 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=HuggingFaceTB/SmolLM3-3B\n","Generation parameters: \n","{'temperature': [0.3, 0.7, 1.3, 2.0], 'top_p': [0.2, 0.5, 0.8, 1.0], 'top_k': [10, 30, 50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","tokenizer_config.json: 50.4kB [00:00, 43.7MB/s]\n","tokenizer.json: 100% 17.2M/17.2M [00:03<00:00, 4.37MB/s]\n","special_tokens_map.json: 100% 289/289 [00:00<00:00, 2.32MB/s]\n","chat_template.jinja: 5.60kB [00:00, 2.80MB/s]\n","config.json: 1.92kB [00:00, 5.21MB/s]\n","model.safetensors.index.json: 26.9kB [00:00, 640kB/s]\n","Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n","model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n","\n","model-00002-of-00002.safetensors:   0% 0.00/1.18G [00:00<?, ?B/s]\u001b[A\u001b[A\n","\n","model-00002-of-00002.safetensors:   0% 42.9k/1.18G [00:02<15:43:41, 20.9kB/s]\u001b[A\u001b[A\n","model-00001-of-00002.safetensors:   0% 776k/4.97G [00:02<4:01:04, 343kB/s]\u001b[A\n","\n","model-00002-of-00002.safetensors:   0% 595k/1.18G [00:02<53:37, 368kB/s]     \u001b[A\u001b[A\n","\n","model-00002-of-00002.safetensors:   1% 8.98M/1.18G [00:02<02:43, 7.20MB/s]\u001b[A\u001b[A\n","\n","model-00002-of-00002.safetensors:   3% 31.0M/1.18G [00:03<01:22, 13.9MB/s]\u001b[A\u001b[A\n","model-00001-of-00002.safetensors:   0% 2.13M/4.97G [00:04<2:34:52, 534kB/s]\u001b[A\n","model-00001-of-00002.safetensors:   0% 10.4M/4.97G [00:04<24:07, 3.42MB/s] \u001b[A\n","\n","model-00002-of-00002.safetensors:   3% 38.8M/1.18G [00:04<01:40, 11.4MB/s]\u001b[A\u001b[A\n","\n","model-00002-of-00002.safetensors:   5% 55.6M/1.18G [00:05<01:11, 15.8MB/s]\u001b[A\u001b[A\n","\n","model-00002-of-00002.safetensors:   7% 83.2M/1.18G [00:05<00:42, 25.9MB/s]\u001b[A\u001b[A\n","\n","model-00002-of-00002.safetensors:  14% 161M/1.18G [00:05<00:13, 74.8MB/s] \u001b[A\u001b[A\n","model-00001-of-00002.safetensors:   2% 76.7M/4.97G [00:06<03:59, 20.4MB/s]\u001b[A\n","\n","model-00002-of-00002.safetensors:  17% 206M/1.18G [00:06<00:15, 61.6MB/s]\u001b[A\u001b[A\n","\n","model-00002-of-00002.safetensors:  26% 312M/1.18G [00:07<00:09, 90.8MB/s]\u001b[A\u001b[A\n","model-00001-of-00002.safetensors:   3% 135M/4.97G [00:07<02:51, 28.1MB/s] \u001b[A\n","\n","model-00002-of-00002.safetensors:  32% 379M/1.18G [00:07<00:08, 97.3MB/s]\u001b[A\u001b[A\n","model-00001-of-00002.safetensors:   4% 202M/4.97G [00:08<01:46, 44.8MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   5% 270M/4.97G [00:09<01:37, 48.1MB/s]\u001b[A\n","\n","model-00002-of-00002.safetensors:  38% 446M/1.18G [00:09<00:11, 66.8MB/s]\u001b[A\u001b[A\n","model-00001-of-00002.safetensors:   7% 337M/4.97G [00:09<01:14, 62.4MB/s]\u001b[A\n","\n","model-00002-of-00002.safetensors:  43% 513M/1.18G [00:09<00:07, 84.5MB/s]\u001b[A\u001b[A\n","model-00001-of-00002.safetensors:   8% 404M/4.97G [00:10<00:56, 81.1MB/s]\u001b[A\n","model-00001-of-00002.safetensors:   9% 471M/4.97G [00:12<01:27, 51.3MB/s]\u001b[A\n","\n","model-00002-of-00002.safetensors:  49% 580M/1.18G [00:12<00:11, 51.2MB/s]\u001b[A\u001b[A\n","model-00001-of-00002.safetensors:  14% 672M/4.97G [00:12<00:36, 118MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  16% 806M/4.97G [00:12<00:25, 164MB/s]\u001b[A\n","\n","model-00002-of-00002.safetensors:  60% 714M/1.18G [00:13<00:07, 62.7MB/s]\u001b[A\u001b[A\n","model-00001-of-00002.safetensors:  18% 873M/4.97G [00:14<00:35, 116MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  19% 940M/4.97G [00:14<00:30, 132MB/s]\u001b[A\n","\n","model-00002-of-00002.safetensors:  66% 782M/1.18G [00:15<00:06, 62.6MB/s]\u001b[A\u001b[A\n","model-00001-of-00002.safetensors:  20% 1.01G/4.97G [00:16<00:54, 72.9MB/s]\u001b[A\n","\n","model-00002-of-00002.safetensors:  72% 849M/1.18G [00:16<00:05, 57.8MB/s]\u001b[A\u001b[A\n","\n","model-00002-of-00002.safetensors:  83% 983M/1.18G [00:16<00:02, 97.4MB/s]\u001b[A\u001b[A\n","model-00001-of-00002.safetensors:  24% 1.21G/4.97G [00:20<01:05, 57.4MB/s]\u001b[A\n","\n","model-00002-of-00002.safetensors:  94% 1.12G/1.18G [00:20<00:01, 54.3MB/s]\u001b[A\u001b[A\n","model-00001-of-00002.safetensors:  26% 1.28G/4.97G [00:21<00:56, 65.6MB/s]\u001b[A\n","\n","model-00002-of-00002.safetensors: 100% 1.18G/1.18G [00:21<00:00, 55.6MB/s]\n","\n","model-00001-of-00002.safetensors:  27% 1.34G/4.97G [00:21<00:47, 76.2MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  28% 1.41G/4.97G [00:21<00:37, 93.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  30% 1.48G/4.97G [00:21<00:29, 117MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  31% 1.55G/4.97G [00:22<00:24, 140MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  32% 1.61G/4.97G [00:22<00:19, 175MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  33% 1.65G/4.97G [00:22<00:19, 168MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  35% 1.72G/4.97G [00:23<00:32, 99.1MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  36% 1.76G/4.97G [00:24<00:31, 102MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  37% 1.83G/4.97G [00:24<00:22, 139MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  38% 1.90G/4.97G [00:24<00:20, 152MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  39% 1.95G/4.97G [00:24<00:17, 168MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  41% 2.01G/4.97G [00:25<00:15, 192MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  42% 2.08G/4.97G [00:25<00:16, 171MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  44% 2.17G/4.97G [00:28<00:39, 70.7MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  47% 2.32G/4.97G [00:28<00:20, 131MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  50% 2.46G/4.97G [00:28<00:12, 197MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  52% 2.59G/4.97G [00:28<00:09, 260MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  54% 2.67G/4.97G [00:29<00:12, 180MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  55% 2.73G/4.97G [00:29<00:11, 192MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  56% 2.80G/4.97G [00:30<00:11, 184MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  58% 2.87G/4.97G [00:30<00:10, 195MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  59% 2.93G/4.97G [00:30<00:11, 176MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  60% 3.00G/4.97G [00:31<00:10, 185MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  62% 3.07G/4.97G [00:31<00:09, 190MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  63% 3.14G/4.97G [00:31<00:09, 188MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  64% 3.20G/4.97G [00:32<00:09, 191MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  66% 3.27G/4.97G [00:32<00:08, 199MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  67% 3.33G/4.97G [00:32<00:08, 187MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  68% 3.40G/4.97G [00:33<00:09, 166MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  70% 3.45G/4.97G [00:33<00:09, 159MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  71% 3.52G/4.97G [00:34<00:08, 166MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  72% 3.59G/4.97G [00:34<00:07, 185MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  74% 3.65G/4.97G [00:34<00:06, 205MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  75% 3.72G/4.97G [00:34<00:05, 213MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  78% 3.85G/4.97G [00:35<00:04, 272MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  79% 3.92G/4.97G [00:35<00:03, 283MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  80% 3.99G/4.97G [00:35<00:03, 263MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  82% 4.06G/4.97G [00:36<00:04, 205MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  82% 4.09G/4.97G [00:36<00:04, 178MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  84% 4.16G/4.97G [00:39<00:12, 65.9MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  88% 4.36G/4.97G [00:39<00:04, 145MB/s] \u001b[A\n","model-00001-of-00002.safetensors:  92% 4.56G/4.97G [00:39<00:01, 239MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  95% 4.70G/4.97G [00:39<00:00, 295MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  97% 4.83G/4.97G [00:39<00:00, 349MB/s]\u001b[A\n","model-00001-of-00002.safetensors:  99% 4.90G/4.97G [00:40<00:00, 212MB/s]\u001b[A\n","model-00001-of-00002.safetensors: 100% 4.97G/4.97G [00:41<00:00, 121MB/s]\n","Fetching 2 files: 100% 2/2 [00:41<00:00, 20.99s/it]\n","Loading checkpoint shards: 100% 2/2 [00:01<00:00,  1.04it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 26.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 41.18it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.13it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.26it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.07it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.10it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.18it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.73it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.45it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.57it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.32it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.84it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.01it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.09it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.08it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.07it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.06it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 112.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.07it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 110.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 114.53it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.70it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.79it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.01it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.90it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.54it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.22it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.34it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.97it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.20it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.67it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.41it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.17it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 107.01it/s]\n","Batches: 100% 1/1 [00:00<00:00, 116.55it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.22it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.79it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 122.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.21it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.85it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.83it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 126.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.81it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.27it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 106.02it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.63it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.54it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.50it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.70it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.68it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.68it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.78it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.32it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 124.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.30it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 114.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.71it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.69it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.84it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.82it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.68it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.28it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.09it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.10it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.85it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.85it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 139.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.55it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 137.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.81it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.45it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.13it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.70it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.89it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.43it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.77it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.21it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.50it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.18it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.36it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.66it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 118.31it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.92it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.06it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.79it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 117.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.49it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.81it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.30it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.09it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.76it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.98it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.34it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.37it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.15it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.20it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 132.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.41it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.36it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.64it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.90it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.70it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.19it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.83it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.68it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.21it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 122.33it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.06it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.42it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 133.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.57it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.23it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.61it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.01it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 123.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.91it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.77it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.02it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.07it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.31it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 136.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.42it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 135.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.09it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.69it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.45it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.91it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.75it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.11it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.04it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.01it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 110.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.74it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.51it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.82it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.96it/s]\n","Batches: 100% 1/1 [00:00<00:00, 104.76it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.07it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 126.27it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.92it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 88.41it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.28it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.88it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.80it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 124.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.10it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.42it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 116.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.00it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 141.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.46it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.64it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.90it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 131.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.93it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 107.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.99it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.70it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.59it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 125.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.02it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.14it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.82it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.31it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.23it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.68it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 122.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.69it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 124.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.39it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 128.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.39it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.69it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 120.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.98it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 123.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.16it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 127.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.34it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 121.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.27it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 133.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.03it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 131.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.71it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.46it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 115.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 110.50it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Batches: 100% 1/1 [00:00<00:00, 133.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.79it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 130.48it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.44it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 134.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.32it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 138.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.09it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 119.94it/s]\n","Batches: 100% 1/1 [00:00<00:00, 120.88it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","Batches: 100% 1/1 [00:00<00:00, 129.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.51it/s]\n"]}]},{"cell_type":"markdown","source":["## GPT2-XL\n","It allocates 7.2 GB in RAM"],"metadata":{"id":"GBWVQWjZUUrm"}},{"cell_type":"code","source":["!python app/reportParamGridSearch.py --model_id openai-community/gpt2-xl --start_idx 20 --end_idx 22 --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XmhpRs4v6SBP","outputId":"a85866a1-5c36-4925-9397-cb2ea45a1374"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2025-08-14 15:46:19.634367: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1755186379.655652   22391 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1755186379.662657   22391 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1755186379.680656   22391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755186379.680682   22391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755186379.680685   22391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1755186379.680688   22391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-08-14 15:46:19.685371: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Parameters passed to main script: \n","{'model_id': ['openai-community/gpt2-xl'], 'start_idx': [20], 'end_idx': [22], 'temperature': [0.3, 0.7, 1.3, 2.0], 'top_p': [0.2, 0.5, 0.8, 1.0], 'top_k': [10, 30, 50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","generation_config.json: 100% 124/124 [00:00<00:00, 1.09MB/s]\n","08/14/2025 15:46:38 - mods.modelLoader - WARNING - No attribute frequency_penalty found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","08/14/2025 15:46:38 - mods.modelLoader - WARNING - No attribute presence_penalty found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","08/14/2025 15:46:38 - mods.modelLoader - WARNING - No attribute stop found in GenerationConfig, for model_id=openai-community/gpt2-xl\n","Generation parameters: \n","{'temperature': [0.3, 0.7, 1.3, 2.0], 'top_p': [0.2, 0.5, 0.8, 1.0], 'top_k': [10, 30, 50], 'max_new_tokens': [300.0], 'do_sample': [True]}\n","tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 211kB/s]\n","vocab.json: 1.04MB [00:00, 2.62MB/s]\n","merges.txt: 456kB [00:00, 5.54MB/s]\n","tokenizer.json: 1.36MB [00:00, 6.26MB/s]\n","config.json: 100% 689/689 [00:00<00:00, 5.31MB/s]\n","model.safetensors: 100% 6.43G/6.43G [00:48<00:00, 133MB/s] \n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:47:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 82.34it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.66it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:47:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.88it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:48:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.44it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:48:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.34it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:48:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.17it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:48:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.56it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:48:34 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.89it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.04it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:48:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 15:49:00 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1223 [type=json_invalid, input_value='{ \"title\": \"Wrong torque... Rivera (Line Operator)', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 15:49:00 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 134.70it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.02it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:49:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 15:49:18 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1288 [type=json_invalid, input_value='{ \"title\": \"Wrong torque...ne was reinspected. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 15:49:19 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 105.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.60it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:49:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.80it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:49:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 15:49:46 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1298 [type=json_invalid, input_value='{ \"title\": \"Wrong torque...d) and Naomi Ellis from', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 15:49:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 109.83it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.90it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:49:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.40it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:49:55 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.44it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.17it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:50:04 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.51it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:50:12 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.62it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:50:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.48it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:50:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.12it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:50:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.35it/s]\n","Batches: 100% 1/1 [00:00<00:00, 127.78it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:50:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.13it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.27it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:50:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.74it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.65it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:50:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 15:51:14 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1212 [type=json_invalid, input_value='{ \"title\": \"Wrong torque...d for clarity. The line', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 15:51:14 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 136.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.64it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:51:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.61it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.53it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:51:20 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 103.14it/s]\n","Batches: 100% 1/1 [00:00<00:00, 123.16it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:51:28 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.40it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:51:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.55it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:51:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.36it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.59it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:51:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.39it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.83it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:52:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 117.16it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.99it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:52:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.50it/s]\n","Batches: 100% 1/1 [00:00<00:00, 125.25it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:52:18 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.88it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:52:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.65it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:52:32 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.54it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.82it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:52:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 129.83it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:52:51 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.79it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.73it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:52:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.28it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.94it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:53:09 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.71it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.61it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:53:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.72it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.22it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:53:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.72it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:53:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.63it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.23it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:53:40 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.67it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:53:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.78it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:53:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 94.80it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.95it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:53:54 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.58it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:53:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.64it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:54:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.30it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.37it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:54:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.22it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:54:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.41it/s]\n","Ref_row:20: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:54:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.01it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:54:25 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.58it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.60it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:54:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.91it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.37it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:54:37 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.33it/s]\n","Batches: 100% 1/1 [00:00<00:00, 128.83it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:54:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.69it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:54:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 127.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.73it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:54:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 119.65it/s]\n","Batches: 100% 1/1 [00:00<00:00, 124.19it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:55:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.05it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.67it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:55:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.40it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:55:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 113.62it/s]\n","Batches: 100% 1/1 [00:00<00:00, 133.56it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:55:21 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.97it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:55:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.07it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:55:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.57it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:55:38 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.64it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.09it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:55:44 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.40it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.35it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:55:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.05it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:55:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.73it/s]\n","Batches: 100% 1/1 [00:00<00:00, 146.50it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:56:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.36it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:56:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.13it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:56:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.07it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:56:22 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 15:56:40 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1206 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet... disk. The LAL disk was', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 15:56:40 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 126.59it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.55it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:56:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 77.38it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.44it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:56:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.62it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:56:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 128.08it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.10it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:57:00 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 132.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.90it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:57:07 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 144.13it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:57:13 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.64it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:57:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.46it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.71it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:57:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 15:57:46 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1132 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet... to Sarah Yoon, QA, and', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 15:57:46 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 125.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.19it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:57:47 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 116.93it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.55it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:57:57 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 15:58:15 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1317 [type=json_invalid, input_value='{ \"title\": \"Error:OOS re.... The lab was retrained', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 15:58:15 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 1.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 122.26it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.62it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:58:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.55it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.40it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:58:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.76it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.61it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:58:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.19it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.51it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:58:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.72it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:58:49 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 121.92it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.36it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:58:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.06it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.87it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:59:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 15:59:25 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1336 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...oxin and found that the', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 15:59:25 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 2.0, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 106.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.71it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:59:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 15:59:44 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1275 [type=json_invalid, input_value='{ \"title\": \"Correct tabl... line was then stopped,', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 15:59:44 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 2.0, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 124.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.17it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:59:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 111.51it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.28it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 15:59:52 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 16:00:09 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1191 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...hy, contingency actions', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 16:00:09 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 2.0, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 133.24it/s]\n","Batches: 100% 1/1 [00:00<00:00, 137.21it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:00:10 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 136.00it/s]\n","Batches: 100% 1/1 [00:00<00:00, 140.19it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:00:15 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.77it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:00:23 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 133.11it/s]\n","Batches: 100% 1/1 [00:00<00:00, 130.28it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:00:27 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.47it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.66it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:00:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 122.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.92it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:00:33 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.60it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.52it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:00:39 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.69it/s]\n","Ref_row:21: Generating text with the following parameters:\n","{'temperature': 2.0, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:00:41 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 139.66it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.91it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:00:45 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 118.53it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.21it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:00:53 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.42it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.58it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:01:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 134.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 132.55it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:01:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.29it/s]\n","Batches: 100% 1/1 [00:00<00:00, 142.99it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:01:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.98it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.22it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:01:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 129.99it/s]\n","Batches: 100% 1/1 [00:00<00:00, 126.51it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:01:31 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 16:01:48 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1442 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...humidity excursion. The', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 16:01:48 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.3, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 121.67it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.04it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:01:50 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.02it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.46it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:01:58 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 131.87it/s]\n","Batches: 100% 1/1 [00:00<00:00, 135.34it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:02:05 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 16:02:23 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1562 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet...replaced, environmental', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 16:02:23 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.3, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 141.56it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.62it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:02:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.84it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.90it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:02:30 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 16:02:47 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1254 [type=json_invalid, input_value='{ \"title\": \"Correct tabl...oom humidity rose to 68', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 16:02:47 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.3, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 122.85it/s]\n","Batches: 100% 1/1 [00:00<00:00, 121.82it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:02:48 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 140.78it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.48it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:02:56 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.18it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.53it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:03:03 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.10it/s]\n","Batches: 100% 1/1 [00:00<00:00, 139.02it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:03:11 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 123.49it/s]\n","Batches: 100% 1/1 [00:00<00:00, 145.76it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:03:19 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 120.52it/s]\n","Batches: 100% 1/1 [00:00<00:00, 108.16it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.5, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:03:26 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.88it/s]\n","Batches: 100% 1/1 [00:00<00:00, 138.18it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:03:35 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 130.43it/s]\n","Batches: 100% 1/1 [00:00<00:00, 134.94it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:03:42 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","08/14/2025 16:03:59 - mods.dataHandler - ERROR - Error while unpacking title or report from model output. Error: 1 validation error for Report\n","  Invalid JSON: EOF while parsing a string at line 1 column 1557 [type=json_invalid, input_value='{ \"title\": \"Wrong tablet... The humidity excursion', input_type=str]\n","    For further information visit https://errors.pydantic.dev/2.11/v/json_invalid\n","08/14/2025 16:03:59 - mods.testBench - ERROR - Failed to generate report with \n","generation_parameters {'temperature': 0.7, 'top_k': 30, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}: cannot unpack non-iterable NoneType object\n","Batches: 100% 1/1 [00:00<00:00, 111.17it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.42it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 0.8, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:04:01 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 126.57it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.26it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 10, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:04:08 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 135.12it/s]\n","Batches: 100% 1/1 [00:00<00:00, 131.56it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 30, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:04:16 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 137.15it/s]\n","Batches: 100% 1/1 [00:00<00:00, 141.75it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 0.7, 'top_k': 50, 'top_p': 1.0, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:04:24 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 138.86it/s]\n","Batches: 100% 1/1 [00:00<00:00, 143.42it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 10, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:04:29 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 82.95it/s]\n","Batches: 100% 1/1 [00:00<00:00, 98.69it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 30, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:04:36 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Batches: 100% 1/1 [00:00<00:00, 124.69it/s]\n","Batches: 100% 1/1 [00:00<00:00, 136.48it/s]\n","Ref_row:22: Generating text with the following parameters:\n","{'temperature': 1.3, 'top_k': 50, 'top_p': 0.2, 'repetition_penalty': 1.0, 'do_sample': True, 'max_new_tokens': 300.0}\n","08/14/2025 16:04:43 - transformers.generation.utils - WARNING - Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]}]},{"cell_type":"markdown","source":["# Several models run in parallel\n","In linux command with the & we can run several programs in parallel\n","**Careful it fills GPU RAM quickly if models are > 2 B parameters**\n","\n","**NOTA:** Better use the Colab L4 GPU for charging two models in parallel. The L4 GPU size is 25 GB, so it could admit up to 5B parameters more or less, i.e.:\n","\n","- two models of at most 2.5 B parameters.\n","- Three models of 1 B parameters each\n","\n"],"metadata":{"id":"JozIHMbjZGtI"}},{"cell_type":"code","source":["!python app/reportParamGridSearch.py --model_id microsoft/phi-2 --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True & python app/reportParamGridSearch.py --model_id HuggingFaceTB/SmolLM3-3B --start_idx 20 --end_idx 22  --temperature 0.3 0.7 1.3 2.0 --top_p 0.2 0.5 0.8 1 --top_k 10 30 50 --max_new_tokens 300 --do_sample True"],"metadata":{"id":"yCGd0oBQZbTL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# KILL SESSION TO AVOID LEAVING SESSION ON AND CONSUME GPU UNITS\n","\n","from google.colab import runtime\n","runtime.unassign()"],"metadata":{"id":"j0P1LHHOXp0d"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b15381868984f48a958540b50f785c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f06d02f23654471b0c1377b8d7f3203","IPY_MODEL_73ddf9e17e6a4cfe85cb47a2625303e8","IPY_MODEL_b5157bdd857148fb9d5c6488691a81b8"],"layout":"IPY_MODEL_c260a953e8564a31bd2d64fbb3445f84"}},"3a0a7902dddf42028ff6c54a3c2af8d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c7b05ef240b4737948d1a48b423624e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f06d02f23654471b0c1377b8d7f3203":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e33742605ca4f12863776e771c9dc77","placeholder":"​","style":"IPY_MODEL_e54018a2d58a4c1e95410f5c9bc419e1","value":"generation_config.json: 100%"}},"73ddf9e17e6a4cfe85cb47a2625303e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c7b05ef240b4737948d1a48b423624e","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3a0a7902dddf42028ff6c54a3c2af8d2","value":124}},"9e33742605ca4f12863776e771c9dc77":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa7496c29bef49fc8a29d3102800ce4f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5157bdd857148fb9d5c6488691a81b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b91e110c59a2464284d4d00d3204eca4","placeholder":"​","style":"IPY_MODEL_aa7496c29bef49fc8a29d3102800ce4f","value":" 124/124 [00:00&lt;00:00, 12.1kB/s]"}},"b91e110c59a2464284d4d00d3204eca4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c260a953e8564a31bd2d64fbb3445f84":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e54018a2d58a4c1e95410f5c9bc419e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}